{
  "Id": "tag:blogger.com,1999:blog-7365737872932202828",
  "Website": "http://www.tpeczek.com/",
  "Title": "Yet another developer blog",
  "LastUpdatedTime": "2025-02-12T00:26:11+00:00",
  "Description": "",
  "Language": null,
  "Items": [
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-1304634620752734117",
      "Title": "Exploring Neon as a Serverless Postgres Alternative for .NET Applications on Azure - Part 1 (Simple ASP.NET Core on App Service)",
      "PublishDate": "2025-02-10T21:26:00+00:00",
      "Summary": "<p><a href=\" https://neon.tech/?refcode=44WD03UH\">Neon</a>, a serverless Postgres platform, was recently brought to my attention. It comes with  the promise of some  interesting features like scale-to-zero, on-demand autoscaling, point-in-time restore and time travel with up to 30 days retention, instant read replicas, and probably the most unique, branching (which allows you to quickly branch your schema and data to create isolated development, test, or other purpose environments).</p>\n<p>That's a lot of stuff to play with, but before I jumped in, I wanted to see how Neon integrated with my tech stack of choice - .NET, Azure, and GitHub. I decided to start with something simple - an ASP.NET Core application hosted on Azure App Service. Since I like to build things from the ground up, my first step was infrastructure.</p>\n<h2 id=\"deploying-the-infrastructure\">Deploying the Infrastructure</h2>\n<p>Neon has three deployment options that might be of interest to me:</p>\n<ul>\n<li>Create a Neon project hosted in the AWS region</li>\n<li>Create a Neon project hosted in the Azure region</li>\n<li>Deploy a Neon organization as an <a href=\"https://learn.microsoft.com/en-us/azure/partner-solutions/neon?WT.mc_id=DT-MVP-5002979\">Azure Native ISV Service</a></li>\n</ul>\n<p>The first two options are fully managed by Neon, you grab the connection details from the Neon console and start hacking. But I wanted to explore the third option because it provides the tightest integration from an Azure perspective (including SSO and unified billing), and that's what the organizations I work with are usually looking for.</p>\n<p>As a strong IaC advocate, I also didn't want to deploy Neon through the Azure Portal or CLI, I wanted to use Bicep. Thanks to the native Azure integration, the Neon organization comes with its own resource type - <a href=\"https://learn.microsoft.com/en-us/azure/templates/neon.postgres/organizations?WT.mc_id=DT-MVP-5002979\"><code>Neon.Postgres/organizations</code></a>.</p>\n<pre><code class=\"lang-bicep\">resource neonOrganization 'Neon.Postgres/organizations@2024-08-01-preview' = {\n  name: 'neon-net-applications-on-azure'\n  location: location\n  properties: {\n    companyDetails: { }\n    marketplaceDetails: {\n      subscriptionId: subscription().id\n      offerDetails: {\n        publisherId: 'neon1722366567200'\n        offerId: 'neon_serverless_postgres_azure_prod'\n        planId: 'neon_serverless_postgres_azure_prod_free'\n        termUnit: 'P1M'\n        termId: 'gmz7xq9ge3py'\n      }\n    }\n    partnerOrganizationProperties: {\n      organizationName: 'net-applications-on-azure'\n    }\n    userDetails: {\n      upn: 'userId@domainName'\n    }\n  }\n}\n</code></pre>\n<p>You may ask, how did I get the values for the properties? Well, they are not documented (yet, I'm assuming) and I had to resort to reverse engineering (inspecting the template of the manually deployed resource).</p>\n<p>The next step is to create a project. Everything in Neon (branches, databases, etc.) lives inside a project. Neon projects don't have an Azure resource representation, so I had to change the tool. I could create a project through the UI, but since I still wanted to have repeatability (something I could later reuse in a GitHub Actions workflow later), I decided to use Neon CLI. I still had to visit the UI (thanks to SSO I could just click on a link available on the resource <em>Overview</em> blade) to get myself an <a href=\"https://neon.tech/docs/manage/api-keys\">API key</a>.</p>\n<pre><code class=\"lang-bash\">export NEON_API_KEY=&lt;neon_api_key&gt;\n\nneon projects create \\\n    --name simple-asp-net-core-on-app-service \\\n    --region-id azure-westus3 \\\n    --output json\n</code></pre>\n<p>The output includes connection details for the created database. I don't want to manage secrets manually if I don't have to, so I decided to quickly create a key vault and put them there.</p>\n<p>The last missing elements of my intended infrastructure were a managed identity, a container registry, an app service plan, and an app service. Below is the final diagram.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0OssiHC2iwP7LDlAbIyfXMVgEDtI8tNU7naUum8-iif0pSnEpCQpObmaLRhGDyBo6mbo-PSm_cVY3B1tn_h-OgLWYAe4fqhr_RtRJrB6WNji1_vzY7cSAHfHXx6BAyidDe5WLbWJrQzLV2F2yzdNtd2dOYzrsYfHGczD4OSdS6xblKegjeHz8iFDZvEg/s1600/SIMPLE~1.PNG\" alt=\"Infrastructure for simple ASP.NET Core application on Azure App Service (managed identity, key vault, container registry, app service plan, and app service) and using Neon deployed as Azure Native ISV Service\"></p>\n<h2 id=\"seeding-the-database\">Seeding the Database</h2>\n<p>With all the infrastructure in place, I could start building my ASP.NET Core application. I wanted something simple, something that just displayed data, as my goal here was to see if Neon could be a drop-in replacement for Posgres from a code perspective. But to display data, you have to have data. So I needed to seed the database. I decided to take the simplest approach I could think of - using the built-in seeding capabilities of the Entity Framework Core. I had two options to choose from: <a href=\"https://learn.microsoft.com/en-us/ef/core/modeling/data-seeding?WT.mc_id=DT-MVP-5002979#use-seeding-method\"><code>UseSeeding</code>/<code>UseAsyncSeeding</code> method</a> or <a href=\"https://learn.microsoft.com/en-us/ef/core/modeling/data-seeding?WT.mc_id=DT-MVP-5002979#model-managed-data\">model managed data</a>. I chose the latter and I quickly created a database context with two sets and a bunch of entities to add.</p>\n<pre><code class=\"lang-cs\">public class StarWarsDbContext : DbContext\n{\n    public DbSet&lt;Character&gt; Characters { get; private set; }\n\n    public DbSet&lt;Planet&gt; Planets { get; private set; }\n\n    public StarWarsDbContext(DbContextOptions&lt;StarWarsDbContext&gt; options)\n    : base(options)\n    { }\n\n    protected override void OnModelCreating(ModelBuilder modelBuilder)\n    {\n        modelBuilder.Entity&lt;Planet&gt;(builder =&gt;\n        {\n            builder.Property(x =&gt; x.Name).IsRequired();\n            builder.HasData(\n                new Planet { Id = 1, Name = \"Tatooine\", ... },\n                ...\n            );\n        });\n\n        modelBuilder.Entity&lt;Character&gt;(builder =&gt;\n        {\n            builder.Property(x =&gt; x.Name).IsRequired();\n            builder.HasData(\n                new Character { Id = 1, Name = \"Luke Skywalker\", ... },\n                ...\n            );\n        });\n    }\n}\n</code></pre>\n<p>Now I needed to register this database context as a service in my application. For the provider I used <code>Npgsql.EntityFrameworkCore.PostgreSQL</code>in the spirit of the drop-in replacement approach. With the connection string in the key vault and a managed identity in place to authenticate against that key vault, I could use <code>DefaultAzureCredential</code> and <code>SecretClient</code> to configure the provider and restrict my application settings to the key vault URI (yes, I choose this option even in demos).</p>\n<pre><code class=\"lang-cs\">var builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddDbContext&lt;StarWarsDbContext&gt;(options =&gt;\n{\n    var keyVaultSecrettClient = new SecretClient(\n        new Uri(builder.Configuration[\"KEY_VAULT_URI\"]),\n        new DefaultAzureCredential()\n    );\n    options.UseNpgsql(keyVaultSecrettClient.GetSecret(\"neon-connection-string\").Value.Value);\n});\n\n...\n</code></pre>\n<p>The last thing to do here was to trigger the model creation code. This only needs to happen once, and as this is a demo, it can be ugly. I used an old trick of getting the database context as part of the startup and calling <code>EnsureCreated</code> (please don't do this in serious applications).</p>\n<pre><code class=\"lang-cs\">...\n\nvar app = builder.Build();\n\nusing (var serviceScope = app.Services.GetRequiredService&lt;IServiceScopeFactory&gt;().CreateScope())\n{\n    var context = serviceScope.ServiceProvider.GetRequiredService&lt;StarWarsDbContext&gt;();\n    context.Database.EnsureCreated();\n}\n\n...\n</code></pre>\n<p>All the pieces are now in place. It's time to wrap things up.</p>\n<h2 id=\"completing-and-deploying-the-application\">Completing and Deploying the Application</h2>\n<p>For the application to be complete, it needed some UI. Since I didn't have an idea for anything special, I did something I used to do a lot in the past - a jqGrid based table that was quickly set up with the help of my old project <a href=\"https://github.com/tpeczek/Lib.AspNetCore.Mvc.JqGrid\">Lib.AspNetCore.Mvc.JqGrid</a> (I've basically copied some stuff from its <a href=\"https://github.com/tpeczek/Demo.AspNetCore.JqGrid\">demo</a>).</p>\n<p>As you've probably guessed from the infrastructure, my intention from the start was to deploy this application as a container. So my last step was to add a docker file, build, push, and voilà. I was able to navigate to the application, browse and sort the data.</p>\n<p>Everything worked as expected and I consider my first experiment with Neon a success 😊.</p>\n<h2 id=\"thoughts\">Thoughts</h2>\n<p>They say to never publish part one if you don't have part two ready. I don't have part two ready, but I really think I'll write one, I just don't know when 😉. That's because Neon really got me interested.</p>\n<p>In this post I wanted to check out the basics and set the stage for digging deeper. While writing it, I've also created a <a href=\"https://github.com/tpeczek/demo-neon-for-net-applications-on-azure\">repository</a> where you can find ready-to-deploy infrastructure and application. I've created GitHub Actions workflows for deploying a Neon organization, creating a Neon project, and deploying the solution itself. All you need to do to play with it is clone and provide credentials 🙂.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2025/02/exploring-neon-as-serverless-postgres.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-2520358221106669221",
      "Title": "Monitoring C# Azure Functions in the Isolated Worker Model - Infrastructure & Configuration Deep Dive",
      "PublishDate": "2024-11-26T12:20:00+00:00",
      "Summary": "<p>Not so long ago my colleague reached out with a question \"Are there any known issues with <code>ITelemetryInitializer</code> in Azure Functions?\". This question started a discussion about the monitoring configuration for C# Azure Functions in the isolated worker model. At some point in that discussion I stated \"Alright, you've motivated me, I'll make a sample\". When I sat down to do that, I started wondering which scenario should I cover and my conclusion was that there are several things I should go through... So here I am.</p>\n<h2 id=\"setting-the-scene\">Setting the Scene</h2>\n<p>Before I start describing how we can monitor an isolated worker model C# Azure Function, allow me to introduce you to the one we are going to use for this purpose. I want to start with as basic setup as possible. This is why the initial <code>Program.cs</code> will contain only four lines.</p>\n<pre><code class=\"lang-cs\">var host = new HostBuilder()\n    .ConfigureFunctionsWebApplication()\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>The <code>host.json</code> will also be minimal.</p>\n<pre><code class=\"lang-json\">{\n    \"version\": \"2.0\",\n    \"logging\": {\n        \"logLevel\": {\n            \"default\": \"Information\"\n        }\n    }\n}\n</code></pre>\n<p>For the function itself, I've decided to go for the Fibonacci sequence implementation as it can easily generate a ton of logs.</p>\n<pre><code class=\"lang-cs\">public class FibonacciSequence\n{\n    private readonly ILogger&lt;FibonacciSequence&gt; _logger;\n\n    public FibonacciSequence(ILogger&lt;FibonacciSequence&gt; logger)\n    {\n        _logger = logger;\n    }\n\n    [Function(nameof(FibonacciSequence))]\n    public async Task&lt;HttpResponseData&gt; Run(\n        [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", Route = \"fib/{index:int}\")]\n        HttpRequestData request,\n        int index)\n    {\n        _logger.LogInformation(\n            $\"{nameof(FibonacciSequence)} function triggered for {{index}}.\",\n            index\n        );\n\n        var response = request.CreateResponse(HttpStatusCode.OK);\n        response.Headers.Add(\"Content-Type\", \"text/plain; charset=utf-8\");\n\n        await response.WriteStringAsync(FibonacciSequenceRecursive(index).ToString());\n\n        return response;\n    }\n\n    private int FibonacciSequenceRecursive(int index)\n    {\n        int fibonacciNumber = 0;\n\n        _logger.LogInformation(\"Calculating Fibonacci sequence for {index}.\", index);\n\n        if (index &lt;= 0)\n        {\n            fibonacciNumber = 0;\n        }\n        else if (index &lt;= 1)\n        {\n            fibonacciNumber = 1;\n        }\n        else\n        {\n            fibonacciNumber = \n              FibonacciSequenceRecursive(index - 1)\n              + FibonacciSequenceRecursive(index - 2);\n        }\n\n        _logger.LogInformation(\n            \"Calculated Fibonacci sequence for {index}: {fibonacciNumber}.\",\n            index,\n            fibonacciNumber\n        );\n\n        return fibonacciNumber;\n    }\n}\n</code></pre>\n<p>This is a recursive implementation which in our case has the added benefit of being crashable on demand 😉.</p>\n<p>Now we can start capturing the signals this function will produce after deployment.</p>\n<h2 id=\"simple-and-limited-option-for-specific-scenarios-file-system-logging\">Simple and Limited Option for Specific Scenarios - File System Logging</h2>\n<p>What we can capture in a minimal deployment scenario is logs coming from our function. What do I understand by a minimal deployment scenario? The bare minimum that Azure Function requires is a storage account, app service plan, and function app. The function can push all its logs into that storage account. Is this something I would recommend for a production scenario? Certainly not. It's only logs (and in production you will want metrics) and works reasonably only for Azure Functions deployed on Windows (for production I would suggest Linux due to better cold start performance or lower pricing for dedicated plan). But it may be the right option for some development scenarios (when you want to run some work, download the logs and analyze them locally). So, how to achieve this? Let's start with some Bicep snippets for the required infrastructure. First, we need to deploy a storage account.</p>\n<pre><code class=\"lang-bicep\">resource storageAccount 'Microsoft.Storage/storageAccounts@2023-05-01' = {\n  name: 'stwebjobs${uniqueString(resourceGroup().id)}'\n  location: resourceGroup().location\n  sku: {\n    name: 'Standard_LRS'\n  }\n  kind: 'Storage'\n}\n</code></pre>\n<p>We also need an app service plan. It must be a Windows one (the below snippet creates a <em>Windows Consumption Plan</em>).</p>\n<pre><code class=\"lang-bicep\">resource appServicePlan 'Microsoft.Web/serverfarms@2024-04-01' = {\n  name: 'plan-monitored-function'\n  location: resourceGroup().location\n  sku: {\n    name: 'Y1'\n    tier: 'Dynamic'\n  }\n  properties: {\n    computeMode: 'Dynamic'\n  }\n}\n</code></pre>\n<p>And the actual service doing the work, the function app.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  name: 'func-monitored-function'\n  location: resourceGroup().location\n  kind: 'functionapp'\n  properties: {\n    serverFarmId: appServicePlan.id\n    siteConfig: {\n      netFrameworkVersion: 'v8.0'\n      appSettings: [\n        {\n          name: 'AzureWebJobsStorage'\n          value: 'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${storageAccount.listKeys().keys[0].value}'\n        }\n        {\n          name: 'WEBSITE_CONTENTAZUREFILECONNECTIONSTRING'\n          value: 'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${storageAccount.listKeys().keys[0].value}'\n        }\n        {\n          name: 'WEBSITE_CONTENTSHARE'\n          value: 'func-monitored-function'\n        }\n        {\n          name: 'FUNCTIONS_EXTENSION_VERSION'\n          value: '~4'\n        }\n        {\n          name: 'FUNCTIONS_WORKER_RUNTIME'\n          value: 'dotnet-isolated'\n        }\n      ]\n    }\n    httpsOnly: true\n  }\n}\n</code></pre>\n<p>A couple of words about this function app. The <code>kind: 'functionapp'</code> indicates that this is a Windows function app that creates the requirement of setting <code>netFrameworkVersion</code> to desired .NET version. To make this an isolated worker model function, the <code>FUNCTIONS_EXTENSION_VERSION</code> is set to <code>~4</code> and <code>FUNCTIONS_WORKER_RUNTIME</code> to <code>dotnet-isolated</code>. When it comes to all the settings referencing the storage account, your attention should go to <code>WEBSITE_CONTENTAZUREFILECONNECTIONSTRING</code> and <code>WEBSITE_CONTENTSHARE</code> - those indicate the file share that will be created for this function app in the storage account (this is where the logs will go).</p>\n<p>After deployment, the resulting infrastructure should be as below.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_iJvUHWJAAvLFHhOOkGzTO92CB2o3UF5qKHcXLy3BqaVgldC2EkjvgiLZAwhL-XGDx1Zgol4BZz6YH5cay4s3GyXbZteYGthJvN0oP0_IrX_HiIY5XYHNhNc3iEt6c-Q4fv5HeWae9c4mHlXdK3mfjWfxKJVq3FLPbsXmA2fprs9SpFqb94B2GR9yu_8/s1600/azure-functions-with-file-system-logging.png\" alt=\"Infrastructure for Azure Functions with file system logging (storage account, app service plan, and function app)\"></p>\n<p>What remains is configuring the file system logging in the <code>host.json</code> file of our function. The default behavior is to generate log files only when the function is being debugged using the Azure portal. We want them to be generated always.</p>\n<pre><code class=\"lang-json\">{\n    \"version\": \"2.0\",\n    \"logging\": {\n        \"fileLoggingMode\": \"always\",\n        \"logLevel\": {\n            \"default\": \"Information\"\n        }\n    }\n}\n</code></pre>\n<p>When you deploy the code and execute some requests, you will be able to find the log files in the defined file share (the path is <code>/LogFiles/Application/Functions/Host/</code>).</p>\n<pre><code class=\"lang-txt\">2024-11-17T15:33:39.339 [Information] Executing 'Functions.FibonacciSequence' ...\n2024-11-17T15:33:39.522 [Information] FibonacciSequence function triggered for 6.\n2024-11-17T15:33:39.526 [Information] Calculating Fibonacci sequence for 6.\n2024-11-17T15:33:39.526 [Information] Calculating Fibonacci sequence for 5.\n...\n2024-11-17T15:33:39.527 [Information] Calculated Fibonacci sequence for 5: 5.\n...\n2024-11-17T15:33:39.528 [Information] Calculated Fibonacci sequence for 4: 3.\n2024-11-17T15:33:39.528 [Information] Calculated Fibonacci sequence for 6: 8.\n2024-11-17T15:33:39.566 [Information] Executed 'Functions.FibonacciSequence' ...\n</code></pre>\n<h2 id=\"for-production-scenarios-entra-id-protected-application-insights\">For Production Scenarios - Entra ID Protected Application Insights</h2>\n<p>As I said, I wouldn't use file system logging for production. Very often it's not even enough for development. This is why Application Insights are considered a default these days. But before we deploy one, we can use the fact that we no longer new Windows and change the deployment to be a Linux based one. First I'm going to change the app service plan to a <em>Linux Consumption Plan</em>.</p>\n<pre><code class=\"lang-bicep\">resource appServicePlan 'Microsoft.Web/serverfarms@2024-04-01' = {\n  ...\n  kind: 'linux'\n  properties: {\n    reserved: true\n  }\n}\n</code></pre>\n<p>With a different app service plan, the function app can be changed to a Linux one, which means changing the kind to <code>functionapp,linux</code> and replacing <code>netFrameworkVersion</code> with the corresponding <code>linuxFxVersion</code>.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  ...\n  kind: 'functionapp,linux'\n  properties: {\n    serverFarmId: appServicePlan.id\n    siteConfig: {\n      linuxFxVersion: 'DOTNET-ISOLATED|8.0'\n      ...\n    }\n    httpsOnly: true\n  }\n}\n</code></pre>\n<p>There is also a good chance that you no longer need the file share (although the <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/storage-considerations?WT.mc_id=DT-MVP-5002979\">documentation</a> itself is inconsistent whether it's needed when running Linux functions on the Elastic Premium plan) so the <code>WEBSITE_CONTENTAZUREFILECONNECTIONSTRING</code> and <code>WEBSITE_CONTENTSHARE</code> can simply be removed. No requirement for the file share creates one more improvement opportunity - we can drop credentials for the blob storage (<code>AzureWebJobsStorage</code>) as this connection can use managed identity. I'm going to use a user-assigned managed identity because I often prefer them and they usually cause more trouble to set up 😉. To do so, we need to create one and grant it the <em>Storage Blob Data Owner</em> role for the storage account.</p>\n<pre><code class=\"lang-bicep\">resource managedIdentity 'Microsoft.ManagedIdentity/userAssignedIdentities@2023-01-31' = {\n  name: 'id-monitored-function'\n  location: resourceGroup().location\n}\n\nresource storageBlobDataOwnerRoleDefinition 'Microsoft.Authorization/roleDefinitions@2022-04-01' existing = {\n  name: 'b7e6dc6d-f1e8-4753-8033-0f276bb0955b' // Storage Blob Data Owner\n  scope: subscription()\n}\n\nresource storageBlobDataOwnerRoleAssignment 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  scope: storageAccount\n  name: guid(storageAccount.id, managedIdentity.id, storageBlobDataOwnerRoleDefinition.id)\n  properties: {\n    roleDefinitionId: storageBlobDataOwnerRoleDefinition.id\n    principalId: managedIdentity.properties.principalId\n    principalType: 'ServicePrincipal'\n  }\n}\n</code></pre>\n<p>The change to the function app is only about assigning the managed identity and replacing <code>AzureWebJobsStorage</code> with <code>AzureWebJobsStorage__accountName</code>.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  ...\n\n  ...\n  properties: {\n    ...\n    siteConfig: {\n      ...\n      appSettings: [\n        {\n          name: 'AzureWebJobsStorage__accountName'\n          value: storageAccount.name\n        }\n        ...\n      ]\n    }\n    httpsOnly: true\n  }\n}\n</code></pre>\n<p>Enough detouring (although it wasn't without a purpose 😜) - it's time to deploy Application Insights. As the classic Application Insights are retired, we are going to create a workspace-based instance.</p>\n<pre><code class=\"lang-bicep\">resource logAnalyticsWorkspace 'Microsoft.OperationalInsights/workspaces@2023-09-01' = {\n  name: 'log-monitored-function'\n  location: resourceGroup().location\n  properties: {\n    sku: { \n      name: 'PerGB2018' \n    }\n  }\n}\n\nresource applicationInsights 'Microsoft.Insights/components@2020-02-02' = {\n  name: 'appi-monitored-function'\n  location: resourceGroup().location\n  kind: 'web'\n  properties: {\n    Application_Type: 'web'\n    WorkspaceResourceId: logAnalyticsWorkspace.id\n    DisableLocalAuth: true\n  }\n}\n</code></pre>\n<p>You might have noticed that I've set <code>DisableLocalAuth</code> to <code>true</code>. This is a security improvement. It enforces authentication by <em>Entra ID</em> for ingestion and as a result, makes <code>InstrumentationKey</code> a resource identifier instead of a secret. This is nice and we can easily handle it because we already have managed identity in place (I told you it had a purpose 😊). All we need to do is grant the <em>Monitoring Metrics Publisher</em> role to our managed identity.</p>\n<pre><code class=\"lang-bicep\">resource monitoringMetricsPublisherRoleDefinition 'Microsoft.Authorization/roleDefinitions@2022-04-01' existing = {\n  name: '3913510d-42f4-4e42-8a64-420c390055eb' // Monitoring Metrics Publisher\n  scope: subscription()\n}\n\nresource monitoringMetricsPublisherRoleAssignment 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  scope: applicationInsights\n  name: guid(applicationInsights.id, managedIdentity.id, monitoringMetricsPublisherRoleDefinition.id)\n  properties: {\n    roleDefinitionId: monitoringMetricsPublisherRoleDefinition.id\n    principalId: managedIdentity.properties.principalId\n    principalType: 'ServicePrincipal'\n  }\n}\n</code></pre>\n<p>Adding two application settings definitions to our function app will tie the services together.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  ...\n  properties: {\n    ...\n    siteConfig: {\n      ...\n      appSettings: [\n        ...\n        {\n          name: 'APPLICATIONINSIGHTS_CONNECTION_STRING'\n          value: applicationInsights.properties.ConnectionString\n        }\n        {\n          name: 'APPLICATIONINSIGHTS_AUTHENTICATION_STRING'\n          value: 'ClientId=${managedIdentity.properties.clientId};Authorization=AAD'\n        }\n        ...\n      ]\n    }\n    httpsOnly: true\n  }\n}\n</code></pre>\n<p>Are we done with the infrastructure? Not really. There is one more useful thing that is often forgotten - enabling storage logs. There are some important function app data in there so it would be nice to monitor it.</p>\n<pre><code class=\"lang-bicep\">resource storageAccountBlobService 'Microsoft.Storage/storageAccounts/blobServices@2023-05-01' existing = {\n  name: 'default'\n  parent: storageAccount\n}\n\nresource storageAccountDiagnosticSettings 'Microsoft.Insights/diagnosticSettings@2021-05-01-preview' = {\n  name: '${storageAccount.name}-diagnostic'\n  scope: storageAccountBlobService\n  properties: {\n    workspaceId: logAnalyticsWorkspace.id\n    logs: [\n      {\n        category: 'StorageWrite'\n        enabled: true\n      }\n    ]\n    metrics: [\n      {\n        category: 'Transaction'\n        enabled: true\n      }\n    ]\n  }\n}\n</code></pre>\n<p>Now we are done and our infrastructure should look like below.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUJnh-IuO3BTDOM3EdRbr7P1C75n6JO5J8l3VZO9RnmrrVZlq3BzZUebxAqqQzpLWo-1xsNkoGknE_uY0S2zqD3SCtFRoib4yvj4EjrNFG_mnf1tgMz6NLeeysf8_0B5fW6DZNX-OIFW8vHsnC9f89BtvvGO1fh1sdNUcCy9EysgNoduTXzUWGVh2hJqg/s1600/azure-functions-with-application-insights.png\" alt=\"Infrastructure for Azure Functions with Application Insights (managed identity, application insights, log workspace, storage account, app service plan, and function app)\"></p>\n<p>Once we deploy our function and make some requests, thanks to the codeless monitoring by the host and relaying worker logs through the host, we can find the traces in Application Insights.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmRaqy2hBcCz2kyPjNXV4sQPV2IrscHKmKljpb0F6iPlLrIc45KCD4e4fcPtwbtWHkZ-MdOffH2i_UvsGh1K1Hb578DX9G7i408NUphAtfKp8RIBhhBl7aS9nL7L-dczdbOLZ9jJ_UZkHTtrwuJbIwDv3bI_PubEBAhNzaD2IBZp8amyT3jN-2qgxC7dM/s1600/relayed-worker-logs-in-application-insights.png\" alt=\"Relayed worker logs ingested through host codeless monitoring\"></p>\n<p>You may be asking what I mean by <em>\"relaying worker logs through the host\"</em>? You probably remember that in the case of C# Azure Functions in the isolated worker model, we have two processes: functions host and isolated worker. Azure Functions wants to be helpful and by default, it sends the logs from the worker process to the functions host process, which then sends them to Application Insights.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSBk_xKsiHQHqiUD7gG9X6xUlgqQZBakg7-kfA_oEuVtQqOPiE0SQSxv1paT-8742lsxcUbFX3hDDx4sJ7ULkb_aOjlJXv62SrHsk1OxQrz8u510YSyW4LFOq_ueuK0-Qv0r_8ri5cThQMTYrtKtOJPP4AseVgBTwN_DpruSRMu6FkKM-0-Nie9KmLWT8/s1600/azure-functions-relaying-worker-logs-through-the-host.png\" alt=\"Azure Functions relaying worker logs through the host\"></p>\n<p>This is nice, but may not be exactly what you want. You may want to split the logs so you can treat them separately (for example by configuring different default log levels for the host and the worker). To achieve that you must explicitly set up Application Insights integration in the worker code.</p>\n<pre><code class=\"lang-cs\">var host = new HostBuilder()\n    .ConfigureFunctionsWebApplication()\n    .ConfigureServices(services =&gt; {\n        services.AddApplicationInsightsTelemetryWorkerService();\n        services.ConfigureFunctionsApplicationInsights();\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>The telemetry from the worker can now be controlled in the code, while from the host through the <code>host.json</code>. But if you deploy this, you will find no telemetry from the worker in Application Insights. You can still see your logs in the <em>Log stream</em>. You will also see a lot of those:</p>\n<pre><code class=\"lang-txt\">Azure.Identity: Service request failed.\nStatus: 400 (Bad Request)\n\nContent:\n{\"statusCode\":400,\"message\":\"Unable to load the proper Managed Identity.\",\"correlationId\":\"...\"}\n</code></pre>\n<p>That's Application Insights SDK not being able to use the user-assigned managed identity. Azure Functions runtime uses the <code>APPLICATIONINSIGHTS_AUTHENTICATION_STRING</code> setting to provide a user-assigned managed identity OAuth token to the Application Insights but none of that (at the time of writing this) happens when we set up the integration explicitly. That said, we can do it ourselves. The parser for the setting is available in the <code>Microsoft.Azure.WebJobs.Logging.ApplicationInsights</code> so we can mimic the host implementation.</p>\n<pre><code class=\"lang-cs\">using TokenCredentialOptions =\n    Microsoft.Azure.WebJobs.Logging.ApplicationInsights.TokenCredentialOptions;\n\nvar host = new HostBuilder()\n    ...\n    .ConfigureServices(services =&gt; {\n        services.Configure&lt;TelemetryConfiguration&gt;(config =&gt;\n        {\n            string? authenticationString =\n                Environment.GetEnvironmentVariable(\"APPLICATIONINSIGHTS_AUTHENTICATION_STRING\");\n\n            if (!String.IsNullOrEmpty(authenticationString))\n            {\n                var tokenCredentialOptions = TokenCredentialOptions.ParseAuthenticationString(\n                    authenticationString\n                );\n                config.SetAzureTokenCredential(\n                    new ManagedIdentityCredential(tokenCredentialOptions.ClientId)\n                );\n            }\n        });\n        services.AddApplicationInsightsTelemetryWorkerService();\n        services.ConfigureFunctionsApplicationInsights();\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>The telemetry should now reach the Application Insights without a problem.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJ35FDPtaLob7hxRZMQfK4XoyYM_UlLmMvP0CnghyhZD8JaIsXBr3VfOAHGuKfzet7lAlvf-zkYTwrFWFYkC1q59wm_joRbYU2YFaW3IiqtWSz6hnexFKl-TcuH381lbMTCXr9aNh6jJpDoYi57Ud8_bflDw97swo57Z9hmwLd0WIfGWWO4_jEvMv56g/s1600/azure-functions-emitting-worker-logs-directly.png\" alt=\"Azure Functions emitting worker logs directly\"></p>\n<p>Be cautious. As Application Insights SDK now controls emitting the logs you must be aware of its <em>opinions</em>. It so happens that it tries to optimize by default and adds a logging filter that sets the minimum log level to <em>Warning</em>. You may want to get rid of that (be certain to make it after <code>ConfigureFunctionsApplicationInsights</code>).</p>\n<pre><code class=\"lang-cs\">...\n\nvar host = new HostBuilder()\n    ...\n    .ConfigureServices(services =&gt; {\n        ...\n        services.ConfigureFunctionsApplicationInsights();\n        services.Configure&lt;LoggerFilterOptions&gt;(options =&gt;\n        {\n            LoggerFilterRule? sdkRule = options.Rules.FirstOrDefault(rule =&gt;\n                rule.ProviderName == typeof(Microsoft.Extensions.Logging.ApplicationInsights.ApplicationInsightsLoggerProvider).FullName\n            );\n\n            if (sdkRule is not null)\n            {\n                options.Rules.Remove(sdkRule);\n            }\n        });\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<h2 id=\"modifying-application-map-in-application-insights\">Modifying Application Map in Application Insights</h2>\n<p>The default application map is not impressive - it will simply show your function app by its resource name.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf40-tJWNi4AOhWfscHb-svDOL1SHw_GMhcIqYFhCirGiFzVfsNBKwerK6hPtR8doLysl-f2gVD9y5mVxSTGYSEZkEL7IE4kDCYYubxit6Hbr83MdIvFIYnkOhQ-EQ4G8M1A4VVfQGemR3ARVwYhu8egab9FTfOuqRbmEvpxiK2UwbjEPHEEZrNdMCvhM/s1600/application-insights-default-application-map.png\" alt=\"Default application map in Application Insights\"></p>\n<p>As function apps rarely exist in isolation, we often want to present them in a more meaningful way on the map by providing a cloud role. The simplest way to do this is through the <code>WEBSITE_CLOUD_ROLENAME</code> setting.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  ...\n  properties: {\n    ...\n    siteConfig: {\n      ...\n      appSettings: [\n        ...\n        {\n          name: 'WEBSITE_CLOUD_ROLENAME'\n          value: 'InstrumentedFunctions'\n        }\n        ...\n      ]\n    }\n    ...\n  }\n}\n</code></pre>\n<p>This will impact both the host and the work. With explicit Application Insights integration, we can separate the two (if there is such a need) and change the cloud role for the worker. This is done in the usual way, by registering an <code>ITelemetryInitializer</code>. The only important detail is the place of the registration - it needs to be after <code>ConfigureFunctionsApplicationInsights</code> as Azure Functions are adding their own initializers.</p>\n<pre><code class=\"lang-cs\">public class IsolatedWorkerTelemetryInitializer : ITelemetryInitializer\n{\n    public void Initialize(ITelemetry telemetry)\n    {\n        telemetry.Context.Cloud.RoleName = \"InstrumentedFunctionsIsolatedWorker\";\n    }\n}\n</code></pre>\n<pre><code class=\"lang-cs\">...\n\nvar host = new HostBuilder()\n    ...\n    .ConfigureServices(services =&gt; {\n        ...\n        services.ConfigureFunctionsApplicationInsights();\n        services.AddSingleton&lt;ITelemetryInitializer, IsolatedWorkerTelemetryInitializer&gt;();\n        services.Configure&lt;LoggerFilterOptions&gt;(options =&gt;\n        {\n            ...\n        });\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>The resulting map will look as below.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjB432aidxf6s6XwxqLkvHarlR3pb2Sy2F3TcbbVnaqTxl6c7ULivUwyx-x9phyphenhypheneGaWNGPovbf-Rnf3hGG8fEh93gaK3JOnPL6D_GNZMI4Hw5tmqJXcIBYcwyD0tj9V6Ead56qJ5td0-_9l1Gkyn6xsFwPK_LnHSfG1VUKdCBfbX9WQVdWik5FvXP_4M0I/s1600/application-insights-modified-application-map.png\" alt=\"Modified application map in Application Insights\"></p>\n<h2 id=\"monitoring-for-scaling-decisions\">Monitoring for Scaling Decisions</h2>\n<p>This is currently in preview, but you can enable emitting scale controller logs (for a chance to understand the scaling decisions). This is done with a single setting (<code>SCALE_CONTROLLER_LOGGING_ENABLED</code>) that takes a value in the <code>:</code> format. The possible values for destination are <code>Blob</code> and <code>AppInsights</code>, while the verbosity can be <code>None</code>, <code>Verbose</code>, and <code>Warning</code>. The <code>Verbose</code> seems to be the most useful one when you are looking for understanding as it's supposed to provide reason for changes in the worker count, and information about the triggers that impact those decisions.</p>\n<pre><code class=\"lang-bicep\">resource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  ...\n  properties: {\n    ...\n    siteConfig: {\n      ...\n      appSettings: [\n        ...\n        {\n          name: 'SCALE_CONTROLLER_LOGGING_ENABLED'\n          value: 'AppInsights:Verbose'\n        }\n        ...\n      ]\n    }\n    ...\n  }\n}\n</code></pre>\n<h2 id=\"is-this-exhaustive-\">Is This Exhaustive?</h2>\n<p>Certainly not 🙂. There are other things that you can fiddle with. You can have a very granular configuration of different log levels for different categories. You can fine-tune aggregation and sampling. When you are playing with all those settings, please remember that you're looking for the right balance between the gathered details and costs. My advice is not to configure monitoring for the worst-case scenario (when you need every detail) as that often isn't financially sustainable. Rather aim for a lower amount of information and change the settings to gather more if needed (a small hint - you can override <code>host.json</code> settings through application settings without deployment).</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2024/11/monitoring-c-azure-functions-in.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-3693878604342508492",
      "Title": "ASP.NET Core 9 and IAsyncEnumerable - Async Streaming JSON and NDJSON From Blazor WebAssembly",
      "PublishDate": "2024-09-24T08:57:00+00:00",
      "Summary": "<p>I've been exploring <a href=\"https://www.tpeczek.com/p/working-with-asynchronous-streaming.html\">working with asynchronous streaming data sources over HTTP</a> for quite some time on this blog. I've been playing with async streamed JSON and NDJSON. I've been streaming and receiving streams in ASP.NET Core and console applications. But there is one scenario I haven't touched yet - streaming from Blazor WebAssembly. Why? Simply it wasn't possible.</p>\n<p>Streaming objects from Blazor WebAssembly requires two things. First is the support for streaming upload in browsers Fetch API. The Fetch API did support streams for response bodies for quite some time (my first post using it is from 2019) but non-experimental support for streams as request bodies come to Chrome, Edge, and Safari in 2022 (and it is yet to come to Firefox as far as I can tell). Still, it's been available for two years and I haven't explored it yet? Well, I did, more than a year ago, with <a href=\"https://github.com/tpeczek/Demo.Ndjson.AsyncStreams/pull/10\">NDJSON and pure Javascript</a>, where I used <a href=\"https://www.tpeczek.com/2021/05/receiving-json-objects-stream-ndjson-in.html\">the ASP.NET Core endpoint</a> I created long ago. But here we are talking about Blazor WebAssembly, which brings us to the second requirement - the browser API needs to be surfaced in Blazor. This is <a href=\"https://github.com/dotnet/runtime/issues/36634\">finally happening with .NET 9</a> and now I can fully explore it.</p>\n<h2 id=\"async-streaming-ndjson-from-blazor-webassembly\">Async Streaming NDJSON From Blazor WebAssembly</h2>\n<p>I've decided to start with NDJSON because I already have my own building blocks for it (that mentioned ASP.NET Core endpoint, and <code>NdjsonAsyncEnumerableContent</code> coming from one of my <a href=\"https://github.com/tpeczek/Ndjson.AsyncStreams\">packages</a>). If things wouldn't work I would be able to easily debug them. </p>\n<p>I've quickly put together a typical code using <code>HttpClient</code>. I just had to make sure I've enabled streaming upload by calling <code>SetBrowserRequestStreamingEnabled</code> on the request instance.</p>\n<pre><code class=\"lang-cs\">private async Task PostWeatherForecastsNdjsonStream()\n{\n    ...\n\n    try\n    {\n        ....\n\n        HttpRequestMessage request = new HttpRequestMessage(\n            HttpMethod.Post, \"api/WeatherForecasts/stream\");\n        request.Content = new NdjsonAsyncEnumerableContent&lt;WeatherForecast&gt;(\n            StreamWeatherForecastsAsync());\n        request.SetBrowserRequestStreamingEnabled(true);\n\n        using HttpResponseMessage response = await Http.SendAsync(request, cancellationToken);\n\n        response.EnsureSuccessStatusCode();\n    }\n    finally\n    {\n        ...\n    }\n}\n</code></pre>\n<p>It worked! No additional changes and no issues to resolve. I could see the items being logged by my ASP.NET Core service as they were streamed.</p>\n<p>After this initial success, I could move to a potentially more challenging scenario - async streaming JSON.</p>\n<h2 id=\"async-streaming-json-from-blazor-webassembly\">Async Streaming JSON From Blazor WebAssembly</h2>\n<p>In theory, async streaming JSON should also just work. .NET has the support for <code>IAsyncEnumerable</code> built into <code>JsonSerializer.SerializeAsync</code> so <code>JsonContent</code> should just use it. Well, there is no better way than to try, so I've quickly changed the code.</p>\n<pre><code class=\"lang-cs\">private async Task PostWeatherForecastsJsonStream()\n{\n    ...\n\n    try\n    {\n        ...\n\n        HttpRequestMessage request = new HttpRequestMessage(\n            HttpMethod.Post, \"api/WeatherForecasts/stream\");\n        request.Content = JsonContent.Create(StreamWeatherForecastsAsync());\n        request.SetBrowserRequestStreamingEnabled(true);\n\n        using HttpResponseMessage response = await Http.SendAsync(request, cancellationToken);\n\n        response.EnsureSuccessStatusCode();\n    }\n    finally\n    {\n        ...\n    }\n}\n</code></pre>\n<p>To my surprise, it also worked! I couldn't test it against my ASP.NET Core service because it didn't support it, but I quickly created a dumb endpoint that would simply dump whatever was incoming.</p>\n<p>Now, why was I surprised? Because this is in fact a platform difference - this behavior seems to be unique for the <em>\"browser\"</em> platform. I did attempt the same in a desktop application earlier and <code>HttpClient</code> would buffer the request - I had to create a simple <a href=\"https://github.com/tpeczek/Demo.Ndjson.AsyncStreams/blob/main/Demo.Ndjson.AsyncStreams.Net.Http/JsonAsyncEnumerableContent.cs\"><code>HttpContent</code></a> that would wrap the underlying stream and flush on write. I think I like the Blazor WebAssembly behavior better and I'm happy that I didn't have to do the same here.</p>\n<p>Ok, but if I can't handle the incoming stream nicely in ASP.NET Core, it's not really that useful. So I've tried to achieve that as well.</p>\n<h2 id=\"receiving-async-streamed-json-in-asp-net-core\">Receiving Async Streamed JSON in ASP.NET Core</h2>\n<p>Receiving async streamed JSON is a little bit more tricky. <code>JsonSerializer</code> has a dedicated method (<code>DeserializeAsyncEnumerable</code>) to deserialize an incoming JSON stream into an <code>IAsyncEnumerable</code> instance. The built-in <code>SystemTextJsonInputFormatter</code> doesn't use that method, it simply uses <code>JsonSerializer.DeserializeAsync</code> with request body as an input. That shouldn't be a surprise, it is the standard way to deserialize incoming JSON. To support async streamed JSON, a custom formatter is required. What is more, to be sure that this custom formatter will be used, it can't be just added - it has to replace the built-in one. But that would mean that the custom formatter has to have all the functionality of the built-in one (if we want to support not only async streamed JSON). Certainly, not something I wanted to reimplement, so I've decided to simply inherit from <code>SystemTextJsonInputFormatter</code>.</p>\n<pre><code class=\"lang-cs\">internal class SystemTextStreamedJsonInputFormatter : SystemTextJsonInputFormatter\n{\n    ...\n\n    public override Task&lt;InputFormatterResult&gt; ReadRequestBodyAsync(InputFormatterContext context)\n    {\n        return base.ReadRequestBodyAsync(context);\n    }\n}\n</code></pre>\n<p>Now, for the actual functionality, the logic has to branch based on the type of the model as we want to use <code>DeserializeAsyncEnumerable</code> only if it's <code>IAsyncEnumerable</code>.</p>\n<pre><code class=\"lang-cs\">internal class SystemTextStreamedJsonInputFormatter : SystemTextJsonInputFormatter\n{\n    ...\n\n    public override Task&lt;InputFormatterResult&gt; ReadRequestBodyAsync(InputFormatterContext context)\n    {\n        if (context.ModelType.GetGenericTypeDefinition() == typeof(IAsyncEnumerable&lt;&gt;))\n        {\n            ...\n        }\n\n        return base.ReadRequestBodyAsync(context);\n    }\n}\n</code></pre>\n<p>That's not the end of challenges (and reflection). <code>JsonSerializer.DeserializeAsyncEnumerable</code> is a generic method. That means we have to get the actual type of values, based on it construct the right method, and call it.</p>\n<pre><code class=\"lang-cs\">internal class SystemTextStreamedJsonInputFormatter : SystemTextJsonInputFormatter\n{\n    ...\n\n    public override Task&lt;InputFormatterResult&gt; ReadRequestBodyAsync(InputFormatterContext context)\n    {\n        if (context.ModelType.GetGenericTypeDefinition() == typeof(IAsyncEnumerable&lt;&gt;))\n        {\n            MethodInfo deserializeAsyncEnumerableMethod = typeof(JsonSerializer)\n                .GetMethod(\n                    nameof(JsonSerializer.DeserializeAsyncEnumerable),\n                    [typeof(Stream), typeof(JsonSerializerOptions), typeof(CancellationToken)])\n                .MakeGenericMethod(context.ModelType.GetGenericArguments()[0]);\n\n            return Task.FromResult(InputFormatterResult.Success(\n                deserializeAsyncEnumerableMethod.Invoke(null, [\n                    context.HttpContext.Request.Body,\n                    SerializerOptions,\n                    context.HttpContext.RequestAborted\n                ])\n            ));\n        }\n\n        return base.ReadRequestBodyAsync(context);\n    }\n}\n</code></pre>\n<p>The implementation above is intentionally ugly, so it doesn't hide anything and can be understood more easily. You can find a cleaner one, which also performs some constructed methods caching, in the <a href=\"https://github.com/tpeczek/Demo.Ndjson.AsyncStreams/blob/main/Demo.Ndjson.AsyncStreams.AspNetCore.Mvc/Formatters/SystemTextStreamedJsonInputFormatter.cs\">demo repository</a>.</p>\n<p>Now that we have the formatter, we can create a setup for <code>MvcOptions</code> that will replace the <code>SystemTextJsonInputFormatter</code> with a custom implementation.</p>\n<pre><code class=\"lang-cs\">internal class SystemTextStreamedJsonMvcOptionsSetup : IConfigureOptions&lt;MvcOptions&gt;\n{\n    private readonly IOptions&lt;JsonOptions&gt;? _jsonOptions;\n    private readonly ILogger&lt;SystemTextJsonInputFormatter&gt; _inputFormatterLogger;\n\n    public SystemTextStreamedJsonMvcOptionsSetup(\n        IOptions&lt;JsonOptions&gt;? jsonOptions,\n        ILogger&lt;SystemTextJsonInputFormatter&gt; inputFormatterLogger)\n    {\n        _jsonOptions = jsonOptions;\n        _inputFormatterLogger = inputFormatterLogger;\n    }\n\n    public void Configure(MvcOptions options)\n    {\n        options.InputFormatters.RemoveType&lt;SystemTextJsonInputFormatter&gt;();\n        options.InputFormatters.Add(\n            new SystemTextStreamedJsonInputFormatter(_jsonOptions?.Value, _inputFormatterLogger)\n        );\n    }\n}\n</code></pre>\n<p>A small convenience method to register that setup.</p>\n<pre><code class=\"lang-cs\">internal static class SystemTextStreamedJsonMvcBuilderExtensions\n{\n    public static IMvcBuilder AddStreamedJson(this IMvcBuilder builder)\n    {\n        ...\n\n        builder.Services.AddSingleton\n            &lt;IConfigureOptions&lt;MvcOptions&gt;, SystemTextStreamedJsonMvcOptionsSetup&gt;();\n\n        return builder;\n    }\n}\n</code></pre>\n<p>And we can configure the application to use it.</p>\n<pre><code class=\"lang-cs\">public class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddControllers()\n            ...\n            .AddStreamedJson();\n\n        ...\n    }\n\n    ...\n}\n</code></pre>\n<p>It gets the job done. As the formatter for NDJSON expects a different content type (<code>application/x-ndjson</code>) the content negotiation also works and I could asynchronously stream NDJSON and JSON to the same endpoint. Honestly, I think it's cool.</p>\n<h2 id=\"afterthoughts\">Afterthoughts</h2>\n<p>One question that arises at this point is \"Which one to use?\". I prefer NDJSON here. Why? Both require custom formatters and both formatters must use reflection. But in the case of NDJSON that is isolated only to the requests coming with a dedicated content type. To support async streamed JSON, the initial model type check has to happen on every request with JSON-related content type. Also I don't feel great with replacing the built-in formatter like that. On top of that, NDJSON enables clean cancellation as the endpoint is receiving separate JSON objects. In the case of JSON that is a single JSON collection of objects that will not be terminated properly and it will trip the deserialization.</p>\n<p>But this is just my opinion. You can grab the <a href=\"https://github.com/tpeczek/Demo.Ndjson.AsyncStreams\">demo</a> and play with it, or use one of my <a href=\"https://github.com/tpeczek/Ndjson.AsyncStreams\">NDJON packages</a> and experiment with it yourself. That's the best way to choose the right approach for your scenario.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2024/09/aspnet-core-9-and-iasyncenumerable.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-6028191154407920725",
      "Title": "Azure Functions Extensibility - Extensions and Isolated Worker Model",
      "PublishDate": "2024-03-05T09:22:00+00:00",
      "Summary": "<p>I've been exploring the subject of Azure Functions extensibility on this blog for quite some time. I've touched on subjects directly related to creating extensions and their anatomy, but also some peripheral ones.</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2018/11/azure-functions-20-extensibility.html\">Azure Functions Extensibility - Overview</a></li>\n<li><a href=\"https://www.tpeczek.com/2018/11/azure-functions-20-extensibility_20.html\">Azure Functions Extensibility - Triggers</a></li>\n<li><a href=\"https://www.tpeczek.com/2024/02/azure-functions-extensibility-runtime.html\">Azure Functions Extensibility - Runtime Scaling</a></li>\n<li><a href=\"https://www.tpeczek.com/2019/01/azure-functions-20-extensibility.html\">Azure Functions Extensibility - Extending Extensions</a></li>\n</ul>\n<p>I have always written from the perspective of the in-process model, but since 2020 there has been a continued evolution when it comes to the preferred model for .NET-based function apps. The isolated worker model, first introduced with .NET 5, has been gaining parity and becoming the leading vehicle for making new .NET versions available with Azure Functions. In August 2023 Microsoft <a href=\"https://techcommunity.microsoft.com/t5/apps-on-azure-blog/net-on-azure-functions-august-2023-roadmap-update/ba-p/3910098?wt.mc_id=DT-MVP-5002979\">announced</a> the intention for .NET 8 to be the last LTS release to receive in-process model support. So the question comes, does it invalidate all that knowledge about Azure Functions extensibility? The short answer is no. But before I go into details, I need to cover the common ground.</p>\n<h2 id=\"isolated-worker-model-in-a-nutshell\">Isolated Worker Model in a Nutshell</h2>\n<p>.NET was always a little bit special in Azure Functions. It shouldn't be a surprise. After all, it's Microsoft technology and there was a desire for the integration to be efficient and powerful. So even when Azure Functions v2 brought the separation between the host process and language worker process, .NET-based function apps were running in the host process. This had performance benefits (no communication between processes) and allowed .NET functions apps to leverage the full capabilities of the host, but started to become a bottleneck when the pace of changes in the .NET ecosystem accelerated. There were more and more conflicts between the assemblies that developers wanted to use in the apps and the ones used by the host. There was a delay in making new .NET versions available because the host had to be updated. Also, there were things that the app couldn't do because it was coupled with the host. Limitations like those were reasons for bringing Azure Functions .NET Worker to life.</p>\n<p>At the same time, Microsoft didn't want to take away all the benefits that .NET developers had when working with Azure Functions. The design had to take performance and developers' experience into account. So how does Azure Functions .NET Worker work? In simplification, it's an ASP.NET Core application that receives inputs and provides outputs to the host over gRPC (which is more performant than HTTP primitives used in the case of <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers?wt.mc_id=DT-MVP-5002979\">custom handlers</a>)</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8CLf3FJxWA0xgxjlIBwbAOdtzMWs_HBOM8WQOAdpuPzTaDTW1ERhg1Rx3HIkvdQg4jvRpC0pssO8mMyOf1lsyTyn0Ast_7P-GSyTVJf5BhCJSw4DP99Ne1XHzttYTihW3uaP-JqR0siejfUFoOea7k-La9qX54LKT_Q-cMdCPMKIaTUhuXc2blSIw7xw/s1600/azure-functions-extensibility-in-isolated-worker-model-01.png\" alt=\"Azure Functions .NET Worker overview\"></p>\n<p>The request and response payloads are also pretty well hidden. Developers have been given a new binding model with required attributes available through <code>*.Azure.Functions.Worker.Extensions.*</code> packages. But if the actual bindings activity happens in the host, what do those new packages provide? And what is their relation with the <code>*.Azure.WebJobs.Extensions.*</code> packages?</p>\n<h2 id=\"worker-extensions-and-webjobs-extensions\">Worker Extensions and WebJobs Extensions</h2>\n<p>The well-hidden truth is that the worker extension packages are just a bridge to the in-process extension packages. It means that if you want to create a new extension or understand how an existing one works, you should start with an extension for the in-process model. The worker extensions are mapped to the in-process ones through an assembly-level attribute, which takes the name of the package and version to be used as parameters.</p>\n<pre><code class=\"lang-cs\">[assembly: ExtensionInformation(\"RethinkDb.Azure.WebJobs.Extensions\", \"0.6.0\")]\n</code></pre>\n<p>The integration is quite seamless. During the build, the Azure Functions tooling will use NuGet to install the needed in-process extension package, it doesn't have to be referenced. Of course that has its drawbacks (tight coupling to a specific version and more challenges during debugging). So, the final layout of the packages can be represented as below.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhkM5Z3PUZ3_kLVx3pT303BnQDMVWkacivKfQPtxHeVX4nzBom2eEwClkc0KWt-kVnPg1K0CQmEfOecTPviKzSQPTan0tz9tNh6P7farURFBpZ6Bfvrob386Bko9o1Mml2TsCixHLSQ2tq8Lgmv9U6uSrjX__KD2teCDdteuqzl6_i-vvAkjOMsFLym0QI/s1600/azure-functions-extensibility-in-isolated-worker-model-02.png\" alt=\"Azure Functions .NET Worker Extensions and WebJobs Extensions relation overview\"></p>\n<p>What ensures the cooperation between those two packages running in two different processes are the binding attributes.</p>\n<h2 id=\"binding-attributes\">Binding Attributes</h2>\n<p>In the case of the in-process model extensions we have two types of attributes - one for bindings and one for trigger. In the case of the isolated worker model, there are three - for input binding, for output binding, and for trigger.</p>\n<pre><code class=\"lang-cs\">public class RethinkDbInputAttribute : InputBindingAttribute\n{\n    ...\n}\n</code></pre>\n<pre><code class=\"lang-cs\">public sealed class RethinkDbOutputAttribute : OutputBindingAttribute\n{\n    ...\n}\n</code></pre>\n<pre><code class=\"lang-cs\">public sealed class RethinkDbTriggerAttribute : TriggerBindingAttribute\n{\n    ...\n}\n</code></pre>\n<p>The isolated worker model attributes are used in two ways. One is for developers, who use them to decorate their functions and provide needed settings. The other is for the worker, which uses them as data transfer objects. They are being serialized and transferred as metadata. On the host side, they are being deserialized to the corresponding in-process model extension attribute. The input and output attributes will be deserialized to the binding attribute, and the trigger will be deserialized to the trigger. This means that we need to ensure that the names of properties which we want to support are matching.</p>\n<p>Implementing the attributes and decorating the functions with them is all we need to make it work. This will give us support for POCOs as values (the host and worker will take care of serialization, transfer over gRPC, and deserialization). But what if we want something more than POCO?</p>\n<h2 id=\"beyond-poco-inputs-with-converters\">Beyond POCO Inputs With Converters</h2>\n<p>It's quite common for in-process extensions to support binding data provided using types from specific service SDKs (for example <code>CosmosClient</code> in the case of Azure Cosmos DB). That kind of binding data is not supported out-of-the-box by isolated worker extensions as they can't be serialized and transferred. But there is a way for isolated worker extensions to go beyond POCOs - input converters.</p>\n<p>Input converters are classes that implement the <code>IInputConverter</code> interface. This interface defines a single method, which is supposed to return a conversation result. Conversation result can be one of the following:</p>\n<ul>\n<li><code>Unhandled</code> (the converter did not act on the input)</li>\n<li><code>Succeeded</code> (conversion was successful and the result is included)</li>\n<li><code>Failed</code></li>\n</ul>\n<p>The converter should check if it's being used with an extension it supports (the name that has been used for isolated extensions registration will be provided as part of the model binding data) and if the incoming content is in supported format. The converter can also be decorated with multiple <code>SupportedTargetType</code> attributes to narrow its scope.</p>\n<p>Below is a sample template for an input converter.</p>\n<pre><code class=\"lang-cs\">[SupportsDeferredBinding]\n[SupportedTargetType(typeof(...))]\n[SupportedTargetType(typeof(...))]\ninternal class RethinkDbConverter : IInputConverter\n{\n    private const string RETHINKDB_EXTENSION_NAME = \"RethinkDB\";\n    private const string JSON_CONTENT_TYPE = \"application/json\";\n\n    ...\n\n    public RethinkDbConverter(...)\n    {\n        ...\n    }\n\n    public async ValueTask&lt;ConversionResult&gt; ConvertAsync(ConverterContext context)\n    {\n        ModelBindingData modelBindingData = context?.Source as ModelBindingData;\n\n        if (modelBindingData is null)\n        {\n            return ConversionResult.Unhandled();\n        }\n\n        try\n        {\n            if (modelBindingData.Source is not RETHINKDB_EXTENSION_NAME)\n            {\n                throw new InvalidOperationException($\"Unexpected binding source.\");\n            }\n\n            if (modelBindingData.ContentType is not JSON_CONTENT_TYPE)\n            {\n                throw new InvalidOperationException($\"Unexpected content-type.\");\n            }\n\n            object result = context.TargetType switch\n            {\n                // Here you can use modelBindingData.Content,\n                // any injected services, etc.\n                // to prepare the value.\n                ...\n            };\n\n\n            return ConversionResult.Success(result);\n        }\n        catch (Exception ex)\n        {\n            return ConversionResult.Failed(ex);\n        }\n    }\n}\n</code></pre>\n<p>Input converters can be applied to input and trigger binding attributes by simply decorating them with an attribute (we should also define the fallback behavior policy).</p>\n<pre><code class=\"lang-cs\">[InputConverter(typeof(RethinkDbConverter))]\n[ConverterFallbackBehavior(ConverterFallbackBehavior.Default)]\npublic class RethinkDbInputAttribute : InputBindingAttribute\n{\n    ...\n}\n</code></pre>\n<p>Adding input converters to the extensions moves some of the logic from host to worker. It may mean that the worker will be now establishing connections to the services or performing other operations. This will most likely create a need to register some dependencies, read configuration, and so on. Such things are best done at a function startup.</p>\n<h2 id=\"participating-in-the-function-app-startup\">Participating in the Function App Startup</h2>\n<p>Extensions for the isolated worker model can implement a startup hook. It can be done by creating a public class with a parameterless constructor, that derives from <code>WorkerExtensionStartup</code>. This class also has to be registered through an assembly-level attribute. Now we can override the <code>Configure</code> method and register services and middlewares. The mechanic is quite similar to its equivalent for in-process extension.</p>\n<pre><code class=\"lang-cs\">[assembly: WorkerExtensionStartup(typeof(RethinkDbExtensionStartup))]\n\nnamespace Microsoft.Azure.Functions.Worker\n{\n    public class RethinkDbExtensionStartup : WorkerExtensionStartup\n    {\n        public override void Configure(IFunctionsWorkerApplicationBuilder applicationBuilder)\n        {\n            if (applicationBuilder == null)\n            {\n                throw new ArgumentNullException(nameof(applicationBuilder));\n            }\n\n            ...\n        }\n    }\n}\n</code></pre>\n<h2 id=\"the-conclusion\">The Conclusion</h2>\n<p>The isolated worker model doesn't invalidate what we know about the Azure Functions extensions, on the contrary, it builds another layer on top of that knowledge. Sadly, there are limitations in the supported data types for bindings which come from the serialization and transfer of data between the host and the worker. Still, in my opinion, the benefits of the new model can outweigh those limitations.</p>\n<p>If you are looking for a working sample to learn and explore, you can find one <a href=\"https://github.com/tpeczek/RethinkDb.Azure\">here</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2024/03/azure-functions-extensibility.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-5557502819831846633",
      "Title": "Azure Functions Extensibility - Runtime Scaling",
      "PublishDate": "2024-02-22T12:28:00+00:00",
      "Summary": "<p>In the past, I've written posts that explored the core aspects of creating Azure Functions extensions:</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2018/11/azure-functions-20-extensibility.html\">Azure Functions Extensibility - Overview</a></li>\n<li><a href=\"https://www.tpeczek.com/2018/11/azure-functions-20-extensibility_20.html\">Azure Functions Extensibility - Triggers</a></li>\n</ul>\n<p>In this post, I want to dive into how extensions implement support for the Azure Functions scaling model.</p>\n<p>The Azure Functions scaling model (as one can expect) has evolved over the years. The first model is incremental scaling handled by the <em>scale controller</em> (a dedicated Azure Functions component). The scale controller monitors the rate of incoming events and makes magical (following the rule that every sophisticated enough technology is indistinguishable from magic) decisions about whether to add or remove a worker. The workers count can change only by one and at a specific rate. This mechanism has some limitations.</p>\n<p>The first problem is the inability to scale in network-isolated deployments. In such a scenario, the scale controller doesn't have access to the services and can't monitor events - only the function app (and as a consequence the extensions) could. As a result, toward the end of 2019, the SDK provided support for extensions to vote in scale in and scale out decisions.</p>\n<p>The second problem is the scaling process clarity and rate. Change by one worker at a time driven by complex heuristics is not perfect. This led to the introduction of target-based scaling at the beginning of 2023. Extensions become able to implement their own algorithms for requesting a specific number of workers and scale up by four instances at a time.</p>\n<p>So, how do those two mechanisms work and can be implemented? To answer this question I'm going to describe them from two perspectives. One will be how it is being done by a well-established extension - Azure Cosmos DB bindings for Azure Functions. The other will be a sample implementation in my extension that I've used previously while writing on Azure Functions extensibility - <a href=\"https://github.com/tpeczek/RethinkDb.Azure\">RethinkDB bindings for Azure Functions</a>.</p>\n<p>But first things first. Before we can talk about the actual logic driving the scaling behaviors, we need the right SDK and the right classes in place.</p>\n<h2 id=\"runtime-and-target-based-scaling-support-in-azure-webjobs-sdk\">Runtime and Target-Based Scaling Support in Azure WebJobs SDK</h2>\n<p>As I've mentioned above, support for extensions to participate in the scaling model has been introduced gradually to the Azure WebJobs SDK. At the same time, the team makes an effort to introduce changes in a backward-compatible manner. Support for voting in scaling decisions came in version 3.0.14 and for target-based scaling in 3.0.36, but you can have an extension that is using version 3.0.0 and it will work on the latest Azure Functions runtime. You can also have an extension that uses the latest SDK and doesn't implement the scalability mechanisms. You need to know that they are there and choose to utilize them.</p>\n<p>This is reflected in the official extensions as well. The Azure Cosmos DB extension has implemented support for runtime scaling in version 3.0.5 and target-based scaling in 4.1.0 (you can find some tables that are gathering version numbers, for example in the section about <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/functions-networking-options?wt.mc_id=DT-MVP-5002979#premium-plan-with-virtual-network-triggers\">virtual network triggers</a>). This means, that if your function app is using lower versions of extensions, it won't benefit from these capabilities.</p>\n<p>So, regardless if you're implementing your extension or you're just using an existing one, the versions of dependencies matter. However, if you're implementing your extension, you will need some classes.</p>\n<h2 id=\"classes-needed-to-implement-scaling-support\">Classes Needed to Implement Scaling Support</h2>\n<p>The Azure Functions scaling model revolves around triggers. After all, it's their responsibility to handle the influx of events. As you may know (for example from my previous post 😉), the heart of a trigger is a listener. This is where all the heavy lifting is being done and this is where we can inform the runtime that our trigger supports scalability features. We can do so by implementing two interfaces: <code>IScaleMonitorProvider</code> and <code>ITargetScalerProvider</code>. They are respectively tied to runtime scaling and target-based scaling. If they are implemented, the runtime will use the properties they define to obtain the actual logic implementations.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTriggerListener : IListener, IScaleMonitorProvider, ITargetScalerProvider\n{\n    ...\n\n    private readonly IScaleMonitor&lt;RethinkDbTriggerMetrics&gt; _rethinkDbScaleMonitor;\n    private readonly ITargetScaler _rethinkDbTargetScaler;\n\n    ...\n\n    public IScaleMonitor GetMonitor()\n    {\n        return _rethinkDbScaleMonitor;\n    }\n\n    public ITargetScaler GetTargetScaler()\n    {\n        return _rethinkDbTargetScaler;\n    }\n}\n</code></pre>\n<p>From the snippets above you can notice one more class - <code>RethinkDbTriggerMetrics</code>. It derives from the <code>ScaleMetrics</code> class and is used for capturing the values on which the decisions are being made.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTriggerMetrics : ScaleMetrics\n{\n    ...\n}\n</code></pre>\n<p>The <code>IScaleMonitor</code> implementation contributes to the runtime scaling part of the scaling model. Its responsibilities are to provide snapshots of the metrics and to vote in the scaling decisions.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbScaleMonitor : IScaleMonitor&lt;RethinkDbTriggerMetrics&gt;\n{\n    public ScaleMonitorDescriptor Descriptor =&gt; ...;\n\n    public Task&lt;RethinkDbTriggerMetrics&gt; GetMetricsAsync()\n    {\n        ...\n    }\n\n    Task&lt;ScaleMetrics&gt; IScaleMonitor.GetMetricsAsync()\n    {\n        ...\n    }\n\n    public ScaleStatus GetScaleStatus(ScaleStatusContext&lt;RethinkDbTriggerMetrics&gt; context)\n    {\n        ...\n    }\n\n    public ScaleStatus GetScaleStatus(ScaleStatusContext context)\n    {\n        ...\n    }\n}\n</code></pre>\n<p>The <code>ITargetScaler</code> implementation is responsible for the target-based scaling. It needs to focus only on one thing - calculating the desired worker count.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTargetScaler : ITargetScaler\n{\n    public TargetScalerDescriptor TargetScalerDescriptor =&gt; ...;\n\n    public Task&lt;TargetScalerResult&gt; GetScaleResultAsync(TargetScalerContext context)\n    {\n        ...\n    }\n}\n</code></pre>\n<p>As both of these implementations are tightly tied with a specific trigger, it is typical to instantiate them in the listener constructor.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTriggerListener : IListener, IScaleMonitorProvider, ITargetScalerProvider\n{\n    ...\n\n    public RethinkDbTriggerListener(...)\n    {\n        ...\n\n        _rethinkDbScaleMonitor = new RethinkDbScaleMonitor(...);\n        _rethinkDbTargetScaler = new RethinkDbTargetScaler(...);\n    }\n\n    ...\n}\n</code></pre>\n<p>We are almost ready to start discussing the details of scaling logic, but before we do that, we need to talk about gathering metrics. The decisions need to be based on something.</p>\n<h2 id=\"gathering-metrics\">Gathering Metrics</h2>\n<p>Yes, the decisions need to be based on something and the quality of the decisions will depend on the quality of metrics you can gather. So a lot depends on how well the service, for which the extension is being created, is prepared to be monitored.</p>\n<p>Azure Cosmos DB is well prepared. The Azure Cosmos DB bindings for Azure Functions implement the trigger on top of the Azure Cosmos DB change feed processor and use the <a href=\"https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-use-change-feed-estimator?wt.mc_id=DT-MVP-5002979\">change feed estimator</a> to gather the metrics. This gives access to quite an accurate estimate of the remaining work. As an additional metric, the extension is gathers the number of leases for the container that the trigger is observing.</p>\n<p>RethinkDB is not so well prepared. It seems like change feed provides only one metric - buffered items count.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTriggerMetrics : ScaleMetrics\n{\n    public int BufferedItemsCount { get; set; }\n}\n</code></pre>\n<p>Also, the metric can only be gathered while iterating the change feed. This forces an intermediary between the listener and the scale monitor (well you could use the scale monitor directly, but it seemed ugly to me).</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbMetricsProvider\n{\n    public int CurrentBufferedItemsCount { get; set; }\n\n    public RethinkDbTriggerMetrics GetMetrics()\n    {\n        return new RethinkDbTriggerMetrics { BufferedItemsCount = CurrentBufferedItemsCount };\n    }\n}\n</code></pre>\n<p>Now the same instance of this intermediary can be used by the listener to provide the value.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTriggerListener : IListener, IScaleMonitorProvider, ITargetScalerProvider\n{\n    ...\n\n    private readonly RethinkDbMetricsProvider _rethinkDbMetricsProvider;\n\n    ...\n\n    public RethinkDbTriggerListener(...)\n    {\n        ...\n\n        _rethinkDbMetricsProvider = new RethinkDbMetricsProvider();\n        _rethinkDbScaleMonitor = new RethinkDbScaleMonitor(..., _rethinkDbMetricsProvider);\n\n        ...\n    }\n\n    ...\n\n    private async Task ListenAsync(CancellationToken listenerStoppingToken)\n    {\n        ...\n\n        while (!listenerStoppingToken.IsCancellationRequested\n               &amp;&amp; (await changefeed.MoveNextAsync(listenerStoppingToken)))\n        {\n            _rethinkDbMetricsProvider.CurrentBufferedItemsCount = changefeed.BufferedItems.Count;\n            ...\n        }\n\n        ...\n    }\n}\n</code></pre>\n<p>And the scale monitor to read it.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbScaleMonitor : IScaleMonitor&lt;RethinkDbTriggerMetrics&gt;\n{\n    ...\n\n    private readonly RethinkDbMetricsProvider _rethinkDbMetricsProvider;\n\n    ...\n\n    public RethinkDbScaleMonitor(..., RethinkDbMetricsProvider rethinkDbMetricsProvider)\n    {\n        ...\n\n        _rethinkDbMetricsProvider = rethinkDbMetricsProvider;\n\n        ...\n    }\n\n    public Task&lt;RethinkDbTriggerMetrics&gt; GetMetricsAsync()\n    {\n        return Task.FromResult(_rethinkDbMetricsProvider.GetMetrics());\n    }\n\n    Task&lt;ScaleMetrics&gt; IScaleMonitor.GetMetricsAsync()\n    {\n        return Task.FromResult((ScaleMetrics)_rethinkDbMetricsProvider.GetMetrics());\n    }\n\n    ...\n}\n</code></pre>\n<p>We have the metrics now, it is finally time to make some decisions.</p>\n<h2 id=\"voting-to-scale-in-or-scale-out\">Voting to Scale In or Scale Out</h2>\n<p>An extension can cast one of three votes: to scale out, to scale in, or neutral. This is where the intimate knowledge of the service that the extension is created for comes into play.</p>\n<p>As I've already written, the Azure Cosmos DB change feed processor is well prepared to gather metrics about it. It is also well prepared to be consumed in a <a href=\"https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-processor?wt.mc_id=DT-MVP-5002979#dynamic-scaling\">scalable</a> manner. It can distribute the work among multiple compute instances, by balancing the number of leases owned by each compute instance. It will also adjust the number of leases based on throughput and storage. This is why the Azure Cosmos DB bindings are tracking the number of leases as one of the metrics - it's an upper limit for the number of workers. So the extension utilizes all this knowledge and employs the following algorithm to cast a vote:</p>\n<ol>\n<li>If there are no metrics gathered yet, cast a neutral vote.</li>\n<li>If the current number of leases is greater than the number of workers, cast a scale-in vote.</li>\n<li>If there are less than five metrics samples, cast a neutral vote.</li>\n<li>If the ratio of workers to remaining items is less than 1 per 1000, cast a scale-out vote.</li>\n<li>If there are constantly items waiting to be processed and the number of workers is smaller than the number of leases, cast a scale-out vote.</li>\n<li>If the trigger source has been empty for a while, cast a scale-in vote.</li>\n<li>If there has been a continuous increase across the last five samples in items to be processed, cast a scale-out vote.</li>\n<li>If there has been a continuous decrease across the last five samples in items to be processed, cast a scale-in vote.</li>\n<li>If none of the above has happened, cast a neutral vote.</li>\n</ol>\n<p>RethinkDB is once again lacking here. It looks like its change feed is not meant to be processed in parallel at all. This leads to an interesting edge case, where we never want to scale beyond a single instance.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbScaleMonitor : IScaleMonitor&lt;RethinkDbTriggerMetrics&gt;\n{\n    ...\n\n    public ScaleStatus GetScaleStatus(ScaleStatusContext&lt;RethinkDbTriggerMetrics&gt; context)\n    {\n        return GetScaleStatus(\n            context.WorkerCount,\n            context.Metrics?.ToArray()\n        );\n    }\n\n    public ScaleStatus GetScaleStatus(ScaleStatusContext context)\n    {\n        return GetScaleStatus(\n            context.WorkerCount, \n            context.Metrics?.Cast&lt;RethinkDbTriggerMetrics&gt;().ToArray()\n        );\n    }\n\n    private ScaleStatus GetScaleStatus(int workerCount, RethinkDbTriggerMetrics[] metrics)\n    {\n        ScaleStatus status = new ScaleStatus\n        {\n            Vote = ScaleVote.None\n        };\n\n        // RethinkDB change feed is not meant to be processed in paraller.\n        if (workerCount &gt; 1)\n        {\n            status.Vote = ScaleVote.ScaleIn;\n\n            return status;\n        }\n\n        return status;\n    }\n}\n</code></pre>\n<h2 id=\"requesting-desired-number-of-workers\">Requesting Desired Number of Workers</h2>\n<p>Being able to tell if you want more workers or less is great, being able to tell how many workers you want is even better. Of course, there is no promise the extension will get the requested number (even in the case of target-based scaling the scaling happens with a maximum rate of four instances at a time), but it's better than increasing and decreasing by one instance. Extensions can also use this mechanism to participate in <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/functions-concurrency?wt.mc_id=DT-MVP-5002979#dynamic-concurrency\">dynamic concurrency</a>.</p>\n<p>This is exactly what the Azure Cosmos DB extension is doing. It divides the number of remaining items by the value of the <code>MaxItemsPerInvocation</code> trigger setting (the default is 100). The result is capped by the number of leases and that's the desired number of workers.</p>\n<p>We already know  that in the case of RethinkDB, it's even simpler - we always want just one worker.</p>\n<pre><code class=\"lang-cs\">internal class RethinkDbTargetScaler : ITargetScaler\n{\n    ...\n\n    public Task&lt;TargetScalerResult&gt; GetScaleResultAsync(TargetScalerContext context)\n    {\n        return Task.FromResult(new TargetScalerResult\n        {\n            TargetWorkerCount = 1\n        });\n    }\n}\n</code></pre>\n<p>That's it. The runtime scaling implementation is now complete.</p>\n<h2 id=\"consequences-of-runtime-scaling-design\">Consequences of Runtime Scaling Design</h2>\n<p>Creating extensions for Azure Functions is not something commonly done. Still, there is a value in understanding their anatomy and how they work as it is often relevant also when you are creating function apps.</p>\n<p>The unit of scale for Azure Functions is the entire functions app. At the same time, the scaling votes and the desired number of workers are delivered at a trigger level. This means that you need to be careful when creating function apps with multiple triggers. If the triggers will be aiming for completely different scaling decisions it may lead to undesired scenarios. For example, one trigger may constantly want to scale in, while another will want to scale out. As you could notice throughout this post, some triggers have hard limits on how far they can scale to not waste resources (even two Azure Cosmos DB triggers may have a different upper limit because the lease containers they are attached to will have different numbers of leases available). This all should be taken into account while designing the function app and trying to foresee how it will scale.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2024/02/azure-functions-extensibility-runtime.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-5098970047446691928",
      "Title": "Experimenting With .NET & WebAssembly - Running .NET Based Slight Application On WASM/WASI Node Pool in AKS",
      "PublishDate": "2024-01-09T11:09:00+00:00",
      "Summary": "<p>A year ago I wrote about <a href=\"https://www.tpeczek.com/2022/12/experimenting-with-net-webassembly.html\">running a .NET based Spin application on a WASI node pool in AKS</a>. Since then the support for WebAssembly in AKS hasn't changed. We still have the same preview, supporting the same versions of two ContainerD shims: Spin and Slight (a.k.a. SpiderLightning). So why am I coming back to the subject? The broader context has evolved.</p>\n<p>With WASI preview 2, the ecosystem is embracing the <a href=\"https://github.com/WebAssembly/component-model\">component model</a> and standardized APIs. When I was experimenting with Spin, I leveraged WAGI (WebAssembly Gateway Interface) which allowed me to be ignorant about the Wasm runtime context. Now I want to change that, cross the barrier and dig into direct interoperability between .NET and Wasm.</p>\n<p>Also, regarding the mentioned APIs, one of the emerging ones is <a href=\"https://github.com/WebAssembly/wasi-cloud-core\"><code>wasi-cloud-core</code></a> which aims to provide a generic way for WASI applications to interact with services. This proposal is not yet standardized but it has an experimental host implementation which happens to be Slight. By running a .NET based Slight application I want to get a taste of what that API might bring.</p>\n<p>Last but not least, .NET 8 has brought a new way of building .NET based Wasm applications with a <code>wasi-experimental</code> workload. I want to build something with it and see where the .NET support for WASI is heading.</p>\n<p>So, this \"experiment\" has multiple angles and brings together a bunch of different things. How am I going to start? In the usual way, by creating a project.</p>\n<h2 id=\"creating-a-net-8-wasi-wasm-project\">Creating a .NET 8 <code>wasi-wasm</code> Project</h2>\n<p>The <code>wasi-experimental</code> workload is optional, so we need to install it before we can create a project.</p>\n<pre><code class=\"lang-ps\">dotnet workload install wasi-experimental\n</code></pre>\n<p>It also doesn't bundle the <a href=\"https://github.com/WebAssembly/wasi-sdk\">WASI SDK</a> (the <a href=\"https://github.com/SteveSandersonMS/dotnet-wasi-sdk\">WASI SDK for .NET 7</a> did), so we have to install it ourselves. The releases of WASI SDK available on GitHub contain binaries for different platforms. All you need to do is download the one appropriate for yours, extract it, and create the WASI_SDK_PATH environment variable pointing to the output. The version I'll be using here is <a href=\"https://github.com/WebAssembly/wasi-sdk/releases/tag/wasi-sdk-20\">20.0</a>.</p>\n<p>With the prerequisites in place, we can create the project.</p>\n<pre><code class=\"lang-ps\">dotnet new wasiconsole -o Demo.Wasm.Slight\n</code></pre>\n<p>Now, if you run <code>dotnet build</code> and inspect the output folder, you can notice that it contains <em>Demo.Wasm.Slight.dll</em>, other managed DLLs, and <em>dotnet.wasm</em>. This is the default output, where the <em>dotnet.wasm</em> is responsible for loading the Mono runtime and then loading the functionality from DLLs. This is not what we want. We want a single file. To achieve that we need to modify the project file by adding the <code>WasmSingleFileBundle</code> property (in my opinion this should be the default).</p>\n<pre><code class=\"lang-xml\">&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;\n  &lt;PropertyGroup&gt;\n    ...\n    &lt;WasmSingleFileBundle&gt;true&lt;/WasmSingleFileBundle&gt;\n  &lt;/PropertyGroup&gt;\n&lt;/Project&gt;\n</code></pre>\n<p>If you run <code>dotnet build</code> after this modification, you will find <em>Demo.Wasm.Slight.wasm</em> in the output. Exactly what we want.</p>\n<p>But, before we can start implementing the actual application, we need to work on the glue between the .NET and WebAssembly so we can interact with the APIs provided by Slight from C#.</p>\n<h2 id=\"from-webassembly-idl-to-c-\">From WebAssembly IDL To C#</h2>\n<p>The imports and exports in WASI APIs are described in <a href=\"https://github.com/WebAssembly/component-model/blob/main/design/mvp/WIT.md\">Wasm Interface Type (WIT)</a> format. This format is an IDL which is a foundation behind tooling for the WebAssembly Component Model.</p>\n<p>WIT aims at being a developer-friendly format, but writing bindings by hand is not something that developers expect. This is where <a href=\"https://github.com/bytecodealliance/wit-bindgen\"><code>wit-bindgen</code></a> comes into the picture. It's a (still young) binding generator for languages that are compiled into WebAssembly. It currently supports languages like Rust, C/C++, or Java. The <a href=\"https://github.com/bytecodealliance/wit-bindgen/issues/713\">C# support</a> is being actively worked on and we can expect that at some point getting C# bindings will be as easy as running a single command (or even simpler as Steve Sanderson is already <a href=\"https://github.com/SteveSandersonMS/wasm-component-sdk\">experimenting with making it part of the toolchain</a>) but for now, it's too limited and we will have to approach things differently. What we can use is the C/C++ support.</p>\n<p>There is one more challenge on our way. The current version of <code>wit-bindgen</code> is meant for WASI preview 2. Meanwhile, a lot of existing WIT definitions and WASI tooling around native languages are using WASI preview 1. This is exactly the case when it comes to the Slight implementation available in the AKS preview. To handle that we need an old (and I mean old) version of <code>wit-bindgen</code>. I'm using version <a href=\"https://github.com/bytecodealliance/wit-bindgen/releases/tag/v0.2.0\">v0.2.0</a>. Once you install it, you can generate C/C++ bindings for the desired imports and exports. Slight in version 0.1.0 provides a <a href=\"https://github.com/deislabs/spiderlightning/blob/v0.1.0/docs/primer.md#spiderlightning-capabilities\">couple of capabilities</a>, but I've decided to start with just one, the HTTP Server. That means we need imports for <code>http.wit</code> and exports for <code>http-handler.wit</code>.</p>\n<pre><code class=\"lang-ps\">wit-bindgen c --import ./wit/http.wit --out-dir ./native/\nwit-bindgen c --export ./wit/http-handler.wit --out-dir ./native/\n</code></pre>\n<p>Once we have the C/C++ bindings, we can configure the build to include those files in the arguments passed to Clang. For this purpose, we can use the <a href=\"https://learn.microsoft.com/en-us/visualstudio/msbuild/customize-your-build#choose-between-adding-properties-to-a-props-or-targets-file\"><code>.targets</code></a> file.</p>\n<pre><code class=\"lang-xml\">&lt;Project&gt;\n  &lt;ItemGroup&gt;\n    &lt;_WasmNativeFileForLinking Include=\"$(MSBuildThisFileDirectory)\\..\\native\\*.c\" /&gt;\n  &lt;/ItemGroup&gt;\n&lt;/Project&gt;\n</code></pre>\n<p>We also need to implement the C# part of the interop. For the main part, it's like any other interop you might have ever done. You can use generators or ask for help from a friendly AI assistant and it will get you through the code for types and functions. However, there is one specific catch. The <code>DllImportAttribute</code> requires passing a library name for the P/Invoke generator, but we have no library. The solution is as simple as surprising (at least to me), we can provide the name of any library that the P/Invoke generator knows about.</p>\n<pre><code class=\"lang-cs\">internal static class HttpServer\n{\n    // Any library name that P/Invoke generator knows\n    private const string LIBRARY_NAME = \"libSystem.Native\";\n\n    [DllImport(LIBRARY_NAME)]\n    internal static extern unsafe void http_server_serve(WasiString address,\n        uint httpRouterIndex, out WasiExpected&lt;uint&gt; ret0);\n\n    [DllImport(LIBRARY_NAME)]\n    internal static extern unsafe void http_server_stop(uint httpServerIndex,\n        out WasiExpected&lt;uint&gt; ret0);\n}\n</code></pre>\n<p>With all the glue in place, we can start implementating the HTTP server capability.</p>\n<h2 id=\"implementing-the-slight-http-server-capability\">Implementing the Slight HTTP Server Capability</h2>\n<p>In order for a Slight application to start accepting HTTP requests, the application needs to call the <code>http_server_serve</code> function to which it must provide the address it wants to listen on and the index of a router that defines the supported routes. I've decided to roll out to simplest implementation I could think of to start testing things out - the <code>HttpRouter</code> and <code>HttpServer</code> classes which only allow for calling the serve function (no support for routing).</p>\n<pre><code class=\"lang-cs\">internal class HttpRouter\n{\n    private uint _index;\n\n    public uint Index =&gt; _index;\n\n    private HttpRouter(uint index)\n    {\n        _index = index;\n    }\n\n    public static HttpRouter Create()\n    {\n        http_router_new(out WasiExpected&lt;uint&gt; expected);\n\n        if (expected.IsError)\n        {\n            throw new Exception(expected.Error?.ErrorWithDescription.ToString());\n        }\n\n        return new HttpRouter(expected.Result.Value);\n    }\n}\n\ninternal static class HttpServer\n{\n    private static uint? _index;\n\n    public static void Serve(string address)\n    {\n        if (_index.HasValue)\n        {\n            throw new Exception(\"The server is already running!\");\n        }\n\n        HttpRouter router = HttpRouter.Create();\n        http_server_serve(\n            WasiString.FromString(address),\n            router.Index,\n            out WasiExpected&lt;uint&gt; expected\n        );\n\n        if (expected.IsError)\n        {\n            throw new Exception(expected.Error?.ErrorWithDescription.ToString());\n        }\n\n        _index = expected.Result;\n    }\n}\n</code></pre>\n<p>This allowed me to implement a simplistic application, a one-liner.</p>\n<pre><code class=\"lang-cs\">HttpServer.Serve(\"0.0.0.0:80\");\n</code></pre>\n<p>It worked! Every request results in 404, because there is no routing, but it worked. So, how to add support for routing?</p>\n<p>The <code>http_router_*</code> functions for defining routes expect two strings - one for the route and one for the handler. This suggests that the handler should be an exported symbol that Slight will be able to call. I went through the bindings exported for <code>http-handler.wit</code> and I've found a function that is being exported as <code>handle-http</code>. That function seems to be what we are looking for. It performs transformations to/from the request and response objects and calls a <code>http_handler_handle_http</code> function which has only a definition. So it looks like the <code>http_handler_handle_http</code> implementation is a place for the application logic. To test this theory, I've started by implementing a simple route registration method.</p>\n<pre><code class=\"lang-cs\">internal class HttpRouter\n{\n    ...\n\n    private static readonly WasiString REQUEST_HANDLER = WasiString.FromString(\"handle-http\");\n\n    ...\n\n    public HttpRouter RegisterRoute(HttpMethod method, string route)\n    {\n        WasiExpected&lt;uint&gt; expected;\n\n        switch (method)\n        {\n            case HttpMethod.GET:\n                http_router_get(_index, WasiString.FromString(route), REQUEST_HANDLER,\n                    out expected);\n                break;\n            case HttpMethod.PUT:\n                http_router_put(_index, WasiString.FromString(route), REQUEST_HANDLER,\n                    out expected);\n                break;\n            case HttpMethod.POST:\n                http_router_post(_index, WasiString.FromString(route), REQUEST_HANDLER,\n                    out expected);\n                break;\n            case HttpMethod.DELETE:\n                http_router_delete(_index, WasiString.FromString(route), REQUEST_HANDLER,\n                    out expected);\n                break;\n            default:\n                throw new NotSupportedException($\"Method {method} is not supported.\");\n        }\n\n        if (expected.IsError)\n        {\n            throw new Exception(expected.Error?.ErrorWithDescription.ToString());\n        }\n\n        return new HttpRouter(expected.Result.Value)\n    }\n\n    ...\n}\n</code></pre>\n<p>Next I've registered some catch-all routes and implemented the <code>HandleRequest</code> method as the .NET handler. It would still return a 404, but it would be mine 404.</p>\n<pre><code class=\"lang-cs\">internal static class HttpServer\n{\n    ...\n\n    public static void Serve(string address)\n    {\n        ...\n\n        HttpRouter router = HttpRouter.Create()\n                            .RegisterRoute(HttpMethod.GET, \"/\")\n                            .RegisterRoute(HttpMethod.GET, \"/*\");\n        http_server_serve(\n            WasiString.FromString(address),\n            router.Index,\n            out WasiExpected&lt;uint&gt; expected\n        );\n\n        ...\n    }\n\n    private static unsafe void HandleRequest(ref HttpRequest request,\n        out WasiExpected&lt;HttpResponse&gt; result)\n    {\n        HttpResponse response = new HttpResponse(404);\n        response.SetBody($\"Handler Not Found ({request.Method} {request.Uri.AbsolutePath})\");\n\n        result = new WasiExpected&lt;HttpResponse&gt;(response);\n    }\n\n    ...\n}\n</code></pre>\n<p>It's time for some C. I went through the Mono WASI C driver and found two functions that looked the right tools for the job: <code>lookup_dotnet_method</code> and <code>mono_wasm_invoke_method_ref</code>. The implementation didn't seem overly complicated.</p>\n<pre><code class=\"lang-c\">#include &lt;string.h&gt;\n#include &lt;wasm/driver.h&gt;\n#include \"http-handler.h\"\n\nMonoMethod* handle_request_method;\n\nvoid mono_wasm_invoke_method_ref(MonoMethod* method, MonoObject** this_arg_in,\n                                 void* params[], MonoObject** _out_exc, MonoObject** out_result);\n\nvoid http_handler_handle_http(http_handler_request_t* req,\n                              http_handler_expected_response_error_t* ret0)\n{\n    if (!handle_request_method)\n    {\n        handle_request_method = lookup_dotnet_method(\n            \"Demo.Wasm.Slight\",\n            \"Demo.Wasm.Slight\",\n            \"HttpServer\",\n            \"HandleRequest\",\n        -1);\n    }\n\n    void* method_params[] = { req, ret0 };\n    MonoObject* exception;\n    MonoObject* result;\n    mono_wasm_invoke_method_ref(handle_request_method, NULL, method_params, &amp;exception, &amp;result);\n}\n</code></pre>\n<p>But it didn't work. The thrown exception suggested that the Mono runtime wasn't loaded. I went back to studying Mono to learn how it is being loaded. What I've learned is that during compilation a <code>_start()</code> function is being generated. This function performs the steps necessary to load the Mono runtime and wraps the entry point to the .NET code. I could call it, but this would mean going through the <code>Main</code> method and retriggering <code>HttpServer.Serve</code>, which was doomed to fail. I needed to go a level lower. By reading the code of the <code>_start()</code> function I've learned that it calls the <code>mono_wasm_load_runtime</code> function. Maybe I could as well?</p>\n<pre><code class=\"lang-c\">...\n\nint mono_runtime_loaded = 0;\n\n...\n\nvoid http_handler_handle_http(http_handler_request_t* req,\n                              http_handler_expected_response_error_t* ret0)\n{\n    if (!mono_runtime_loaded) {\n        mono_wasm_load_runtime(\"\", 0);\n\n        mono_runtime_loaded = 1;\n    }\n\n    ...\n}\n</code></pre>\n<p>Now it worked. But I wasn't out of the woods yet. What I've just learned meant that to provide dedicated handlers for routes I couldn't rely on registering dedicated methods as part of the <code>Main</code> method flow. I could only register the routes and the handlers needed to be discoverable later, in a new context, with static <code>HandleRequest</code> as the entry point. My thoughts went in the direction of a poor man's attribute-based routing, so I've started with an attribute for decorating handlers.</p>\n<pre><code class=\"lang-cs\">internal class HttpHandlerAttribute: Attribute\n{\n    public HttpMethod Method { get; }\n\n    public string Route { get; }\n\n    public HttpHandlerAttribute(HttpMethod method, string route)\n    {\n        Method = method;\n        Route = route;\n    }\n\n    ...\n}\n</code></pre>\n<p>A poor man's implementation of an attribute-based routing must have an ugly part and it is reflection. To register the routes (and later match them with handlers), the types must be scanned for methods with the attributes. In a production solution, it would be necessary to narrow the scan scope but as this is just a small demo I've decided to keep it simple and scan the whole assembly for static, public and non-public, methods decorated with the attribute. The code supports adding multiple attributes to a single method just because it's simpler than putting proper protections in place. As you can probably guess, I did override the <code>Equals</code> and <code>GetHashCode</code> implementations in the attribute to ensure it behaves nicely as a dictionary key.</p>\n<pre><code class=\"lang-cs\">internal class HttpRouter\n{\n    ...\n\n    private static readonly Type HTTP_HANDLER_ATTRIBUTE_TYPE = typeof(HttpHandlerAttribute);\n\n    private static Dictionary&lt;HttpHandlerAttribute, MethodInfo&gt;? _routes;\n\n    ...\n\n    private static void DiscoverRoutes()\n    {\n        if (_routes is null)\n        {\n            _routes = new Dictionary&lt;HttpHandlerAttribute, MethodInfo&gt;();\n\n            foreach (Type type in Assembly.GetExecutingAssembly().GetTypes())\n            {\n                foreach(MethodInfo method in type.GetMethods(BindingFlags.Static |\n                                                             BindingFlags.Public |\n                                                             BindingFlags.NonPublic))\n                {\n                    foreach (object attribute in method.GetCustomAttributes(\n                             HTTP_HANDLER_ATTRIBUTE_TYPE, false))\n                    {\n                        _routes.Add((HttpHandlerAttribute)attribute, method);\n                    }\n                }\n            }\n        }\n    }\n\n    ...\n}\n</code></pre>\n<p>With the reflection stuff (mostly) out of the way, I could implement a method that can be called to register all discovered routes and a method to invoke a handler for a route. This implementation is not \"safe\". I don't do any checks on the reflected <code>MethodInfo</code> to ensure that the method has a proper signature. After all, I can only hurt myself here.</p>\n<pre><code class=\"lang-cs\">internal class HttpRouter\n{\n    ...\n\n    internal HttpRouter RegisterRoutes()\n    {\n        DiscoverRoutes();\n\n        HttpRouter router = this;\n        foreach (KeyValuePair&lt;HttpHandlerAttribute, MethodInfo&gt; route in _routes)\n        {\n            router = router.RegisterRoute(route.Key.Method, route.Key.Route);\n        }\n\n        return router;\n    }\n\n    internal static HttpResponse? InvokeRouteHandler(HttpRequest request)\n    {\n        DiscoverRoutes();\n\n        HttpHandlerAttribute attribute = new HttpHandlerAttribute(request.Method,\n                                                                  request.Uri.AbsolutePath);\n        MethodInfo handler = _routes.GetValueOrDefault(attribute);\n\n        return (handler is null) ? null : (HttpResponse)handler.Invoke(null,\n                                                                     new object[] { request });\n    }\n\n    ...\n}\n</code></pre>\n<p>What remained was small modifications to the <code>HttpServer</code> to use the new methods.</p>\n<pre><code class=\"lang-cs\">internal static class HttpServer\n{\n    ...\n\n    public static void Serve(string address)\n    {\n        ...\n\n        HttpRouter router = HttpRouter.Create()\n                            .RegisterRoutes();\n        http_server_serve(\n            WasiString.FromString(address),\n            router.Index,\n            out WasiExpected&lt;uint&gt; expected\n        );\n\n        ...\n    }\n\n    private static unsafe void HandleRequest(ref HttpRequest request,\n        out WasiExpected&lt;HttpResponse&gt; result)\n    {\n        HttpResponse? response = HttpRouter.InvokeRouteHandler(request);\n\n        if (!response.HasValue)\n        {\n            response = new HttpResponse(404);\n            response.Value.SetBody(\n                $\"Handler Not Found ({request.Method} {request.Uri.AbsolutePath})\"\n            );\n        }\n\n        result = new WasiExpected&lt;HttpResponse&gt;(response.Value);\n    }\n\n    ...\n}\n</code></pre>\n<p>To test this out, I've created two simple handlers.</p>\n<pre><code class=\"lang-cs\">[HttpHandler(HttpMethod.GET, \"/hello\")]\ninternal static HttpResponse HandleHello(HttpRequest request)\n{\n    HttpResponse response = new HttpResponse(200);\n    response.SetHeaders(new[] { KeyValuePair.Create(\"Content-Type\", \"text/plain\") });\n    response.SetBody($\"Hello from Demo.Wasm.Slight!\");\n\n    return response;\n}\n\n[HttpHandler(HttpMethod.GET, \"/goodbye\")]\ninternal static HttpResponse HandleGoodbye(HttpRequest request)\n{\n    HttpResponse response = new HttpResponse(200);\n    response.SetHeaders(new[] { KeyValuePair.Create(\"Content-Type\", \"text/plain\") });\n    response.SetBody($\"Goodbye from Demo.Wasm.Slight!\");\n\n    return response;\n}\n</code></pre>\n<p>That's it. Certainly not complete, certainly not optimal, most likely buggy, and potentially leaking memory. But it works and can be deployed to the cloud.</p>\n<h2 id=\"running-a-slight-application-in-wasm-wasi-node-pool\">Running a Slight Application in WASM/WASI Node Pool</h2>\n<p>To deploy our Slight application to the cloud, we need an AKS Cluster with a WASM/WASI node pool. The process of setting it up hasn't changed since my <a href=\"https://www.tpeczek.com/2022/12/experimenting-with-net-webassembly.html\">previous post</a> and you can find all the necessary steps there. Here we can start with dockerizing our application.</p>\n<p>As we are dockerizing a Wasm application, the final image should be from <em>scratch</em>. In the case of a Slight application, it should contain two elements: <em>app.wasm</em> and <em>slightfile.toml</em>. The <em>slightfile.toml</em> is a configuration file and its main purpose is to define and provide options for the capabilities needed by the application. In our case, that's just the HTTP Server capability.</p>\n<pre><code class=\"lang-toml\">specversion = \"0.1\"\n\n[[capability]]\nname = \"http\"\n</code></pre>\n<p>The <em>app.wasm</em> file is our application. It should have this exact name and be placed at the root of the image. To be able to publish the application, the <code>build</code> stage in our <em>Dockerfile</em> must install the same prerequisites as we did for local development (WASI SDK and <code>wasi-experimental</code> workload).</p>\n<pre><code class=\"lang-Dockerfile\">FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build\n\nRUN curl https://github.com/WebAssembly/wasi-sdk/releases/download/wasi-sdk-20/wasi-sdk-20.0-linux.tar.gz -L --output wasi-sdk-20.0-linux.tar.gz\nRUN tar -C /usr/local/lib -xvf wasi-sdk-20.0-linux.tar.gz\nENV WASI_SDK_PATH=/usr/local/lib/wasi-sdk-20.0\n\nRUN dotnet workload install wasi-experimental\n\nWORKDIR /src\nCOPY . .\nRUN dotnet publish --configuration Release\n\nFROM scratch\n\nCOPY --from=build /src/bin/Release/net8.0/wasi-wasm/AppBundle/Demo.Wasm.Slight.wasm ./app.wasm\nCOPY --from=build /src/slightfile.toml .\n</code></pre>\n<p>With the infrastructure in place and the image pushed to the container registry, all that is needed is a deployment manifest for Kubernetes resources. It is the same as it was for a Spin application, the only difference is the <code>kubernetes.azure.com/wasmtime-slight-v1</code> node selector.</p>\n<pre><code class=\"lang-yaml\">apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: \"wasmtime-slight-v1\"\nhandler: \"slight\"\nscheduling:\n  nodeSelector:\n    \"kubernetes.azure.com/wasmtime-slight-v1\": \"true\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slight-with-dotnet-8\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: slight-with-dotnet-8\n  template:\n    metadata:\n      labels:\n        app: slight-with-dotnet-8\n    spec:\n      runtimeClassName: wasmtime-slight-v1\n      containers:\n        - name: slight-with-dotnet-8\n          image: crdotnetwasi.azurecr.io/slight-with-dotnet-8:latest\n          command: [\"/\"]\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: slight-with-dotnet-8\nspec:\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  selector:\n    app: slight-with-dotnet-8\n  type: LoadBalancer\n</code></pre>\n<p>After applying the above manifest, you can get the service IP with <code>kubectl get svc</code> or from the Azure Portal and execute some requests.</p>\n<h2 id=\"webassembly-cloud-net-and-future-\">WebAssembly, Cloud, .NET, and Future?</h2>\n<p>Everything I toyed with here is either a preview or experimental, but it's a glimpse into where WebAssembly in the Cloud is heading.</p>\n<p>If I dare to make a prediction, I think that when the wasi-cloud-core proposal reaches a mature enough phase, we will see it supported on AKS (probably it will replace the Spin and Slight available in the current preview). The support for WASI in .NET will also continue to evolve and we will see a non-experimental SDK once the specification gets stable.</p>\n<p>For now, we can keep experimenting and exploring. If you want to kick-start your own exploration with what I've described in this post, the source code is <a href=\"https://github.com/tpeczek/demo-dotnet-on-aks-wasi-node-pool\">here</a>.</p>\n<h2 id=\"postscript\">Postscript</h2>\n<p>Yes, I know that this post is already a little bit long, but I wanted to mention one more thing.</p>\n<p>When I was wrapping this post, I read the <a href=\"https://devblogs.microsoft.com/dotnet/extending-web-assembly-to-the-cloud/\">\"Extending WebAssembly to the Cloud with .NET\"</a> and learned that Steve Sanderson has also built some <a href=\"https://github.com/SteveSandersonMS/spiderlightning-dotnet/tree/main/sample\">Slight samples</a>. Despite that, I've decided to publish this post. I had two main reasons for that. First,I dare to think that there is valuable knowledge in this dump of thoughts of mine. Second, those samples take a slightly different direction than mine, and I believe that the fact that you can arrive at different destinations from the same origin is one of the beautiful aspects of software development - something worth sharing.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2024/01/experimenting-with-net-webassembly.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-6340462120240203656",
      "Title": "Azure Functions Integration Testing With Testcontainers",
      "PublishDate": "2023-10-31T12:03:00+00:00",
      "Summary": "<p>As developers, we want reassurance that our code functions as expected. We also want to be given that reassurance as fast as possible. This is why we are writing automated tests. We also desire for those tests to be easy to run on our machine or in the worst-case scenario on the build agent as part of the continuous integration pipeline. This is something that can be challenging to achieve when it comes to Azure Functions.</p>\n<p>Depending on the language we are using for our Azure Functions, the challenges can be different. Let's take .NET (I guess I haven't surprised anyone with that choice) as an example. In the case of .NET, we can write any <em>unit tests</em> we want (I will deliberately avoid trying to define what is that <em>unit</em>). But the moment we try to move to <em>integration tests</em>, things get tricky. If our Azure Function is using the in-process model, we have an option of crafting a <em>system under test</em> based on the WebJobs host which will be good enough for some scenarios. If our Azure Function is using the isolated worker model there are only two options: accept that our tests will integrate only to a certain level and implement <em>Test Doubles</em> or wait for the Azure Functions team to <a href=\"https://github.com/Azure/azure-functions-dotnet-worker/issues/281\">implement a test worker</a>. This is all far from perfect.</p>\n<p>To work around at least some of the above limitations, I've adopted with my teams a different approach for Azure Functions integration testing - we've started using <a href=\"https://testcontainers.com/\"><em>Testcontainers</em></a>. Testcontainers is a framework for defining through code throwaway, lightweight instances of containers, to be used in test context.</p>\n<p>We initially adopted this approach for .NET Azure Functions, but I know that teams creating Azure Functions in different languages also started using it. This is possible because the approach is agnostic to the language in which the functions are written (and the tests can be written using any language/framework supported by Testcontainers).</p>\n<p>In this post, I want to share with you the core parts of this approach. It starts with creating a Dockerfile for your Azure Functions.</p>\n<h2 id=\"creating-a-dockerfile-for-an-azure-functions-container-image\">Creating a Dockerfile for an Azure Functions Container Image</h2>\n<p>You may already have a Dockerfile for your Azure Functions (for example if you decided to host them in Azure Container Apps or Kubernetes). From my experience, that's usually not the case. That means you need to create a Dockerfile. There are two options for doing that. You can use <em>Azure Functions Core Tools</em> and call <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/functions-core-tools-reference?tabs=v2#func-init\"><code>func init</code></a> with the <code>--docker-only</code> option, or you can create the Dockerfile manually. The Dockerfile is different for every language, so until you gain experience I suggest using the command. Once you are familiar with the structure, you will very likely end up with a modified template that you will be reusing with small adjustments. The one below is my example for .NET Azure Functions using the isolated worker model.</p>\n<pre><code class=\"lang-Dockerfile\">FROM mcr.microsoft.com/dotnet/sdk:7.0 AS installer-env\nARG RESOURCE_REAPER_SESSION_ID=\"00000000-0000-0000-0000-000000000000\"\nLABEL \"org.testcontainers.resource-reaper-session\"=$RESOURCE_REAPER_SESSION_ID\n\nWORKDIR /src\nCOPY function-app/ ./function-app/\n\nRUN dotnet publish function-app \\\n    --output /home/site/wwwroot\n\nFROM mcr.microsoft.com/azure-functions/dotnet-isolated:4-dotnet-isolated7.0\n\nENV AzureWebJobsScriptRoot=/home/site/wwwroot \\\n    AzureFunctionsJobHost__Logging__Console__IsEnabled=true\n\nCOPY --from=installer-env [\"/home/site/wwwroot\", \"/home/site/wwwroot\"]\n</code></pre>\n<p>What's probably puzzling you right now is that label based on the provided argument. This is something very specific to Testcontainers. The above Dockerfile is for a multi-stage build, so it will generate intermediate layers. Testcontainers has a concept of <em>Resource Reaper</em> which job is to remove Docker resources once they are no longer needed. This label is needed for the Resource Reaper to be able to track those intermediate layers.</p>\n<p>Once we have the Dockerfile we can create the test context.</p>\n<h2 id=\"creating-a-container-instance-in-test-context\">Creating a Container Instance in Test Context</h2>\n<p>The way you create the test context depends on the testing framework you are going to use and the isolation strategy you want for that context. My framework of choice is xUnit. When it comes to the isolation strategy, it depends 😉. That said, the one I'm using most often is test class. For xUnit that translates to class fixture. You can probably guess that there are also requirements when it comes to the context lifetime management. After all, we will be spinning containers and that takes time. That's why the class fixture must implement <code>IAsyncLifetime</code> to provide support for asynchronous operations.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    ...\n\n    public AzureFunctionsTestcontainersFixture()\n    { \n        ...\n    }\n\n    public async Task InitializeAsync()\n    {\n        ...\n    }\n\n    public async Task DisposeAsync()\n    {\n        ...\n    }\n}\n</code></pre>\n<p>There are a couple of things that we need to do here. The first is creating an image based on our Dockerfile. For this purpose, we can use <code>ImageFromDockerfileBuilder</code>. The minimum we need to provide is the location of the Dockerfile (directory and file name). Testcontainers provides us with some handy helpers for getting the solution, project, or Git directory. We also want to set that <code>RESOURCE_REAPER_SESSION_ID</code> argument.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    private readonly IFutureDockerImage _azureFunctionsDockerImage;\n\n    public AzureFunctionsTestcontainersFixture()\n    {\n        _azureFunctionsDockerImage = new ImageFromDockerfileBuilder()\n            .WithDockerfileDirectory(CommonDirectoryPath.GetSolutionDirectory(), String.Empty)\n            .WithDockerfile(\"AzureFunctions-Testcontainers.Dockerfile\")\n            .WithBuildArgument(\n                 \"RESOURCE_REAPER_SESSION_ID\",\n                 ResourceReaper.DefaultSessionId.ToString(\"D\"))\n            .Build();\n    }\n\n    public async Task InitializeAsync()\n    {\n        await _azureFunctionsDockerImage.CreateAsync();\n\n        ...\n    }\n\n    ...\n}\n</code></pre>\n<p>With the image in place, we can create a container instance. This will require reference to the image, port binding, and wait strategy. Port binding is something that Testcontainers can almost completely handle for us. We just need to tell which port to bind and the host port can be assigned randomly. The wait strategy is quite important. This is how the framework knows that the container instance is available. We have a lot of options here: port availability, specific message in log, command completion, file existence, successful request, or <code>HEALTHCHECK</code>. What works great for Azure Functions is a successful request to its default page.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    private readonly IFutureDockerImage _azureFunctionsDockerImage;\n\n    public IContainer AzureFunctionsContainerInstance { get; private set; }\n\n    ...\n\n    public async Task InitializeAsync()\n    {\n        await _azureFunctionsDockerImage.CreateAsync();\n\n        AzureFunctionsContainerInstance = new ContainerBuilder()\n            .WithImage(_azureFunctionsDockerImage)\n            .WithPortBinding(80, true)\n            .WithWaitStrategy(\n                Wait.ForUnixContainer()\n                .UntilHttpRequestIsSucceeded(r =&gt; r.ForPort(80)))\n            .Build();\n        await AzureFunctionsContainerInstance.StartAsync();\n    }\n\n    ...\n}\n</code></pre>\n<p>The last missing part is the cleanup. We should nicely dispose the container instance and the image.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    private readonly IFutureDockerImage _azureFunctionsDockerImage;\n\n    public IContainer AzureFunctionsContainerInstance { get; private set; }\n\n    ...\n\n    public async Task DisposeAsync()\n    {\n        await AzureFunctionsContainerInstance.DisposeAsync();\n\n        await _azureFunctionsDockerImage.DisposeAsync();\n    }\n}\n</code></pre>\n<p>Now we are ready to write some tests.</p>\n<h2 id=\"implementing-integration-tests\">Implementing Integration Tests</h2>\n<p>At this point, we can start testing our function. We need a test class using our class fixture.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTests : IClassFixture&lt;AzureFunctionsTestcontainersFixture&gt;\n{\n    private readonly AzureFunctionsTestcontainersFixture _azureFunctionsTestcontainersFixture;\n\n    public AzureFunctions(AzureFunctionsTestcontainersFixture azureFunctionsTestcontainersFixture)\n    {\n        _azureFunctionsTestcontainersFixture = azureFunctionsTestcontainersFixture;\n    }\n\n    ...\n}\n</code></pre>\n<p>Now for the test itself, let's assume that the function has an HTTP trigger. To build the URL of our function we can use the <code>Hostname</code> provided by the container instance and acquire the host port by calling <code>.GetMappedPublicPort</code>. This means that the test only needs to create an instance of <code>HttpClient</code>, make a request, and assert the desired aspects of the response. The simplest test I could think of was to check for a status code indicating success.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTests : IClassFixture&lt;AzureFunctionsTestcontainersFixture&gt;\n{\n    private readonly AzureFunctionsTestcontainersFixture _azureFunctionsTestcontainersFixture;\n\n    ...\n\n    [Fact]\n    public async Task Function_Request_ReturnsResponseWithSuccessStatusCode()\n    {\n        HttpClient httpClient = new HttpClient();\n        var requestUri = new UriBuilder(\n            Uri.UriSchemeHttp,\n            _azureFunctionsTestcontainersFixture.AzureFunctionsContainerInstance.Hostname,\n            _azureFunctionsTestcontainersFixture.AzureFunctionsContainerInstance.GetMappedPublicPort(80),\n            \"api/function\"\n        ).Uri;\n\n        HttpResponseMessage response = await httpClient.GetAsync(requestUri);\n\n        Assert.True(response.IsSuccessStatusCode);\n    }\n}\n</code></pre>\n<p>And Voila. This will run on your machine (assuming you have Docker) and in any CI/CD environment which build agents have Docker pre-installed (for example Azure DevOps or GitHub).</p>\n<h2 id=\"adding-dependencies\">Adding Dependencies</h2>\n<p>What I've shown you so far covers the scope of the function itself. This is already beneficial because it allows for verifying if dependencies are registered properly or if the middleware pipeline behaves as expected. But Azure Functions rarely exist in a vacuum. There are almost always dependencies and Testcontainers can help us with those dependencies as well. There is a wide set of <a href=\"https://testcontainers.com/modules/\">preconfigured implementations</a> that we can add to our test context. A good example can be storage. In the majority of cases, storage is required to run the function itself. For local development, Azure Functions are using the <a href=\"https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite?WT.mc_id=DT-MVP-5002979\">Azurite</a> emulator and we can do the same with Testcontainers as it is available as a ready-to-use <a href=\"https://testcontainers.com/modules/azurite/\">module</a>. To add it to the context you just need to reference the proper NuGet package and add a couple of lines of code.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    ...\n\n    public AzuriteContainer AzuriteContainerInstance { get; private set; }\n\n    ...\n\n    public async Task InitializeAsync()\n    {\n        AzuriteContainerInstance = new AzuriteBuilder().Build();\n        await AzuriteContainerInstance.StartAsync();\n\n        ...\n    }\n\n    public async Task DisposeAsync()\n    {\n        ...\n\n        await AzuriteContainerInstance.DisposeAsync();\n    }\n}\n</code></pre>\n<p>We also need to point Azure Functions to use this Azurite container by setting the <code>AzureWebJobsStorage</code> parameter.</p>\n<pre><code class=\"lang-cs\">public class AzureFunctionsTestcontainersFixture : IAsyncLifetime\n{\n    ...\n\n    public async Task InitializeAsync()\n    {\n        ...\n\n        AzureFunctionsContainerInstance = new ContainerBuilder()\n            ...\n            .WithEnvironment(\"AzureWebJobsStorage\", AzuriteContainerInstance.GetConnectionString())\n            ...\n            .Build();\n\n        ...\n    }\n\n    ...\n}\n</code></pre>\n<p>That's it. Having Azurite in place also enables testing functions that use triggers and bindings based on Azure Storage. There are also ready-to-use modules for Redis, Azure Cosmos DB, Azure SQL Edge, MS SQL, Kafka, or RabbitMQ. So there is quite good out-of-the-box coverage for potential Azure Functions dependencies. Some other dependencies can be covered by creating containers yourself (for example with an unofficial <a href=\"https://github.com/pmcilreavy/AzureEventGridSimulator\">Azure Event Grid simulator</a>). That said, some dependencies can be only satisfied by a real thing (at least for <a href=\"https://github.com/Azure/azure-service-bus/issues/223\">now</a>).</p>\n<h2 id=\"a-powerful-tool-in-your-toolbox\">A Powerful Tool in Your Toolbox</h2>\n<p>Is Testcontainers a solution for every integration problem - no. Should Testcontainers be your default choice when thinking about integration tests - also no. But it is a very powerful tool and you should be familiar with it, so you can use it when appropriate.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/10/azure-functions-integration-testing.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-5075869972314954380",
      "Title": "Revisiting Various Change Feeds Consumption in .NET",
      "PublishDate": "2023-10-10T11:54:00+00:00",
      "Summary": "<p>The first time I wrote about change feeds consumption from .NET (ASP.NET Core to be more precise) was back in <a href=\"https://www.tpeczek.com/2018/05/exposing-rethinkdb-changefeed-from.html\">2018</a> in the context of RethinkDB. It was always a very powerful concept. Having access to an ordered flow of information about changes to items is a low-entry enabler for various event-driven, stream processing, or data movement scenarios. As a result, over the years, this capability (with various name variations around the words change, stream, and feed) has found its way to many databases and sometimes even other storage services. The list includes (but is not limited to) MongoDB, RavenDB, Cosmos DB, DynamoDB or Azure Blob Storage (in preview).</p>\n<p>As I was cleaning up and updating a <a href=\"https://github.com/tpeczek/Demo.AspNetCore.Changefeed\">demo application</a> that shows how to consume and expose various change feeds from ASP.NET Core, I decided to write down some notes to refresh the content from my previous posts.</p>\n<h2 id=\"iasyncenumerable-as-universal-change-feed-abstraction\">IAsyncEnumerable as Universal Change Feed Abstraction</h2>\n<p>When I started working with change feeds over 5 years ago, I initially didn't put them behind any abstraction. I like to think that I was smart and avoided premature generalization. The abstraction came after a couple of months when I could clearly see that I was implementing the same concepts through similar components in different projects where teams were using RethinkDB, MongoDB, or Cosmos DB. The abstraction that I started advocating back then looked usually like this.</p>\n<pre><code class=\"lang-cs\">public interface IChangeFeed&lt;T&gt;\n{\n    T CurrentChange { get; }\n\n    Task&lt;bool&gt; MoveNextAsync(CancellationToken cancelToken = default(CancellationToken));\n}\n</code></pre>\n<p>In retrospect, I'm happy with this abstraction, because around two or more years later, when those teams and projects started to adopt C# 8 and .NET Core 3 (or later versions), refactoring all those implementations was a lot easier. C# 8 has brought async streams, a natural programming model for asynchronous streaming data sources. Asynchronous streaming data source is exactly what change feeds are and modeling them through <code>IAsyncEnumerable</code> results in nice and clean consumption patterns. This is why currently I advocate for using <code>IAsyncEnumerable</code> as a universal change feed abstraction. The trick to properly using that abstraction is defining the right change representation to be returned. That should depend on change feed capabilities and actual needs in a given context. Not all change feeds are the same. Some of them can provide information on all operations performed on an item, and some only on a subset. Some can provide old and new value, and some only the old. Your representation of change should consider all that. In the samples ahead I'm avoiding this problem by reducing the change representation to the changed version of the item.</p>\n<h2 id=\"azure-cosmos-db-change-feed\">Azure Cosmos DB Change Feed</h2>\n<p>Azure Cosmos DB change feed is the second (after the RethinkDB one) I've been writing about in the past. It's also the one which consumption has seen the most evolution through time.</p>\n<p>The <a href=\"https://www.tpeczek.com/2018/08/exposing-cosmos-db-change-feed-from.html\">first consumption model</a> was quite complicated. It required going through partition key ranges, building document change feed queries for them, and then obtaining enumerators. This whole process required managing its state, which resulted in non-trivial code. It's good that it has been deprecated as part of Azure Cosmos DB .NET SDK V2, and it's going out of support in August 2024.</p>\n<p>Azure Cosmos DB .NET SDK V3 has brought the <a href=\"https://www.tpeczek.com/2020/01/exposing-cosmos-db-change-feed-from.html\">second consumption model</a> based on <em>change feed processor</em>. The whole inner workings of consuming the change feed have been enclosed within a single class, which reduced the amount of code required. But <em>change feed processor</em> has its oddities. It requires an additional container - a lease container that deals with previously described state management. This is beneficial in complex scenarios as it allows for coordinated processing by multiple workers, but becomes an unnecessary complication for simple scenarios. It also provides only a push-based programming model. The consumer must provide a delegate to receive changes. Once again this is great for certain scenarios, but leads to awkward implementation when you want to abstract change feed as a stream.</p>\n<p>The story doesn't end there, version 3.20.0 of Azure Cosmos DB .NET SDK has introduced the third consumption model based on <em>change feed iterator</em>. It provides a pull-based alternative to <em>change feed processor</em> for scenarios where it's more appropriate. With the <em>change feed iterator</em> the control over the pace of consuming the changes is given back to the consumer. State management is also optional, but it's the consumer's responsibility to persist continuation tokens if necessary. Additionally, the <em>change feed iterator</em> brings the option of obtaining a change feed for a specific partition key.</p>\n<p>The below snippet shows a very simple consumer implementation of the <em>change feed iterator</em> model - no state management, just starting the consumption from a certain point in time and waiting one second before polling for new changes.</p>\n<pre><code class=\"lang-cs\">public async IAsyncEnumerable&lt;T&gt; FetchFeed(\n    [EnumeratorCancellation] CancellationToken cancellationToken = default)\n{\n    FeedIterator&lt;T&gt; changeFeedIterator = _container.GetChangeFeedIterator&lt;T&gt;(\n        ChangeFeedStartFrom.Time(DateTime.UtcNow),\n        ChangeFeedMode.LatestVersion\n    );\n\n    while (changeFeedIterator.HasMoreResults &amp;&amp; !cancellationToken.IsCancellationRequested)\n    {\n        FeedResponse&lt;T&gt; changeFeedResponse = await changeFeedIterator\n            .ReadNextAsync(cancellationToken);\n\n        if (changeFeedResponse.StatusCode == HttpStatusCode.NotModified)\n        {\n            await Task.Delay(TimeSpan.FromSeconds(1), cancellationToken);\n        }\n        else\n        {\n            foreach (T item in changeFeedResponse)\n            {\n                yield return item;\n            }\n        }\n    }\n}\n</code></pre>\n<h2 id=\"mongodb-change-feed\">MongoDB Change Feed</h2>\n<p>MongoDB is probably the most popular NoSQL choice among the teams that I've been working with, which doesn't use cloud PaaS databases for their needs. Among its many features, it has quite powerful change feed (a.k.a. <em>Change Streams</em>) capability.</p>\n<p>The incoming change information can cover a wide spectrum of <a href=\"https://www.mongodb.com/docs/manual/reference/change-events/\">operations</a> which can come from a single collection, database, or entire deployment. If the operation relates to a document, the change feed can provide the current version, the previous version, and the delta. There is also support for resume tokens which can be used to manage state if needed.</p>\n<p>One unintuitive thing when it comes to MongoDB change feed is that it's only available when you are running a <em>replica set</em> or a <em>sharded cluster</em>. This doesn't mean that you have to run a cluster. You can run a single instance as a replica set (even in a container), you just need the right configuration (you will find a workflow that handles such a deployment to Azure Container Instances in the <a href=\"https://github.com/tpeczek/Demo.AspNetCore.Changefeed\">demo repository</a>).</p>\n<p>The consumption of MongoDB change feed is available through the <code>Watch</code> and <code>WatchAsync</code> methods available on <code>IMongoCollection</code>, <code>IMongoDatabase</code>, and <code>IMongoClient</code> instances. The below snippet watches a single collection and configures the change feed to return the current version of the document. You can also provide a pipeline definition when calling <code>Watch</code> or <code>WatchAsync</code> to filter the change feed (for example to monitor only specific operation types).</p>\n<pre><code class=\"lang-cs\">public async IAsyncEnumerable&lt;T&gt; FetchFeed(\n    [EnumeratorCancellation]CancellationToken cancellationToken = default)\n{\n    IAsyncCursor&lt;ChangeStreamDocument&lt;T&gt;&gt; changefeed = await _collection.WatchAsync(\n        new ChangeStreamOptions { FullDocument = ChangeStreamFullDocumentOption.UpdateLookup },\n        cancellationToken: cancellationToken\n    );\n\n    while (!cancellationToken.IsCancellationRequested)\n    {\n        while (await changefeed.MoveNextAsync(cancellationToken))\n        {\n            IEnumerator&lt;ChangeStreamDocument&lt;T&gt;&gt;  changefeedCurrentEnumerator = changefeed\n                .Current.GetEnumerator();\n\n            while (changefeedCurrentEnumerator.MoveNext())\n            {\n                if (changefeedCurrentEnumerator.Current.OperationType\n                    == ChangeStreamOperationType.Insert)\n                {\n                    yield return changefeedCurrentEnumerator.Current.FullDocument;\n                }\n\n                ...\n            }\n        }\n\n        await Task.Delay(_moveNextDelay, cancellationToken);\n    }\n}\n</code></pre>\n<h2 id=\"azure-blob-storage-change-feed\">Azure Blob Storage Change Feed</h2>\n<p>Azure Blob Storage is the odd one on this list because it's an object storage, not a database. Its change feed provides information about changes to blobs and blobs metadata in an entire storage account. Under the hood the change feed is implemented as a special container (yes it's visible, yes you can take a look) which is being created once you enable it. As it is a container you should consider the configuration of the retention period as it will affect your costs.</p>\n<p>There is one more important aspect of Azure Blob Storage change feed when considering its usage - latency. It's pretty slow. It can take minutes for changes to appear.</p>\n<p>From the consumption perspective, it follows the enumerator approach. You can obtain the enumerator by calling <code>BlobChangeFeedClient.GetChangesAsync</code>. The enumerator is not infinite, it will return the changes currently available and once you process them you have to poll for new ones. This makes managing the continuation tokens required even for a local state. What is unique is that you can request changes within a specified time window.</p>\n<p>The change feed supports six events in the latest schema version. In addition to expected ones like created or deleted, there are some interesting ones like tier changed. The information never contains the item, which shouldn't be surprising as in the context of object storage this would be quite risky.</p>\n<p>The below snippet streams the change feed by locally managing the continuation token and for changes that represent blob creation, it downloads the current version of the item.</p>\n<pre><code class=\"lang-cs\">public async IAsyncEnumerable&lt;T&gt; FetchFeed(\n    [EnumeratorCancellation]CancellationToken cancellationToken = default)\n{\n    string? continuationToken = null;\n\n    TokenCredential azureCredential = new DefaultAzureCredential();\n\n    BlobServiceClient blobServiceClient = new BlobServiceClient(_serviceUri, azureCredential);\n    BlobChangeFeedClient changeFeedClient = _blobServiceClient.GetChangeFeedClient();\n\n    while (!cancellationToken.IsCancellationRequested)\n    {\n        IAsyncEnumerator&lt;Page&lt;BlobChangeFeedEvent&gt;&gt; changeFeedEnumerator = changeFeedClient\n            .GetChangesAsync(continuationToken)\n            .AsPages()\n            .GetAsyncEnumerator();\n\n        while (await changeFeedEnumerator.MoveNextAsync())\n        {\n            foreach (BlobChangeFeedEvent changeFeedEvent in changeFeedEnumerator.Current.Values)\n            {\n                if ((changeFeedEvent.EventType == BlobChangeFeedEventType.BlobCreated)\n                    &amp;&amp; changeFeedEvent.Subject.StartsWith($\"/blobServices/default/containers/{_container}\"))\n                {\n                    BlobClient createdBlobClient = new BlobClient(\n                        changeFeedEvent.EventData.Uri,\n                        azureCredential);\n\n                    if (await createdBlobClient.ExistsAsync())\n                    {\n                        MemoryStream blobContentStream =\n                            new MemoryStream((int)changeFeedEvent.EventData.ContentLength);\n                        await createdBlobClient.DownloadToAsync(blobContentStream);\n                        blobContentStream.Seek(0, SeekOrigin.Begin);\n\n                        yield return JsonSerializer.Deserialize&lt;T&gt;(blobContentStream);\n                    }\n                }\n            }\n\n            continuationToken = changeFeedEnumerator.Current.ContinuationToken;\n        }\n\n        await Task.Delay(TimeSpan.FromSeconds(1), cancellationToken);\n    }\n}\n</code></pre>\n<h2 id=\"there-is-more\">There Is More</h2>\n<p>The above samples are in no way exhaustive. They don't show all the features of given change feeds and they don't show all the change feeds out there. But they are a good start, this is why  I've been evolving them for the past five years.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/10/revisiting-various-change-feeds.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-4717310053772857696",
      "Title": "Deploying a Dapr Sidecar to Azure Container Instances",
      "PublishDate": "2023-09-05T11:08:00+00:00",
      "Summary": "<p>Containers have become one of the main, if not the main, ways to modularize, isolate, encapsulate, and package applications in the cloud. The sidecar pattern allows for taking this even further by allowing the separation of functionalities like monitoring, logging, or configuration from the business logic. This is why I recommend that the teams who are adopting containers adopt sidecars as well. One of my preferred suggestions is Dapr which can bring early value by providing abstractions for message broker integration, encryption, observability, secret management, state management, or configuration management.</p>\n<p>To my surprise, many conversations starting around adopting sidecars quickly deviate to \"we should set up a Kubernetes cluster\". It's almost like there are only two options out there - you either run a standalone container or you need Kubernetes for anything more complicated. This is not the case. There are multiple ways to run containers and you should choose the one which is most suitable for your current context. Many of those options will give you more sophisticated features like sidecars or init containers while your business logic is still in a single container. Sidecars give here an additional benefit of enabling later evolution to more complex container hosting options without requirements for code changes.</p>\n<p>In the case of Azure, such a service that enables adopting sidecars at an early stage is Azure Container Instances.</p>\n<h2 id=\"quick-reminder-azure-container-instances-can-host-container-groups\">Quick Reminder - Azure Container Instances Can Host Container Groups</h2>\n<p>Azure Container Instances provides a managed approach for running containers in a serverless manner, without orchestration. What I've learned is that a common misconception is that Azure Container Instances can host only a single container. That is not exactly the truth, Azure Container Instances can host a <a href=\"https://learn.microsoft.com/en-us/azure/container-instances/container-instances-container-groups?WT.mc_id=DT-MVP-5002979\">container group</a> (if you're using Linux containers 😉).</p>\n<p>A container group is a collection of containers scheduled on the same host and sharing lifecycle, resources, or local network. The container group has a single public IP address, but the publicly exposed ports can forward to ports exposed on different containers. At the same time, all the containers within the group can reach each other via localhost. This is what enables the sidecar pattern.</p>\n<p>How to create a container group? There are three options:</p>\n<ul>\n<li>With ARM/Bicep</li>\n<li>With Azure CLI by using YAML file</li>\n<li>With Azure CLI by using Docker compose file</li>\n</ul>\n<p>I'll go with Bicep here. The <code>Microsoft.ContainerInstance</code> namespace contains only a single type which is <code>containerGroups</code>. This means that from ARM/Bicep perspective there is no difference if you are deploying a standalone container or a container group - there is a <code>containers</code> list available as part of the resource properties where you specify the containers.</p>\n<pre><code class=\"lang-bicep\">resource containerGroup 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  name: CONTAINER_GROUP\n  location: LOCATION\n  ...\n  properties: {\n    sku: 'Standard'\n    osType: 'Linux'\n    ...\n    containers: [\n      ...\n    ]\n    ...\n  }\n}\n</code></pre>\n<p>How about a specific example? I've mentioned that Dapr is one of my preferred sidecars, so I'm going to use it here.</p>\n<h2 id=\"running-dapr-in-self-hosted-mode-within-a-container-group\">Running Dapr in Self-Hosted Mode Within a Container Group</h2>\n<p>Dapr has several hosting options. It can be self-hosted with Docker, Podman, or without containers. It can be hosted in Kubernetes with first-class integration. It's also available as a serverless offering - part of Azure Container Apps. The option interesting us in the context of Azure Container Instances is self-hosted with Docker, but from that list, you could pick up how Dapr enables easy evolution from Azure Container Instances to Azure Container Apps, Azure Kubernetes Services or non-Azure Kubernetes clusters.</p>\n<p>But before we will be ready to deploy the container group, we need some infrastructure around it. We should start with a resource group, container registry and managed identity.</p>\n<pre><code class=\"lang-powershell\">az group create -l $LOCATION -g $RESOURCE_GROUP\naz acr create -n $CONTAINER_REGISTRY -g $RESOURCE_GROUP --sku Basic\naz identity create -n $MANAGED_IDENTITY -g $RESOURCE_GROUP\n</code></pre>\n<p>We will be using the managed identity for role-based access control where possible, so we should reference it as the identity of the container group in our Bicep template.</p>\n<pre><code class=\"lang-bicep\">resource managedIdentity 'Microsoft.ManagedIdentity/userAssignedIdentities@2023-01-31' existing = {\n  name: MANAGED_IDENTITY\n}\n\nresource containerGroup 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  ...\n  identity: {\n    type: 'UserAssigned'\n    userAssignedIdentities: {\n      '${managedIdentity.id}': {}\n    }\n  }\n  properties: {\n    ...\n  }\n}\n</code></pre>\n<p>The Dapr sidecar requires a components directory. It's a folder that will contain YAML files with components definitions. To provide that folder to the Dapr sidecar container, we have to mount it as a volume. Azure Container Instances supports <a href=\"https://learn.microsoft.com/en-us/azure/container-instances/container-instances-volume-azure-files?WT.mc_id=DT-MVP-5002979\">mounting an Azure file share</a> as a volume, so we have to create one.</p>\n<pre><code class=\"lang-powershell\">az storage account create -n $STORAGE_ACCOUNT -g $RESOURCE_GROUP --sku Standard_LRS\naz storage share create -n daprcomponents --account-name $STORAGE_ACCOUNT\n</code></pre>\n<p>The created Azure file share needs to be added to the list of volumes that can be mounted by containers in the group. Sadly, the integration between Azure Container Instances and Azure file share doesn't support role-based access control, an access key has to be used.</p>\n<pre><code class=\"lang-bicep\">...\n\nresource storageAccount 'Microsoft.Storage/storageAccounts@2022-09-01' existing = {\n  name: STORAGE_ACCOUNT\n}\n\nresource containerGroup 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  ...\n  properties: {\n    ...\n    volumes: [\n      {\n        name: 'daprcomponentsvolume'\n        azureFile: {\n          shareName: 'daprcomponents'\n          storageAccountKey: storageAccount.listKeys().keys[0].value\n          storageAccountName: storageAccount.name\n          readOnly: true\n        }\n      }\n    ]\n    ...\n  }\n}\n</code></pre>\n<p>We also need to assign the <code>AcrPull</code> role to the managed identity so it can access the container registry.</p>\n<pre><code class=\"lang-powershell\">az role assignment create --assignee $MANAGED_IDENTITY_OBJECT_ID \\\n    --role AcrPull \\\n    --scope \"/subscriptions/$SUBSCRIPTION_ID/resourcegroups/$RESOURCE_GROUP/providers/Microsoft.ContainerRegistry/registries/$CONTAINER_REGISTRY\"\n</code></pre>\n<p>I'm skipping the creation of the image for the application with the business logic, pushing it to the container registry, adding its definition to the <code>containers</code> list, and exposing needed ports from the container group - I want to focus on the Dapr sidecar.</p>\n<p>In this example, I will be grabbing the <code>daprd</code> image from the Docker Registry.</p>\n<p>The startup command for the sidecar is <code>./daprd</code>. We need to provide a <code>--resources-path</code> parameter which needs to point to the path where the <code>daprcomponentsvolume</code> will be mounted. I'm also providing the <code>--app-id</code> parameter. This parameter is mostly used for service invocation (it won't be the case here and I'm not providing <code>--app-port</code>) but Dapr is using it also in different scenarios (for example as partition key for some state stores).</p>\n<p>Two ports need to be exposed from this container (not publicly): <code>3500</code> is the default HTTP endpoint port and <code>50001</code> is the default gRPC endpoint port. There is an option to change both ports through configuration if they need to be taken by some other container.</p>\n<pre><code class=\"lang-bicep\">resource containerGroup 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  ...\n  properties: {\n    ...\n    containers: [\n      ...\n      {\n        name: 'dapr-sidecar'\n        properties: {\n          image: 'daprio/daprd:1.10.9'\n          command: [ './daprd', '--app-id', 'APPLICATION_ID', '--resources-path', './components']\n          volumeMounts: [\n            {\n              name: 'daprcomponentsvolume'\n              mountPath: './components'\n              readOnly: true\n            }\n          ]\n          ports: [\n            { \n              port: 3500\n              protocol: 'TCP'\n            }\n            { \n              port: 50001\n              protocol: 'TCP'\n            }\n          ]\n          ...\n        }\n      }\n    ]\n    ...\n  }\n}\n</code></pre>\n<p>I've omitted the <code>resources</code> definition for brevity.</p>\n<p>Now the Bicep template can be deployed.</p>\n<pre><code class=\"lang-powershell\">az deployment group create -g $RESOURCE_GROUP -f container-group-with-dapr-sidecar.bicep\n</code></pre>\n<p>The below diagram visualizes the final state after the deployment.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1UiAcIO8l04HphgPJxLFB2ayIDeDwun7uoD66XJt0U5YUuUcPFt0jfIE551ISUJC4wQDaN0LeLJAF_2ezFTt75MQejpBQb8gMNX_KuGLT5-OjIY0g_5_LWABtoGoWkpTfagnRyX1BRNcMahxw5YilmrPiu6thtgc-4u7aQY54l-AzdNVq5AXXHmTNoAU/s1600/container-instance-with-dapr-integrations.png\" alt=\"Diagram of Azure Container Instances hosting a container group including application container and Dapr sidecar integrated with Azure Container Registry and having Azure file share mounted as volume with Dapr components definitions.\"></p>\n<h2 id=\"configuring-a-dapr-component\">Configuring a Dapr Component</h2>\n<p>We have a running Dapr sidecar, but we have yet to make it truly useful. To be able to use APIs provided by Dapr, we have to provide the mentioned earlier components definitions which will provide implementation for those APIs. As we already have a storage account as part of our infrastructure, a state store component seems like a good choice. Dapr supports quite an extensive <a href=\"https://docs.dapr.io/reference/components-reference/supported-state-stores/\">list of stores</a>, out of which two are based on Azure Storage: Azure Blob Storage and Azure Table Storage. Let's use the Azure Table Storage one.</p>\n<p>First I'm going to create a table. This is not a required step, the component can do it for us, but let's assume we want to seed some data manually before the deployment.</p>\n<p>Second, the more important operation is granting needed permissions to the storage account. Dapr has very good support for <a href=\"https://docs.dapr.io/developing-applications/integrations/azure/azure-authentication/authenticating-azure/\">authenticating to Azure</a> which includes managed identities and role-based access control, so I'm just going to assign the <em>Storage Table Data Reader</em> role to our managed identity for the scope of the storage account.</p>\n<pre><code class=\"lang-powershell\">az storage table create -n $TABLE_NAME --account-name $STORAGE_ACCOUNT\naz role assignment create --assignee $MANAGED_IDENTITY_OBJECT_ID \\\n    --role \"Storage Table Data Contributor\" \\\n    --scope \"/subscriptions/$SUBSCRIPTION_ID/resourcegroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT\"\n</code></pre>\n<p>The last thing we need is the <a href=\"https://docs.dapr.io/reference/components-reference/supported-state-stores/setup-azure-tablestorage/\">component definition</a>. The component type we want is <code>state.azure.tablestorage</code>. The name is what we will be using when making calls with a Dapr client. As we are going to use managed identity for authenticating, we should provide <code>accountName</code>, <code>tableName</code>, and <code>azureClientId</code> as metadata. I'm additionally setting <code>skipCreateTable</code> because I created the table earlier and the component will fail on an attempt to create it once again.</p>\n<pre><code class=\"lang-yaml\">apiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: state.table.&lt;TABLE_NAME&gt;\nspec:\n  type: state.azure.tablestorage\n  version: v1\n  metadata:\n  - name: accountName\n    value: &lt;STORAGE_ACCOUNT&gt;\n  - name: tableName\n    value: &lt;TABLE_NAME&gt;\n  - name: azureClientId\n    value: &lt;Client ID of MANAGED_IDENTITY&gt;\n  - name: skipCreateTable\n    value: true\n</code></pre>\n<p>The file with the definition needs to be uploaded to the file share which is mounted as the components directory. The Azure Container Instances need to be restarted for the component to be loaded. We can quickly verify if it has been done by taking a look at logs.</p>\n<pre><code class=\"lang-txt\">time=\"2023-08-31T21:25:22.5325911Z\"\nlevel=info\nmsg=\"component loaded. name: state.table.&lt;TABLE_NAME&gt;, type: state.azure.tablestorage/v1\"\napp_id=APPLICATION_ID\ninstance=SandboxHost-638291138933285823\nscope=dapr.runtime\ntype=log\nver=1.10.9\n</code></pre>\n<p>Now you can start <a href=\"https://docs.dapr.io/developing-applications/building-blocks/state-management/state-management-overview/\">managing your state</a> with a Dapr client for your language of choice or with HTTP API if one doesn't exist.</p>\n<h2 id=\"the-power-of-abstraction-decoupling-and-flexibility\">The Power of Abstraction, Decoupling, and Flexibility</h2>\n<p>As you can see, the needed increase in complexity (when compared to a standalone container hosted in Azure Container Instances) is not that significant. At the same time, the gain is. Dapr allows us to abstract all the capabilities it provides in the form of <a href=\"https://docs.dapr.io/concepts/building-blocks-concept/\">building blocks</a>. It also decouples the capabilities provided by building blocks from the components providing implementation. We can change Azure Table Storage to Azure Cosmos DB if it better suits our solution, or to AWS DynamoDB if we need to deploy the same application to AWS. We also now have the flexibility of evolving our solution when the time comes to use a more sophisticated container offering - we just need to take Dapr with us.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/09/deploying-dapr-sidecar-to-azure.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-1458654489071666460",
      "Title": "DevOps Practices for Azure Infrastructure - Continuous Operations & Continuous Monitoring",
      "PublishDate": "2023-08-01T10:50:00+00:00",
      "Summary": "<p>This series on implementing DevOps practices for Azure infrastructure is nearing its conclusion. The last part remaining is completing the operations side of the loop.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC2t0-uOwZyh4sUE8sO_S96SHTiINcP18_cIV2y2JnS7W37rW3OwKckiGs3zY8Ionzim4M_K0UDJAP0e21Y0n9NRjt8T0iYJZs5HbSK98rEXh_dmtc8n5RyNpP0Gb_ELogdRh2XVP7LW5VXm1j-dmcXGIrbuGt_y0teo8PCCKr7I7g1FppPB_dFTIMDE0/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create, Verify, Package, and Release Stages\"></p>\n<p>This brings focus to the last two practices on our list:</p>\n<ul>\n<li>Continuous Planning</li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">Continuous Integration</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Delivery</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Deployment</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/07/devops-practices-for-azure.html\">Continuous Testing</a></li>\n<li>Continuous Operations</li>\n<li>Continuous Monitoring</li>\n</ul>\n<h2 id=\"continuous-operations-continuous-monitoring\">Continuous Operations &amp; Continuous Monitoring</h2>\n<p>The <em>Continuous Operations</em> and <em>Continuous Monitoring</em> practices are closely tied together. They jointly serve the goal of ensuring the overall reliability, resiliency, and security of solutions. The majority of capabilities supporting that goal are within the scope of Continuous Operations practice and cover aspects like <em>compliance enforcement</em>, <em>cost management</em>, <em>proactive maintenance</em>, <em>security posture management</em>, and <em>intelligence-driven responses</em> to operational and security events. That said, most of those capabilities can't be achieved without capabilities coming from Continuous Monitoring practice. There can be no cost management without <em>cost tracking</em>. There is no way to have proactive maintenance and intelligence-driven responses without gathering <em>observability signals</em>, configuring <em>alerts</em>, and building <em>dashboards</em>.</p>\n<p>Organizations usually have the capabilities covered by Continuous Operations and Continuous Monitoring already established, but often they are not aligned with DevOps cultural philosophies. This means that implementing those practices is often about addressing gaps around automation, collaboration, continuous feedback, and continuous improvement.</p>\n<p>But before we start addressing those gaps, it's worth making sure that the capabilities have been established on the right foundations, as Azure provides a wide range of services to support us here:</p>\n<ul>\n<li><em>Azure Policy</em> for compliance enforcement.</li>\n<li><em>Azure Monitor</em> with its insights, visualization, analytics, and response stack for gathering observability signals, configuring alerts, and building dashboards.</li>\n<li><em>Microsoft Defender for Cloud</em> for workload protection and security posture management.</li>\n<li><em>Azure Sentinel</em> for security information and event management (SIEM) as well as security orchestration, automation, and response (SOAR).</li>\n<li><em>Azure Automation</em> and <em>Azure Logic Apps</em> for automating event-based intelligence-driven responses and orchestrating proactive maintenance.</li>\n<li><em>Microsoft Cost Management and Billing</em> for cost tracking and management.</li>\n</ul>\n<p>With the right foundations in place, we can focus on aspects that make the difference between \"being DevOps\" and \"not being DevOps\". The most crucial one is ensuring that everyone has access to information on how the part they are responsible for is behaving in production.</p>\n<h2 id=\"all-teams-need-observability-signals\">All Teams Need Observability Signals</h2>\n<p>As you may remember from the post on <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Delivery and Continuous Deployment</a>, at certain sizes solutions often start moving from centralized ownership to being owned by multiple independent applications teams and an environment team. This dynamics needs to be reflected in monitoring architecture as well. A single, centralized monitoring service, although needed by the environment team, may not be sufficient. This is why mature monitoring implementations utilize resource-context observability signals and granular insights, visualization, and analytics workspaces from which the signals are later centrally aggregated. This approach enables every application team to have direct access to its signals, configure alerts and build dashboards, while the environment team still has visibility into the whole picture.</p>\n<p>This approach also enables democratization when it comes to the tools itself. The native observability stack in Azure is provided by Azure Monitor, but it no longer means that application teams are fully limited to Application Insights. If they prefer they can use Prometheus and Grafana for metrics (which is great when they need to be more cloud agnostic and looking at adopting <em>Open Telemetry</em>).</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnvlyLjblNfQl80KN15zBvquvqMF9eYv-x2sn0jHgo8hUatUmA70XNWfjPDvTd0yKAqq9Ca-_aww51C4piTBX83-BGBTKUOIY5Arja3llfT_QURpjheY6uvamaPMKJO-TCCgwRng3u7oGkQbrUkrQIhgwKPefixN_Wd0aYlYQAtnVemMPkqsw5I-QE2jo/s1600/Democratized%20Monitoring%20Architecture.png\" alt=\"Diagram representing a democratized monitoring architecture with dedicated workspaces for every team and teams using different tools\"></p>\n<p>Of course, such a democratized monitoring architecture cannot be left without governance. There need to be rules around observability signals granularity, retention, and archiving data to cool-tier storage. Otherwise, we can be very unpleasantly surprised by the cost of our monitoring implementation.</p>\n<p>Automated responses should also be exporting proper context information to the respective tools - because part of automated response should be creating a proper item in the collaboration tool to ensure continuous feedback. What item should that be? That depends on the event category.</p>\n<h2 id=\"operational-events-should-create-issues\">Operational Events Should Create Issues</h2>\n<p>From the infrastructure perspective, there are usually two main types of operational events that are potentially interesting:</p>\n<ul>\n<li><em>Resources events</em> like creation, deletion, or modification</li>\n<li><em>Alerts</em> defined in Azure Monitor</li>\n</ul>\n<p>The usage of resource events often covers adding special tags, granting permissions to special groups, or reacting to delete/create/update operation fails.</p>\n<p>Alerts are usually raised when the measured state of the system deviates from what is considered a baseline. To name just a few examples, this can mean networking issues, an erroneously stopped VM, or a resource reaching its capacity.</p>\n<p>The remediation for every resource event or alert can be different. In some cases, the remediation can be fully automated (restarting a VM, truncating tables in a database, or increasing RUs for Azure Cosmos DB). In some cases, all that is needed is just a notification to deal with the problem at the earliest convenience (failure to delete a resource). There are also those cases that require waking up an engineer immediately (networking issues).</p>\n<p>In <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">the first post of this series</a>, I wrote that the cornerstone of implementing DevOps practices for Azure infrastructure is infrastructure as code and the Git ecosystem used for collaboration. This means that regardless if the remediation is fully automated or an engineer needs to be engaged, part of the process should be issue creation (if the remediation has been already performed that issue can be closed and exist just for tracking purposes). In the stack I've chosen for this series, the Git ecosystem is GitHub. Integrating GitHub issue creation into the response workflow is not a huge challenge, because there is a ready-to-use <a href=\"https://learn.microsoft.com/en-us/connectors/github?WT.mc_id=DT-MVP-5002979\">GitHub connector for Azure Logic Apps</a>. So, if we consider alerts, this means that we can build an automated response flow by using <em>Azure Monitor Alerts</em>, <em>Azure Monitor Action Group</em>, and <em>Azure Logic Apps</em>.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgnW7LxhdISZJ6jWhPwhSIluFWF_juzT10n4XWb_80sqo5Y_ETGusQDkyLJr6la5d9tYYjZ1ZmZHy8z5OwkDVHkqHMBI2sAHwljEcXudyGsU2RZTLmdDCyfbPpINe2qcRVIvLflJ-ZRNRjyUe_DdseaoxwNRGM1lh96b41THkVKxRak1Li-kj3nBHXIVg/s1600/alert-automated-response-flow.png\" alt=\"Diagram representing an automated response flow for an alert raised in Azure Monitor which uses Azure Logic App to create issues in GitHub and perform remediation action\"></p>\n<p>Almost identical flow can be built for the resources events if we use <em>Azure Event Grid</em> in place of Azure Monitor (as Azure Event Grid supports resource groups and subscriptions as sources).</p>\n<p>This is the approach that should be applied to ensure collaboration and continuous feedback when it comes to operational events, how about security events?</p>\n<h2 id=\"security-events-should-create-vulnerabilities\">Security Events Should Create Vulnerabilities</h2>\n<p>Security events have a specific lifecycle that falls under the responsibility of the organization's <em>Security Operations Center (SOC)</em>. It's the SOC that uses available observability signals, CVE alerting platforms like <em>OpenCVE</em>, and other tools to detect, investigate, and remediate threats. In the case of Azure, Azure Sentinel is the one-stop shop to build and automate this responsibility.</p>\n<p>That said, SOC usually deals with the immediate remediation of a threat. For example, SOC operator or automation may determine that to mitigate a threat a specific resource needs to be isolated because a new CVE has been disclosed. The only action performed will be isolation - the responsibility for mitigating the CVE is with the application or environment team. In such cases, the SOC operator or automation should report the specific vulnerability with context and findings in the collaboration tool. When using GitHub as the Git ecosystem for collaboration, a great way to report such vulnerabilities may be through security advisories.</p>\n<p>Security advisories facilitate the process of reporting, discussing, and fixing vulnerabilities. <a href=\"https://docs.github.com/en/code-security/security-advisories/repository-security-advisories/creating-a-repository-security-advisory\">Creating security advisories</a> requires <em>admin</em> or <em>security manager</em> role within the repository, so the integration must be designed properly to avoid excessive permissions within the organization. My approach is to <a href=\"https://docs.github.com/en/apps/creating-github-apps/about-creating-github-apps/about-creating-github-apps\">create a GitHub App</a>. GitHub Apps use OAuth 2.0 and can act on behalf of a user, which in this case will be SOC operator or automation. To make the creation of security advisories available directly from Azure Sentinel, I expose a webhook from the GitHub App which can be called by a Playbook.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaeN_31GhxWu6QJ1ekEsucfVDgH7bgnnrmK-ll8EsJKL1hsvtnw0q_SrIONfQkFNlvOuhs0jdXuETi7ckN9h3LA7PM9T5d1LKMDv-hcpsregurZFeH8OzerDgi9WEk4CCg-AB-x7GC3FQi3ht_g9kzXjUOmtG37J6bZEU526OAyTHkEjEqZE8GcfX1CzM/s1600/security-advisory-creation-flow.png\" alt=\"Diagram representing a security advisory creation flow through Azure Sentinel Playbook and GitHub App\"></p>\n<p>Providing automated tools which don't require context switching from the SOC perspective removes roadblocks, which is crucial for the adoption of collaboration and continuous feedback between otherwise disconnected teams. This is the true spirit of DevOps.</p>\n<h2 id=\"infrastructure-drift-detection\">Infrastructure Drift Detection</h2>\n<p>There is one capability in the context of <em>Continuous Monitoring</em> and <em>Continuous Operations</em>, which is very specific to infrastructure - detecting drift.</p>\n<p>As I have shown through the series, if we want to implement DevOps practices for Azure infrastructure, the infrastructure should be changed only through modifying and deploying its code. The repository should be the single source of truth. But sometimes, when there is pressure, stress, or time constraints (for example when solving a critical issue) engineers do take shortcuts and modify the infrastructure directly. It's not that big of an issue if such an engineer will later reflect the changes in the infrastructure code. But humans are humans and they sometimes forget. This can cause the environment to drift from its source of truth and creates potential risks from applied change being reverted after the deployment to deployment failures. This is why detecting drift is important.</p>\n<p>Infrastructure drift detection is a complex problem. Depending on chosen stack there are different tools you can use to make it as sophisticated as you need. Here, as an example, I'm going to show a mechanism that can be set up quickly based on the stack I've already used throughout this series. It's far from perfect, but it's a good start. It's using the <code>what-if</code> command, which I've already been using for creating previews of changes as part of <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">Continuous Integration</a> implementation.</p>\n<pre><code class=\"lang-ps\">az deployment group what-if \\\n    --resource-group rg-devops-practices-sample-application-prod \\\n    --template-file applications/sample-application/application.bicep \\\n    --mode Complete \\\n    --no-pretty-print\n</code></pre>\n<p>You may notice two differences between the usage of <code>what-if</code> for previews and drift detection.</p>\n<p>The first difference is the <em>Complete</em> deployment mode. The difference between <em>Incremental</em> (the default) and <em>Complete</em> deployment modes is that in the case of the second resources that exist in the resource group but aren't specified in the template will be deleted instead of ignored.</p>\n<p>The second difference is the output format. For the previews, I wanted something human-readable, but here I prefer something which will be easy to process programmatically. Providing the <code>--no-pretty-print</code> switch changes the output format to JSON. Below you can see a snippet of it.</p>\n<pre><code class=\"lang-json\">{\n  \"changes\": [\n    {\n      \"after\": null,\n      \"before\": {\n        \"name\": \"kvsampleapplication\",\n        ...\n      },\n      \"changeType\": \"Delete\",\n      ...\n    },\n    {\n      \"after\": {\n        \"name\": \"id-sampleapplication-gd3f7mnjwpuyu\",\n        ...\n      },\n      \"before\": {\n        \"name\": \"id-sampleapplication-gd3f7mnjwpuyu\",\n        ...\n      },\n      \"changeType\": \"NoChange\",\n      ...\n    },\n    ...\n  ],\n  \"error\": null,\n  \"status\": \"Succeeded\"\n}\n</code></pre>\n<p>Our attention should focus on the <code>changeType</code> property. It provides information on what will happen with the resource after the deployment. The possible values are: <em>Create</em>, <em>Delete</em>, <em>Ignore</em>, <em>NoChange</em>, <em>Modify</em>, and <em>Deploy</em>. Create, Delete, and NoChange are self-explanatory. The Ignore value should not be present in the case of Complete deployment mode unless limits (number of nested templates or expanding time) have been reached - in such case, it will mean that the resource hasn't been evaluated. Modify and Deploy are tricky. They mean that the properties of the resource will be changed after the deployment. Unfortunately, the <em>Resource Manager</em> is not perfect here and those two can give false positive predictions. This is why this technique is far from perfect - the only drift that can be reliably detected are missing resources or resources which shouldn't exist. But, as I said, it's a good start as we can quickly create a GitHub Actions workflow that will be performing the detection. Let's start by checking out the deployed tag and connecting to Azure.</p>\n<pre><code class=\"lang-yaml\">...\n\nenv:\n  TAG: sample-application-v1.0.0\n\njobs:\n  drift-detection:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n      with:\n        ref: ${{ env.TAG }}\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        ...\n  ...\n</code></pre>\n<p>The next step is to run a script that will call <code>what-if</code> and process the results to create an array of detected changes.</p>\n<pre><code class=\"lang-yaml\">...\n\nenv:\n  ...\n  RESOURCE_GROUP: 'rg-devops-practices-sample-application-prod'\n\njobs:\n  drift-detection:\n    ...\n    steps:\n    ...\n    - name: Detect infrastructure drift\n      shell: pwsh\n      run: |\n        $issues = @()\n\n        $drift = az deployment group what-if `\n          --resource-group $env:RESOURCE_GROUP `\n          --template-file applications/sample-application/application.bicep `\n          --mode Complete `\n          --no-pretty-print | ConvertFrom-Json\n\n        foreach ($change in $drift.Changes)\n        {\n          switch ($change.changeType)\n          {\n            'Create'\n            {\n              $issues += @{\n                ResourceName = $change.after.name\n                Description = 'Defined resource doesn''t exist'\n              }\n            }\n            'Delete'\n            {\n              $issues += @{\n                ResourceName = $change.before.name\n                Description = 'Undefined resource exists'\n              }\n            }\n          }\n        }\n\n        'DRIFT_ISSUES&lt;&gt; $env:GITHUB_ENV\n        $issues | ConvertTo-Json -AsArray &gt;&gt; $env:GITHUB_ENV\n        'EOF' &gt;&gt; $env:GITHUB_ENV\n  ...\n</code></pre>\n<p>Having all the changes gathered, we can use the proven <a href=\"https://github.com/marketplace/actions/github-script\">script action</a> to create an issue for every detected change.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  drift-detection:\n    ...\n    permissions:\n      ...\n      issues: write\n    steps:\n    ...\n    - name: Report detected infrastructure drift\n      uses: actions/github-script@v6\n      with:\n        script: |\n          const issues = JSON.parse(process.env.DRIFT_ISSUES);\n          for (const issue of issues) {\n            github.rest.issues.create({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              title: '[DRIFT DETECTED] ' + issue.Description + ' (' + issue.ResourceName + ')'\n            });\n          }\n  ...\n</code></pre>\n<p>We can have this action running regularly. It will be creating nice issues like the one in the screenshot and will give us some start in drift detection.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxKdn0hSxDoXM5eXnapp8TVZDTp3MF9iijrOw7jumeI6VLBV3p09GFDs8eHHd4bLwNriYcozzl0hRKlSiEX0mExeoek6xSJE0Q6LSF42Pp5Hhs7PtdEtVCQxP9lOFVlsuU1i2RI83jYBvOHdEeQcKIqCwTEim6WeZQE0oDg8RxjPoiTpG99RbKMwo62oY/s1600/drift-detection-issue.png\" alt=\"Drift Detection (Sample Application) workflow - created issue for undefined resource\"></p>\n<h2 id=\"the-journey-never-ends\">The Journey Never Ends</h2>\n<p>With Continuous Operations and Continuous Monitoring practices, we have closed the loop.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBG9sAszGTLFjI7YZMv9yESEDsiNWF2tLBH_2gpostPWwnSaJiW7hO7L3_4XoUuvewgQNuQoYY9eJXc5vos-_XyuzIpd7_1upiZCSjTNxNMFrwoomznAnPC3N4MXmkJ5PF_LGjE-F0EQJ7XLbSVjSxwldFPD3zUDoTYbZEumxVfbajhq9lUkgydWhRCs4/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create, Verify, Package, Release, Operate, and Monitor Stages\"></p>\n<p>But the nature of a loop is that an end is also the beginning. The implementation of DevOps is never \"done\". It's a direct consequence of its core cultural philosophies: continuous feedback and continuous improvement. Regardless of how your initial implementation will look, you should constantly evaluate it in the context of the ecosystem around and evolve. This will mean modifying the implementation of already established practices, but also implementing new complementary ones (like <em>Continuous Learning</em> or <em>Continuous Documentation</em>).</p>\n<p>The goal of this series was to draw the overall picture and provide examples that will bring that picture to life. The accompanying <a href=\"https://github.com/tpeczek/demo-devops-practices-for-azure-infrastructure\">repository</a> contains working workflows that can kickstart your journey.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/08/devops-practices-for-azure.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-5903928949952130037",
      "Title": "DevOps Practices for Azure Infrastructure - Continuous Testing",
      "PublishDate": "2023-07-11T12:11:00+00:00",
      "Summary": "<p>So far, as part of my series on implementing DevOps practices for Azure infrastructure, I've walked through <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">Continuous Integration</a>, <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Delivery, and Continuous Deployment</a>. In many conversations I had around implementing DevOps I've heard an opinion that once you have CI/CD (or CI/CD/CD) you have DevOps. That's not true. DevOps is about a continuous loop of feedback, automation, collaboration, and improvement. As you can see in the picture below, those three practices give only about half of that loop and cover mostly the development side.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj_U2rjBnenPoR6Eqgr1zAPeQUrSrssNN1-_JnYls7l4P2KEEYz8mcJ6mUljO6pit9fCIq2BcIyqj4PMVPVhogKbVZjPGSmy5lPN5WGJUuCGLDKUBXc2BfzSPgJmKHjfNmTok5hFs463_0k2SVLxtTTsSYuuSFrO2jun4rNb3cRQ3tNND2JnOVRoCF2xY/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create, Verify, Package, and Release Stages\"></p>\n<p>This is why there are more practices on the list:</p>\n<ul>\n<li>Continuous Planning</li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">Continuous Integration</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Delivery</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Deployment</a></li>\n<li>Continuous Testing</li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Operations</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Monitoring</a></li>\n</ul>\n<p>To complete the loop and speak about complete DevOps implementation, it's time to start implementing practices that provide feedback from the deployed environment to the teams and automate operations concerns. In this post, I'm going to discuss Continuous Testing.</p>\n<p>The goal of Continuous Testing is to ensure quality at different stages of the development life cycle. It's a practice that applies to both sides of the loop. We have already encountered it as part of the Continuous Integration practice. It's sometimes present as part of Continuous Delivery (for example running specific tests when versions of modules referenced from the environment repository are being updated) and it should be present as part of Continuous Deployment and later in the form of after-deployment tests. The after-deployment tests are what I want to focus on.</p>\n<p>Discussing tests often revolves around discussing two aspects: tools to be used for implementation and types of tests to be implemented. Those are two main ingredients used to create a test strategy (of course a mature test strategy covers much more, but it's a discussion for a different occasion). Let's first consider the tools.</p>\n<p>There are no real special requirements when choosing tools for infrastructure tests. As long as the stack allows calling APIs it should be sufficient. Very often the applications teams are using the same tools for testing the infrastructure tied to the application which they are using for testing the application itself. The environment teams, on the other hand, are looking for specific tools which fit the ecosystem they are familiar with. A popular choice when it comes to Azure is <a href=\"https://pester.dev/\">Pester</a>, a test framework for Powershell. I'm going to use it for examples here.</p>\n<p>What types of tests should you consider implementing? There are two which I consider a must-have - smoke tests and negative tests.</p>\n<h2 id=\"smoke-tests\">Smoke Tests</h2>\n<p>Smoke tests should be the first tests to verify the deployment. Their goal is to quickly provide feedback on crucial functions of the system without delving into finer details. Their implementation should be fast and simple. A typical smoke test is a verification if a host is responsive, which in Pester is just a couple of lines:</p>\n<pre><code class=\"lang-ps\">param(\n  [Parameter(Mandatory)]\n  [ValidateNotNullOrEmpty()]\n  [string] $HostName\n)\n\nDescribe 'Application Host' {\n    It 'Serves pages over HTTPS' {\n      $request = [System.Net.WebRequest]::Create(\"https://$HostName/\")\n      $request.AllowAutoRedirect = $false\n      $request.GetResponse().StatusCode |\n        Should -Be 200 -Because \"It's responsive\"\n    }\n}\n</code></pre>\n<p>Notice that we are not trying to determine if the hosted application is healthy beyond getting a successful status code - we are testing infrastructure and the application hasn't been deployed yet.</p>\n<p>Running smoke tests should be the first job in our post-deployment workflow. GitHub-hosted runners come with Pester, which means that running the tests is just two lines of Powershell.</p>\n<pre><code class=\"lang-yaml\">jobs:\n  smoke-tests:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Run Pester Tests\n      shell: pwsh\n      run: |\n        $container = New-PesterContainer `\n          -Path 'applications/sample-application/tests/smoke-tests.ps1' `\n          -Data @{ HostName = '${{ env.APPLICATION_HOST_NAME }}' }\n        Invoke-Pester -Container $container -CI\n  ...\n</code></pre>\n<p>So, running the tests is not a challenge. But running the tests is not the goal by itself. The goal is to properly react when tests fail. What should we do when smoke tests fail? There are two options we can choose from: roll back or roll forward. For smoke tests, we should almost always aim for rolling back. After all a crucial function of our system is not working and reverting it to the previous stable version is usually the quickest way to fix this. Of course roll back may not always be possible and then you are left with roll forward as the only option. Still, you should aim for this to be an edge case.</p>\n<p>The situation with negative tests is a little bit different.</p>\n<h2 id=\"negative-tests\">Negative Tests</h2>\n<p>While smoke tests are providing feedback if crucial functions of the system are working as expected, negative tests are there to provide feedback on how the system will behave in invalid scenarios. A good example can be unencrypted requests over HTTP. They are not secure and we want to disable them at the host level by configuring a redirect. A negative test to verify that can look like below.</p>\n<pre><code class=\"lang-ps\">param(\n  [Parameter(Mandatory)]\n  [ValidateNotNullOrEmpty()]\n  [string] $HostName\n)\n\nDescribe 'Application Host' {\n    It 'Does not serves pages over HTTP' {\n      $request = [System.Net.WebRequest]::Create(\"http://$HostName/\")\n      $request.AllowAutoRedirect = $false\n      $request.GetResponse().StatusCode | \n        Should -Be  301 -Because \"Redirect is forced\"\n    }\n}\n</code></pre>\n<p>As negative tests should be independent of smoke tests and still considered important, the typical approach is to run them in parallel with smoke tests.</p>\n<pre><code class=\"lang-yaml\">jobs:\n  smoke-tests:\n    ...\n  negative-tests:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Run Pester Tests\n      shell: pwsh\n      run: |\n        $container = New-PesterContainer `\n          -Path 'applications/sample-application/tests/negative-tests.ps1' `\n          -Data @{ HostName = '${{ env.APPLICATION_HOST_NAME }}' }\n        Invoke-Pester -Container $container -CI\n  ...\n</code></pre>\n<p>There is often a discussion about how to decide if something is a negative test or a smoke test. The distinction should be based on the impact. Taking our two examples:</p>\n<ul>\n<li>Host not being responsive is catastrophic, we can't provide any service for our users.</li>\n<li>Host responding to HTTP requests is something we can live with for a moment. There is secondary protection in the application code and in our industry context, it's only a recommendation, not a requirement.</li>\n</ul>\n<p>Of course, context matters and what is a negative test in one situation might be a critical smoke test in another. The key aspect is that negative tests failing don't have to mean rolling back. The system can still provide a valuable service for users. This is why the strategy in case of negative tests is often to roll forward, fix the issue as soon as possible, and perform another deployment.</p>\n<h2 id=\"other-after-deployment-tests\">Other After-Deployment Tests</h2>\n<p>Smoke and negative tests are crucial, but they are only scratching the surface to provide initial feedback as soon as possible. They should be followed by different types of tests which go into previously ignored finer details. Depending on the needs you should implement functional, integration, or other types of tests.</p>\n<p>You also shouldn't limit running tests only to the deployment moment. Infrastructure is much more fragile than application code, so you should continuously run at least the key tests to ensure that everything is working as expected. You should also adopt health checks (yes they are usually considered part of monitoring, but sophisticated health checks are often complex tests) to notice when something becomes unavailable or starts to misbehave. You can also go a step further and continuously test how your system will behave when something goes down by adopting chaos engineering.</p>\n<h2 id=\"chaos-engineering\">Chaos Engineering</h2>\n<p>Chaos engineering is testing through experimenting. You can think of it as a form of exploratory testing. It's about discovering and building confidence in system resilience by exploring its reactions to infrastructure failures. </p>\n<p>The level of 'chaotic-ness' can be very different. <a href=\"https://github.com/netflix/chaosmonkey\">Chaos Monkey</a>, probably the most famous chaos engineering tool, randomly terminates virtual machines and containers, but there are more structured approaches. The methodical approach to chaos engineering starts by defining a <em>steady state</em>, a measurable set of system characteristics that indicates normal behavior. That steady state is a base of a <em>hypothesis</em> that the system will continue in the same state after the experiment. To prove or disprove that hypothesis, an <em>experiment</em> is designed. The design of the experiment should include faults and their targets. Once the design is complete, the experiment is executed by injecting the faults into the environment and capturing the output state. The output state is being <em>verified</em> against the steady state. If the hypothesis has been disproven, the output state should be used for <em>learning</em> and <em>improvement</em> of the system. If the hypothesis has been proven, it's time to design a new experiment.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx2Qom7dEf97Y9TyCShxPZXk0X7bE4SvFg6BS3n0GG4UQvr-8WbjjYXVoTyVWY0iP3TNRIVvSx7uJwQi2tdUgJDAHTkfTxI4lDttUhLd1VvqlGGTWtzsnCRGs1pwrmUamtr7sFW_nc5Eyu_IiS6mBKDwqbi62eZJwCjYHmmLuj-rJof_kr_EUPhu6jkLs/s1600/chaos-engineering.png\" alt=\"Chaos Engineering Process (Loop: Steady State, Hypothesis, Design Experiment, Inject Faults, Verify &amp; Learn, Improve)\"></p>\n<p>Despite being around for several years, the tooling ecosystem for chaos engineering wasn't growing as rapidly as one could wish for. That was until 2020 when AWS announced <a href=\"https://aws.amazon.com/fis/\">AWS Fault Injection Simulator</a>. About a year later Microsoft followed by announcing a public preview of <a href=\"https://learn.microsoft.com/en-us/azure/chaos-studio?WT.mc_id=DT-MVP-5002979\">Azure Chaos Studio</a>. Adopting chaos engineering through a managed service has become an option.</p>\n<p>What does Azure Chaos Studio offer? Currently (still in preview) it provides ~30 <a href=\"https://learn.microsoft.com/en-us/azure/chaos-studio/chaos-studio-fault-library?WT.mc_id=DT-MVP-5002979\">faults and actions</a> which can be applied to ~10 <a href=\"https://learn.microsoft.com/en-us/azure/chaos-studio/chaos-studio-fault-providers?WT.mc_id=DT-MVP-5002979\">targets</a>. What is interesting is that Azure Chaos Studio has two types of faults: service-direct and agent-based. The service-direct run directly against resources, while agent-based enable in-guest failures on virtual machines (for example high CPU).</p>\n<p>How to adopt Azure Chaos Studio? The service provides capabilities to create experiments through ARM or Bicep. There is also <a href=\"https://learn.microsoft.com/en-us/rest/api/chaosstudio?WT.mc_id=DT-MVP-5002979\">REST API</a> which can be used to create, manage, and run experiments. Those capabilities can be used to implement an architecture similar to the following, with continuous experiment execution (1, 2, 3, and 4).</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOBTDWQsfdBgu03HLjW0xGr7kVARIAn0sqYgoZ1-TVhSNtcNKLKWvw2OOhPfwI46dFT3Tx0lesux3XtWNj58PRcj7MouxoqPC4LRBIJyoQGlVe9jhrntM0Syc8IXbE5hATK28V6tT-68rgM7QcktbHfZ3umZ8qw8algMB3WX3CZPTc1NZLhNUrHk4P_8g/s1600/chaos-engineering-architecture.png\" alt=\"Chaos Engineering Architecture Based On GitHub\"></p>\n<h2 id=\"not-there-yet\">Not There Yet</h2>\n<p>With Continuous Testing we have moved a little bit toward the right side of the loop, as part of this practice starts to provide us with feedback from the living system that we can use in our cycle of improvement. Still, there is a significant portion missing.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC2t0-uOwZyh4sUE8sO_S96SHTiINcP18_cIV2y2JnS7W37rW3OwKckiGs3zY8Ionzim4M_K0UDJAP0e21Y0n9NRjt8T0iYJZs5HbSK98rEXh_dmtc8n5RyNpP0Gb_ELogdRh2XVP7LW5VXm1j-dmcXGIrbuGt_y0teo8PCCKr7I7g1FppPB_dFTIMDE0/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create, Verify, Package, and Release Stages\"></p>\n<p>There are practices that I haven't touched yet, which are focusing on the missing part - <em>Continuous Operations</em> and <em>Continuous Monitoring</em>. It's quite likely that they are already present in your organization, just not providing feedback to the loop. This is the journey further and I intend to go there in the next post.</p>\n<p>You can find samples for some of the aspects I'm discussing here on <a href=\"https://github.com/tpeczek/demo-devops-practices-for-azure-infrastructure\">GitHub</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/07/devops-practices-for-azure.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-7011756510786474279",
      "Title": "DevOps Practices for Azure Infrastructure - Continuous Delivery & Continuous Deployment",
      "PublishDate": "2023-06-27T07:06:00+00:00",
      "Summary": "<p>In my <a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">previous post</a>, I started the journey of implementing <em>DevOps</em> practices for infrastructure. I've proposed implementation for <em>Continuous Integration</em> practice, which covers the <em>Create</em> and <em>Verify</em> stages of the DevOps pipeline.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcJbRiWRqsqCfK6dqzkSzN1UL_Aj3yvCPknat2esTFI3v_WkyXeSCASur9kP0tGR8HCQvaQ95UDPNQLnFQTv8u4Cllvr-rXrBZ_SRauXlJYUXhqgNeefZDiX06lJEXlNKYRn7fOkW_ij9a2iVo_Ucx4SHGADMxQgy2OkpdVrI3Z4fs7Lc8Iic560Z8/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create and Verify Stages\"></p>\n<p>But Continuous Integration is just the first of several practices which should be implemented for a complete pipeline:</p>\n<ul>\n<li>Continuous Planning</li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure.html\">Continuous Integration</a></li>\n<li>Continuous Delivery</li>\n<li>Continuous Deployment</li>\n<li><a href=\"https://www.tpeczek.com/2023/07/devops-practices-for-azure.html\">Continuous Testing</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Operations</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Monitoring</a></li>\n</ul>\n<p>In this post, I want to focus on <em>Continuous Delivery</em> and <em>Continuous Deployment</em> practices which are there to pick up where Continuous Integration has finished and continue through the <em>Package</em> and <em>Release</em> stages of the pipeline.</p>\n<h2 id=\"continuous-delivery-vs-continuous-deployment\">Continuous Delivery vs. Continuous Deployment</h2>\n<p>Quite often, when I'm discussing <em>Software Development Life Cycle</em> with teams, there is confusion around Continuous Delivery and Continuous Deployment. Teams will often say that they are doing CI/CD and when I ask about the CD part the terms Continuous Delivery and Continuous Deployment are being used interchangeably. A lot of marketing <em>\"What is DevOps\"</em> articles also don't help by confusing the terms. So what is the difference?</p>\n<p>In short, Continuous Delivery is about making artifacts ready for deployment and Continuous Deployment is about actually deploying them. That seems to be quite a clear separation, so why the confusion? Because in the real world, they often blend. In an ideal scenario, when the Continuous Integration workflow is finished, the deployment workflow can kick off automatically and get the changes to the production. In such a scenario, the Continuous Delivery may not be there, and if it is there it will be considered an implicit part of Continuous Deployment. This is where the terms are often misused - the separation is not clear. Continuous Delivery exists in explicit form only when there is some kind of handover or different step between <em>\"packaging\"</em> and <em>\"deployment\"</em>.</p>\n<p>Why am I discussing this here? Because when it comes to infrastructure, especially for solutions of considerable size, there often is a need for separated Continuous Delivery and Continuous Deployment. Where does this need come from? From different responsibilities. For large solutions, there is infrastructure responsible for the overall environment and infrastructure tied to specific applications. That means multiple teams are owning different parts of the infrastructure and working on it independently. But from a governance and security perspective, there is often a desire to treat the entire infrastructure as one. Properly implemented Continuous Delivery and Continuous Deployment can solve this conflict, but before I move to discuss the practices I need to extend the context by discussing the repositories structure for such solutions.</p>\n<h2 id=\"structuring-repositories\">Structuring Repositories</h2>\n<p>How repositories of your solutions are structured has an impact on the implementation of your DevOps practices. Very often projects start small with a monorepo structure.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRLbltr_x4xnNWKppvexWXTljPQGLs0qpFNyguYnWLAZsoPdxPdcrkNaXp0wJ_5WOJX3MX2O-waLvzMLTXzDyjB9kw7KGMy23i496GqHquqsElv_yfwn2E0R3kP34I_zX89OChJ7Z9rlFc5lO7jfoyYrKQlizPugP7E41ViJVOB0iRqp4bRb5N2JHcjhg/s1600/monorepo.png\" alt=\"Monorepo\"></p>\n<p>There is nothing wrong with monorepo structure. It can be the only structure you will ever need. The main benefit of monorepo is that it's simple. All your code lives in one place, you can iterate fast, it's easy to govern and you can implement just a single set of DevOps practices. But there is a point at which those advantages are becoming limitations. This point comes when the solutions grow to consist of multiple applications owned by different teams. Sooner or later those teams start to ask for some level of independence. They want to have a little bit different governance rules (which better suit their culture) and they don't want to be blocked by work being done by other teams. Sometimes just the size of monorepo becomes a productivity issue. This is where the decoupling of the monorepo starts. Usually, the first step is that new applications are being created in their own repositories. Later, the existing ones are moved out from the monorepo. The outcome is multiple, independent repositories.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJ3WpJWQ2q-2wKWN46jUvd9tA-6WFNw-UZYwsEFoaGOALjuM07kvYeh2qxBuo4dBhgMNNOtuhaxWcC9eszIf8QpNh8GIECcNt6BbcJGi7azIctcF-0qTuQ7HZyT_25J3O_kGjkvRqBdMlNhck4RHpSiCeDCx_omHejQIqcXwB3AN2dOdrvbTADajL6iNg/s1600/repo-per-app.png\" alt=\"Repositories per Application Without Dedicated Environment Repository\"></p>\n<p>But, this structure has some problems of its own. There is no longer a single source of truth that would represent the entire solution. Establishing governance rules which are required for the entire solution is harder. There are multiple sources of deployments which means access to the environment from multiple places, which means increased security risk. There is a need for balance between those aspects and teams needs in areas of flexibility and productivity. A good option for such a balance is having applications repositories and a dedicated environment repository.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_u77nMztAZma2u0bXtdM-qcy4zjjITDdGvCA3dgBocQnESMditldJ20BZCye1N5pARF-n5CtjjNsXvk2UKPPVjlvpxAdIhNo2L5tNu_pm91jnW3SNB0rP268FPNdMZ19Mw_p3fUBrohRlTXGitovNPp7BctYrxfCTlPAHXC26i0PwRLswHPbhgKb3iqE/s1600/repo-for-environment-and-per-app.png\" alt=\"Repositories per Application and Dedicated Environment Repository\"></p>\n<p>The environment repository is the single source of truth. It's also the place to apply required governance rules and the only source of deployments. It can also hold the shared infrastructure. The applications repositories are owned by the applications teams and contain the source code for the application as well as the infrastructure code tied to it. This is the structure we will focus on because this is the structure that requires Continuous Delivery and Continuous Deployment. The applications teams should implement Continuous Delivery for packaging the infrastructure to be used by the environment repository, while the team responsible for the environment repository should implement Continuous Deployment. Let's start with Continuous Delivery.</p>\n<h2 id=\"continuous-delivery-for-applications-infrastructure\">Continuous Delivery for Applications Infrastructure</h2>\n<p>The first idea for Continuous Delivery implementation can be simply copying the infrastructure code to the environment repository. The allure of this approach is that the environment repository will contain the complete code of the infrastructure removing any kind of context switching. The problem is that now the same artifacts live in two places and the sad truth is that when the same artifacts live in two places sooner or later something is going to get messed up. So, instead of copying the infrastructure code a better approach is to establish links from the environment module to the applications modules.</p>\n<p>Options for linking from the environment module to applications modules strongly depend on chosen infrastructure as code tooling. Some tools support a wide variety of sources for the links starting with linking directly to git repositories (so Continuous Delivery can be as simple as creating a tag and updating a reference in the environment module). In rare cases, when there is no support for linking by the tooling, you can always use git submodules.</p>\n<p>In the case of Bicep, there is one interesting option - using <a href=\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/private-module-registry?WT.mc_id=DT-MVP-5002979\">Azure Container Registry</a>. This option can be attractive for two reasons. One is the possibility to create a private, isolated registry. The other is treating infrastructure the same way we would treat containers (so if you are using containers, both are treated similarly).</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUW0_KmI4DMtCyNnlDkpUieifdkkNwEsZn8MMRa3Y7ka6fF9-V1MXOnAtjys6-TbOIuO4sQI-wdqYum6FGSgmAOZmpJ1NvSsfdneQ4BuSsBO3Gidg5_wMggkTnVvlrRo1VbefoAfXCKVgNSceh8rEgjTLkNY5uhDG0jqhGtYpiAnOoM0fqfyItwz2uAe0/s1600/repo-for-environment-and-per-app-with-modules.png\" alt=\"Repositories per Application and Dedicated Environment Repository With Links for Modules\"></p>\n<p>The publishing of bicep files is available through the <code>az bicep publish</code> command. We can create a workflow around this command. A good trigger for this workflow may be the creation of a tag. We can even extract the version from the tag name which we will later use to publish the module.</p>\n<pre><code class=\"lang-yaml\">name: Continuous Delivery (Sample Application)\non:\n  push:\n    tags:\n    - \"sample-application-v[0-9]+.[0-9]+.[0-9]+\"\n\n...\n\njobs:\n  publish-infrastructure-to-registry:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Extract application version from tag\n      run: |\n        echo \"APPLICATION_VERSION=${GITHUB_REF/refs\\/tags\\/sample-application-v/}\" &gt;&gt; $GITHUB_ENV\n    ...\n</code></pre>\n<p>Now all that needs to be done is checking-out the repository, connecting to Azure, and pushing the module.</p>\n<pre><code class=\"lang-yaml\">...\n\nenv:\n  INFRASTRUCTURE_REGISTRY: 'crinfrastructuremodules'\n\njobs:\n  publish-infrastructure-to-registry:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n    ...\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: Publish application Bicep to infrastructure registry\n      run: |\n        bicep publish  \\\n          applications/sample-application/application.bicep  \\\n          --target br:${INFRASTRUCTURE_REGISTRY}.azurecr.io/infrastructure/applications/sample-application:${APPLICATION_VERSION}\n</code></pre>\n<p>The linking itself is to be done in the environment infrastructure Bicep file. The <code>module</code> syntax allows the module path to be either a local file or a file in a registry. This is the part that applications teams will be contributing to the environment repository - the module definition.</p>\n<pre><code class=\"lang-bicep\">...\n\nresource sampleApplicationResourceGroupReference 'Microsoft.Resources/resourceGroups@2022-09-01' = {\n  name: 'rg-devops-practices-sample-application-prod'\n  location: environmentLocation\n}\n\nmodule sampleApplicationResourceGroupModule 'br:crinfrastructuremodules.azurecr.io/infrastructure/applications/sample-application:1.0.0' = {\n  name: 'rg-devops-practices-sample-application-rg'\n  scope: resourceGroup(sampleApplicationResourceGroupReference.name)\n}\n\n...\n</code></pre>\n<p>Now the Continuous Deployment practice for the environment can be implemented.</p>\n<h2 id=\"continuous-deployment-for-environment-infrastructure\">Continuous Deployment for Environment Infrastructure</h2>\n<p>There are two deployment strategies that you may have heard of in the context of deploying infrastructure: <em>push-based</em> deployment and <em>pull-based</em> deployment.</p>\n<p>The push-based deployment is what one could call a classic approach to deployment. You implement a workflow that pushes the changes to the environment. That workflow is usually triggered as a result of changes to the code.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaaQPWW0zJGZiyGTYNdoYv6pUWBlhegmylD7cuSYxYnibsQG4GVtGah-Qn7D5Oi-y3MD6Y2X-OHGUukInW8AF2TIC1QvOLk3JSKMA0kIAhSBkEjIfneotAvNXQYNoH9T5g7xU21WbV6JRBPOVF_RE5jsBFMH4wdKPFt_g2VYsEzCwz0ae3aYW-8gynaOo/s1600/push-based-deployment-strategy.png\" alt=\"The Push-based Deployment Strategy\"></p>\n<p>The pull-based deployment strategy is the newer approach. It introduces an <em>operator</em> in place of the workflow. The operator monitors the repository and the environment and reconciles any differences to maintain the infrastructure as described in the environment repository. That means it will not only react to changes done to the code but also changes applied directly to the environment protecting it from drifting (at least in theory).</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNBvlnQ_RHtyldoWsVP0-NYKPfyf6lcWcBQCW7qtG5yuwPD9MtthC44siSpaMhcGknGXtmHHNURceCpQKGr_xnRUAPgxholg8fHLmHLxt9XSqj3gY2eNoI6wbpMF-rINg7_UstQ4FAvjRV12byphYYbs1_0N1k19sj6cO6P8fZ71hGMatPjzoFPVyZZws/s1600/pull-based-deployment-strategy.png\" alt=\"The Pull-based Deployment Strategy\"></p>\n<p>The pull-based deployment strategy has found the most adoption in the Kubernetes space with two ready-to-use operators (<em>Flux</em> and <em>Argo CD</em>). When it comes to general Azure infrastructure, the push-based strategy is still the way to go, although there is a way to have a pull-based deployment for Azure resources that are tied to applications hosted in Kubernetes. <a href=\"https://azure.github.io/azure-service-operator/\">Azure Service Operator</a> for Kubernetes provides <em>Custom Resource Definitions</em> for deploying Azure resources, enabling a unified experience for application teams.</p>\n<p>In the scope of this post, I'm going to stick with a typical push-based deployment, which means checking-out the repository, connecting to Azure, and deploying infrastructure based on the Bicep file.</p>\n<pre><code class=\"lang-yaml\">name: Continuous Deployment (Environment)\n...\n\njobs:\n  deploy-environment:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: Deploy Environment\n      uses: azure/arm-deploy@v1\n      with:\n        scope: 'subscription'\n        region: 'westeurope'\n        template: 'environment/environment.bicep'\n</code></pre>\n<h2 id=\"the-journey-continues\">The Journey Continues</h2>\n<p>The same as with the previous post, this one also just scratches the surface. There are many variations possible, depending on your needs. It can also by further automated - for example the Continuous Delivery implementation can be automatically creating a pull request to the environment repository. Part of the DevOps culture is continuous improvement and that also means improving the practices implementations itself.</p>\n<p>Our journey is also not over yet, there are a couple more practices I would like to explore in the next posts, so the loop is complete.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj_U2rjBnenPoR6Eqgr1zAPeQUrSrssNN1-_JnYls7l4P2KEEYz8mcJ6mUljO6pit9fCIq2BcIyqj4PMVPVhogKbVZjPGSmy5lPN5WGJUuCGLDKUBXc2BfzSPgJmKHjfNmTok5hFs463_0k2SVLxtTTsSYuuSFrO2jun4rNb3cRQ3tNND2JnOVRoCF2xY/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create, Verify, Package, and Release Stages\"></p>\n<p> If you want to play with the workflows, they are sitting on <a href=\"https://github.com/tpeczek/demo-devops-practices-for-azure-infrastructure\">GitHub</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-634270584827131338",
      "Title": "DevOps Practices for Azure Infrastructure  - Continuous Integration",
      "PublishDate": "2023-06-12T13:59:00+00:00",
      "Summary": "<p>The generally adopted definition of <em>DevOps</em> methodology says that it's a combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver solutions. That's very broad. So broad, that initial adoption in the case of many organizations has focused on applying it only to application code. This has led to the naming of several additional methodologies to either further blend DevOps with additional areas or focus on previously neglected aspects: <em>DevSecOps</em>, <em>FinOps</em>, <em>DataOps</em>, <em>GitOps</em>, or <em>MLOps</em>. But, regardless of the flavor, the core remains the same. This is why, although I'm writing about DevOps in the context of infrastructure, I have avoided using GitOps in the title.</p>\n<p>If you look for a definition of GitOps, you may find statements like <em>\"GitOps is a process of automating IT infrastructure using infrastructure as code and software development best practices\"</em> or <em>\"GitOps is a subset of DevOps\"</em>. In reality, the term has been strongly tied to a specific ecosystem and specific tools. I don't want to fight with those associations. Instead, I want to focus on the essence - applying DevOps practices to infrastructure.</p>\n<h2 id=\"devops-practices\">DevOps Practices</h2>\n<p>DevOps practices are a way to bring DevOps cultural philosophies (collaboration, automation, continuous feedback, continuous improvement, etc.) to life. They are used to implement all the stages of the DevOps pipeline:</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG80EZCZkHLsjeIOGWfPG0YAa2_XFNYevuEwBBoUk9wob-LpV4WaEB6F2YQuSNUX8NXS73IYFUDYtLS3j3pSTLliUyMHEUmJQBDmbe8jVDGKOdEbihOirH_T261JWH98wU1jXPcZSsauk8UzejSKJrvvttXXvNC4xz8xOlSiVI59uu1s-2jsbL4wMe/s1600/devops-pipeline.png\" alt=\"DevOps Pipeline\"></p>\n<p>You may find slightly different lists of those practices, this is the one I prefer:</p>\n<ul>\n<li>Continuous Planning</li>\n<li>Continuous Integration</li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Delivery</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/06/devops-practices-for-azure_27.html\">Continuous Deployment</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/07/devops-practices-for-azure.html\">Continuous Testing</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Operations</a></li>\n<li><a href=\"https://www.tpeczek.com/2023/08/devops-practices-for-azure.html\">Continuous Monitoring</a></li>\n</ul>\n<p>In this post, I want to focus on <em>Continuous Integration</em>.</p>\n<h2 id=\"continuous-integration-practice\">Continuous Integration Practice</h2>\n<p>On some occasions, I've heard an opinion that Continuous Integration is about merging the changes. The truth is that correctly implemented Continuous Integration practice covers the <em>Create</em> and <em>Verify</em> stages of the DevOps pipeline.</p>\n<p>It shouldn't be a surprise that the cornerstone of the Create stage in the case of infrastructure is infrastructure as code, the tooling used for development, and the Git ecosystem used for collaboration. This is something that is already widely adopted with many options to choose from:</p>\n<ul>\n<li>Terraform, Bicep, or Pulumi just to name some popular infrastructure as code options.</li>\n<li>GitHub, Azure DevOps, or GitLab as potential Git ecosystems.</li>\n<li>VS Code, Neovim, or JetBrains Fleet as possible development environments.</li>\n</ul>\n<p>The above list is in no way exhaustive. I also don't aim at discussing the superiority of one tool over another. That said, discussing the Verify stage, which is the more challenging part of Continuous Integration practice, will be better done with specific examples. This is why I must choose a stack and I'm going to choose Bicep (as I'm writing this in the context of Azure) and GitHub (because it has some nice features which will make my life easier).</p>\n<p>So, once we have our infrastructure as code created, what should we consider as verification from the perspective of Continuous Integration? In the beginning, I quoted a statement saying that GitOps is about using software development best practices in the process of automating infrastructure. What would be the first thing one would do with an application code to verify it? Most likely build it.</p>\n<h2 id=\"building-and-linting-for-infrastructure-code\">Building and Linting for Infrastructure Code</h2>\n<p>Building or compiling application code is the first step of the Verify stage. In the software development context, it's sometimes thought of as a way to generate the binaries (and it is), but it's also verifying if the code is syntactically correct. In the context of IaC, it means checking for the correct use of language keywords and that resources are defined according to the requirements for their type. This is something that IaC tooling should always support out of the box. Bicep provides this capability through <code>az bicep build</code> command, which we can simply run as a step in a workflow.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  build-and-lint:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Build and lint Bicep\n      run: |\n        az bicep build --file applications/sample-application/application.bicep\n  ...\n</code></pre>\n<p>The <code>az bicep build</code> command also performs a second activity, which is closely tied to building/compiling - it runs <a href=\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/linter?WT.mc_id=DT-MVP-5002979\">linter</a> over the template. The goal of linting is to help enforce best practices and coding standards based on defined rules. Best practices and coding standards are something that sometimes needs to be tailored to a specific team and organization, this is why Bicep allows for the configuration of rules severity through the <a href=\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-config-linter?WT.mc_id=DT-MVP-5002979\"><code>bicepconfig.json</code></a> file. Possible options are <code>Error</code>, <code>Warning</code>, <code>Info</code>, and <code>Off</code>. By default, the majority of rules are set to either <code>Warning</code> or <code>Off</code>. The typical adjustment which I almost always do is bumping <code>No unused parameters</code> to <code>Error</code> and enabling <code>Use recent API versions</code> (as it is <code>Off</code> by default).</p>\n<pre><code class=\"lang-json\">{\n    \"analyzers\": {\n        \"core\": {\n            \"enabled\": true,\n            \"rules\": {\n                ...\n                \"no-unused-params\": {\n                    \"level\": \"error\"\n                },\n                ...\n                \"use-recent-api-versions\": {\n                     \"level\": \"warning\"\n                },\n            }\n        }\n    }\n}\n</code></pre>\n<p>The <code>bicepconfig.json</code> file should be committed to the repository, which will ensure that the local development environment will pick up the same configuration. This includes VS Code (if the Bicep extension is installed), enabling immediate feedback for engineers (in the spirit of DevOps cultural philosophies). Of course, engineers can ignore that feedback or simply use tooling which doesn't provide it, but then the <code>Build and lint Bicep</code> step of the integration workflow will catch it and give them that feedback.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwpTVFm_9fU88cFlJknSAL57qU-1E9YBI8t57qhNa8zrpIl8K_CyHkdHUHieIGxYu9z2IRp9UCcalRq95jtekAIquV8sWbrk-uh0_vpiJiqonXZ2atPV_vyuNzfHPA7EYauGfTMcaPU0fF3Z8nNr0tzxKQWaLkk40HD4Fa5Ku6xuUflrkMot-g5TXk/s1600/continuous-integration-build-and-lint.png\" alt=\"Continuous Integration (Sample Application) workflow - output of Build and lint Bicep step\"></p>\n<p>If everything is correct, the workflow should move to the next phase, which doesn't mean we should be done with looking at the code itself. Following the software development best practices, the next phase should be static analysis.</p>\n<h2 id=\"static-analysis-for-infrastructure-code\">Static Analysis for Infrastructure Code</h2>\n<p>Application code is usually scanned with tools like SonarQube, Veracode, Snyk, or GitHub's own CodeQL to detect potential vulnerabilities or bad patterns. The same should be done for infrastructure code and there are ready-to-use tools for that like <a href=\"https://github.com/Checkmarx/kics\">KICS</a> or <a href=\"https://github.com/bridgecrewio/checkov\">Checkov</a>. They are both designed to detect security vulnerabilities, compliance issues, and misconfigurations in our IaC. They both come with a huge set of configurable rules and the capability to create your own.</p>\n<p>I prefer KICS, especially the way it can be integrated with GitHub. Checkmarx, the company behind KICS, provides a ready-to-use action. The support for Bicep is \"indirect\" - KICS supports ARM so the analysis has to be done after the build step. There is also small preparation needed as the directory for output should be created. Still, adding KICS-based static analysis to the workflow is only about 10 lines.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  build-lint-and-static-analysis:\n    runs-on: ubuntu-latest\n    steps:\n    ...\n    - name: Create static analysis results folder\n      run: |\n        mkdir -p static-analysis-results\n    - name: Perform KICS static analysis\n      id: kics\n      uses: checkmarx/kics-github-action@v1.6.3\n      with:\n        path: 'applications/sample-application/'\n        fail_on: 'high,medium'\n        output_path: 'static-analysis-results'\n        output_formats: 'json,sarif'\n  ...\n</code></pre>\n<p>The above analysis step will fail if any issues with severity high or medium are detected. Similarly to the build step, the feedback will be provided through workflow output.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3oPALKUyjOU7vbkCU5OMccQO2MzCjOG0bVvEZxmLieKKk4RNNvqIoBOD0sPY9h1bmQnH7joLWdhZfwM27mJXlnrGSGXUFGEMOYwvyzzwbvInaZvGujY3WFuRVSKwfHjf30fhnga4vMgUUJAHTmrvncGhjRcUnAwnqrX4gG5QrJalh0ASRfAGQPwdr/s1600/continuous-integration-static-analysis.png\" alt=\"Continuous Integration (Sample Application) workflow - output of Perform KICS static analysis step\"></p>\n<p>But KICS integration is even more powerful than that. As you may have noticed I've configured output formats from the analysis to be <em>JSON</em> and <em>SARIF</em>. SARIF is a standardized format for sharing static analysis results and it can be used to <a href=\"https://docs.github.com/en/code-security/code-scanning/integrating-with-code-scanning/sarif-support-for-code-scanning\">integrate with the code scanning feature of GitHub Advanced Security</a>. Once again we can use an existing action (this time provided by GitHub) to upload the SARIF file. The only tricky part is to put a proper condition on the upload step so the results are pushed also when the analysis step fails due to the severity of detected issues.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  build-lint-and-static-analysis:\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n    steps:\n    ...\n    - name: Upload KICS static analysis results\n      if: always() &amp;&amp; (steps.kics.outcome == 'success' || steps.kics.outcome == 'failure')\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'static-analysis-results/results.sarif'\n  ...\n</code></pre>\n<p>Thanks to this, the issues will be available in the Code Scanning section of the repository Security tab. This will provide alerts for those issues, the ability to triage them, and audit for taken actions.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJOCT_l5bURch7w14jD8_7Nt9BM2B9Tzc6S5qxgNqZ0IlrEoCDBR6Drm8_vYbt7FpXII3n-68-ewCvcf_WoijrMZnGP480efjtX_v-W2FKq3jb9J89llRkybMw-_5aFOTj_9i5X7vPMQ69A3oz3PuXs6iooG37GT8gacNV07tCHAOTvYm-GPyM2Yht/s1600/continuous-integration-code-scanning-results.png\" alt=\"Continuous Integration (Sample Application) workflow - KICS static analysis results in the Code Scanning section of the repository Security tab\"></p>\n<p>Now we can say that we have looked at the code enough as part of the integration workflow. In the case of software development, we would probably run now some unit tests. In the case of infrastructure, the equivalent at this stage is testing if the template will deploy successfully.</p>\n<h2 id=\"preflight-validation-for-infrastructure-code\">Preflight Validation for Infrastructure Code</h2>\n<p>We have verified that the template will build probably and we have removed all important vulnerabilities and misconfigurations. Sadly, this doesn't guarantee that the template will deploy. There may be some policies or conditions on the environment, which are not reflected in any of the checks. To make sure that the template will deploy, we need to perform a preflight validation against the environment. This capability is provided differently by different ecosystems, in the case of Bicep and ARM it comes as <em>Validate</em> deployment mode. This means that we can add another job to our workflow which will establish a connection to Azure and test the deployment.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  ...\n  preflight-validation:\n    needs: build-lint-and-static-analysis\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: Perform preflight validation\n      uses: azure/arm-deploy@v1\n      with:\n        scope: 'resourcegroup'\n        resourceGroupName: 'rg-devops-practices-sample-application-sandbox'\n        template: 'applications/sample-application/application.bicep'\n        deploymentMode: 'Validate'\n        failOnStdErr: false\n  ...\n</code></pre>\n<p>This will catch issues like duplicated storage account names (or simple cases where the name is too long) without actually deploying anything.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsjTQQvJdbgt7BYrUx49Sj1buv_y3Mor6bYz2cwCpr70ZEzqS52a_S4mqHeJlKU6iDu9dHyKHVufjLEkMGsSZb1dKZoetQA6wNzGKGHBajeaFLNjrwLQ944VNccpKhLqO09IbDdxn2zw2GVhmax1QVm1J89SH-u-Fn6CcYy8_84cQRZBkWcwEIfmfH/s1600/continuous-integration-preflight-validation.png\" alt=\"Continuous Integration (Sample Application) workflow - output of Perform preflight validation step\"></p>\n<p>What's next? Well, there is one common software development practice that we haven't touched yet - pull requests and code reviews. This subject recently caused some heated discussions. There are opinions that if you have an explicit code review step in your process it's not true Continuous Integration. There are also opinions that code reviews are perfectly fine. My opinion is that it's part of team culture. If your team has asynchronous culture, then doing code reviews through pull requests may be the correct way. If your team is collocated or strongly collaborates online, using pair or mob programming instead of code reviews may be the best. We can also detach the discussion around pull requests from the discussion around code reviews. I know teams that are relying on pair programming in place of code reviews but still use pull requests (automatically closed) for tracking purposes. And when we are talking pull requests in the context of infrastructure code, there is one challenge - it's hard to understand the actual change just by looking at code diff (especially after some time). This is why generating a preview of changes as part of the integration workflow can be extremely beneficial.</p>\n<h2 id=\"preview-of-infrastructure-changes\">Preview of Infrastructure Changes</h2>\n<p>Infrastructure as code tooling usually provides a method to generate a preview - Terraform has the <code>plan</code>, Pulumi has the <code>preview</code>, and Bicep/ARM has the <code>what-if</code>. From the perspective of the integration workflow we are not thinking about running those commands locally but as part of the workflow. And this time we are not interested in results being available as part of the workflow output, we are looking for adding them as more context to the pull request. To be able to do that we first must capture the results. A good method is writing the results to an environment variable.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  ...\n  preview:\n    needs: preflight-validation\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: Prepare preview\n      run: |\n        echo 'DEPLOYMENT_WHAT_IF&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        az deployment group what-if \\\n          --resource-group rg-devops-practices-sample-application-sandbox \\\n          --template-file applications/sample-application/application.bicep \\\n          --result-format ResourceIdOnly &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n  ...\n</code></pre>\n<p>Once we have the results, we can add them to the pull request. My preferred approach is to create a comment. GitHub provides us with <a href=\"https://github.com/marketplace/actions/github-script\">script action</a> which allows us to use a pre-authenticated GitHub API client. The issue number and all other necessary information will be available through the <code>context</code> object (if we are using the right trigger).</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  ...\n  preview:\n    needs: preflight-validation\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n      pull-requests: write\n    steps:\n    ...\n    - name:  Create preview comment\n      uses: actions/github-script@v6\n      with:\n        script: |\n          github.rest.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body: process.env.DEPLOYMENT_WHAT_IF\n          })\n  ...\n</code></pre>\n<p>As a result of this job, we will get a nice comment describing in a more human-readable form the changes which deploying the template would cause at this very moment.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiit_5llqfOkylYVa88OyroI9BcvUZkAzMC4m0gnR-Wm2U6nojBvcCLEuuwKGlCsNUHXSDEVqsJpHyNwbFBlTBaxkJIKWqQGt35ZzSjUB0Ra9q8monAIKQb2zK-tRoAQ7nlBonrby0lrIkvNzDVQVBlR6lmP73MUwiwYZw3P2dMB32wKCX5g9VeB1yP/s1600/continuous-integration-preview-comment.png\" alt=\"Continuous Integration (Sample Application) workflow - preview comment on the pull request\"></p>\n<p>We may want even more. Changes are not the only valuable context we may be interested in. The second important information is how deploying the changes will impact the costs.</p>\n<h2 id=\"cost-estimation-of-infrastructure-changes\">Cost Estimation of Infrastructure Changes</h2>\n<p>ThoughtWorks has been recommending <em>run cost as an architecture fitness function</em> since 2019, and there is more than one infrastructure cost estimation tool available to us. The two which are worth mentioning are <a href=\"https://github.com/infracost/infracost\">Infracost</a> (for Terraform) and <a href=\"https://github.com/TheCloudTheory/arm-estimator\">Azure Cost Estimator</a> (for Bicep/ARM and recently also Terraform). As I'm using Bicep in this article, I'm going to focus on Azure Cost Estimator.</p>\n<p>Azure Cost Estimator is still a young tool, yet it's already quite powerful. At the moment of writing this, it supports ~86 resource types. What is very important, it's capable of generating usage base consumption for some resources if you provide usage patterns through metadata in the template. The only tricky part can be integrating it into the workflow. The project repository provides a reusable workflow, but this may not be desired (or even allowed) method in many organizations. This is why I'll walk you through the integration step by step.</p>\n<p>The first step is getting the binaries and installing them. If you are using self-hosted runners this can be part of runner setup. You can also download and install the binaries from some central location as part of the workflow itself. Below I'm doing exactly that from the official project releases.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  ...\n  cost-estimation:\n    needs: preflight-validation\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n    - name: Download Azure Cost Estimator\n      id: download-ace\n      uses: robinraju/release-downloader@v1.7\n      with:\n        repository: \"TheCloudTheory/arm-estimator\"\n        tag: \"1.2\"\n        fileName: \"ace-linux-x64.zip\"\n    - name: Install Azure Cost Estimator\n      run: |\n        unzip ace-linux-x64.zip\n        chmod +x ./azure-cost-estimator\n  ...\n</code></pre>\n<p>With the binaries in place, we can use the same pattern as in the case of preview to run the tool, grab the results into an environment variable, and create a comment.</p>\n<pre><code class=\"lang-yaml\">...\n\njobs:\n  ...\n  cost-estimation:\n    needs: preflight-validation\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n      pull-requests: write\n    steps:\n    ...\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: Prepare cost estimation\n      run: |\n        echo 'COST_ESTIMATION&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        azure-cost-estimator applications/sample-application/application.bicep \\\n          ${{ secrets.AZURE_SUBSCRIPTION_ID } \\\n          rg-devops-practices-sample-application-sandbox \\\n          --stdout --disableDetailedMetrics &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n    - name:  Create pull request comment\n      uses: actions/github-script@v6\n      with:\n        script: |\n          github.rest.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body: process.env.COST_ESTIMATION\n          })\n  ...\n</code></pre>\n<p>This will give us a lot of additional, valuable context in the pull request.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirPC55CNPopsNgwKn3xgqqRYPpS1XCNUBwapHuf3xePXbfFKct_kegFO_hfuk7x-EvaGHQRE94gLpFdA5N7LnmYlWx_dAvr_An-ns6h_r-ENR7iM9OEEUhOZ048IahuDxlYO89Z0U2hdjzOPp903CgsfQ-1O2cp2xhIv0t-zmXDEQoMS8ZNoGRMa4C/s1600/continuous-integration-cost-estimation-comment.png\" alt=\"Continuous Integration (Sample Application) workflow - cost estimation comment on the pull request\"></p>\n<h2 id=\"the-beginning-of-the-implementation-journey\">The Beginning of the Implementation Journey</h2>\n<p>This post describes just the beginning of the DevOps practices implementation journey.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcJbRiWRqsqCfK6dqzkSzN1UL_Aj3yvCPknat2esTFI3v_WkyXeSCASur9kP0tGR8HCQvaQ95UDPNQLnFQTv8u4Cllvr-rXrBZ_SRauXlJYUXhqgNeefZDiX06lJEXlNKYRn7fOkW_ij9a2iVo_Ucx4SHGADMxQgy2OkpdVrI3Z4fs7Lc8Iic560Z8/s1600/devops-pipeline-tools.png\" alt=\"DevOps Pipeline With Tools for Create and Verify Stages\"></p>\n<p>It gives a hint about the entire technology ecosystem, but the remaining practices have many interesting aspects to dive into. I do intend to continue walking through them with proposed implementations, just to make your journey easier. I've also created a <a href=\"https://github.com/tpeczek/demo-devops-practices-for-azure-infrastructure\">repository</a> that contains samples with different parts of implementation (available in different branches with results in different closed pull requests). You can review it to find all the relevant information. You can also create a fork and have your own playground.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/06/devops-practices-for-azure.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-7194716743767456138",
      "Title": "Server Timing API Meets Isolated Worker Process Azure Functions - Custom Middleware and Dependency Injection",
      "PublishDate": "2023-03-15T10:56:00+00:00",
      "Summary": "<p>Server Timing API <a href=\"https://www.tpeczek.com/2017/06/feeding-server-timing-api-from-aspnet.html\">has piqued my interest back in 2017</a>. I've always been a promoter of a data-driven approach to non-functional requirements. Also, I always warned the teams I worked with that it's very easy to not see the forest for the trees. Server Timing API was bringing a convenient way to communicate backend performance information to developer tools in the browser. It enabled access to back-end and front-end performance data in one place and within the context of actual interaction with the application. I've experimented with the technology together with a couple of teams where a culture of fronted and backend engineers working close together was high. The results were great, which pushed me to create a small <a href=\"https://www.nuget.org/packages/Lib.AspNetCore.ServerTiming\">library</a> to simplify the onboarding of Server Timing API in ASP.NET Core applications. I've been using that library with multiple teams through the years and judging by the downloads number I wasn't the only one. There were even some contributions to the library.</p>\n<p>Some time ago, <a href=\"https://github.com/tpeczek/Lib.AspNetCore.ServerTiming/issues/25\">an issue</a> was raised asking if the library could also support Azure Functions using isolated worker process mode. I couldn't think of a good reason why not, it was a great idea. Of course, I couldn't add the support directly to the existing library. Yes, the isolated worker process mode of Azure Functions shares a lot of concepts with ASP.NET Core, but the technicalities are different. So, I've decided to create a separate <a href=\"https://www.nuget.org/packages/Lib.Azure.Functions.Worker.ServerTiming\">library</a>. While doing so, I've also decided to put some notes around those concepts into a blog post in hope that someone might find them useful in the future.</p>\n<p>So, first things first, what is isolated worker process mode and why we are talking about it?</p>\n<h2 id=\"azure-functions-execution-modes\">Azure Functions Execution Modes</h2>\n<p>There are two execution modes in Azure Functions: in-process and isolated worker process. The in-process mode means that the function code is running in the same process as the host. This is the approach that has been taken for .NET functions from the beginning (while functions in other languages were running in a separate process since version 2). This enabled Azure Functions to provide unique benefits for .NET functions (like rich bindings and direct access to SDKs) but at a price. The .NET functions could only use the same .NET version as the host. Dependency conflicts were also common. This is why Azure Functions has fully embraced the isolated worker process mode for .NET functions in version 4 and now developers have a choice of which mode they want to use. Sometimes this choice is simple (if you want to use non-LTS versions of .NET, the isolated worker process is your only option), sometimes it is more nuanced (for example isolated worker process functions have slightly longer cold start). You can take a look at the full list of differences <a href=\"https://learn.microsoft.com/en-us/azure/azure-functions/dotnet-isolated-in-process-differences?WT.mc_id=DT-MVP-5002979\">here</a>.</p>\n<p>When considering simplifying the onboarding of Server Timing API, the isolated worker process mode is the only option as it supports a crucial feature - custom middleware registration.</p>\n<h2 id=\"custom-middleware\">Custom Middleware</h2>\n<p>The ability to register a custom middleware is crucial for enabling capabilities like Server Timing API because it allows for injecting logic into the invocation pipeline.</p>\n<p>In isolated worker process Azure Functions, the invocation pipeline is represented by <code>FunctionExecutionDelegate</code>. Although it would be possible to work with <code>FunctionExecutionDelegate</code> directly (by wrapping it with parent invocations), Azure Functions provides a convenient extension method <code>UseMiddleware()</code> which enables registering inline or factory-based middleware. What is missing in comparison to ASP.NET Core is the convention-based middleware. This might be surprising at first as the convention-based approach is probably the most popular one in ASP.NET Core. So, for those of you who are not familiar with the factory-based approach, it requires the middleware class to implement a specific interface. In the case of Azure Functions, it's <code>IFunctionsWorkerMiddleware</code> (in ASP.NET Core it's <code>IMiddleware</code>). The factory-based middleware is prepared to be registered with different lifetimes, so the <code>Invoke</code> method takes not only the context but also the delegate representing the next middleware in the pipeline as a parameter. Similarly to ASP.NET Core, we are being given the option to run code before and after functions execute, by wrapping it around the call to the next middleware delegate.</p>\n<pre><code class=\"lang-cs\">internal class ServerTimingMiddleware : IFunctionsWorkerMiddleware\n{\n    public async Task Invoke(FunctionContext context, FunctionExecutionDelegate next)\n    {\n        // Pre-function execution\n\n        await next(context);\n\n        // Post-function execution\n    }\n}\n</code></pre>\n<p>The aforementioned <code>UseMiddleware()</code> extension method should be called inside the <code>ConfigureFunctionsWorkerDefaults</code> method as part of the host preparation steps. This method registers the middleware as a singleton (so it has the same lifetime as convention-based middleware in ASP.NET Core). It can be registered with different lifetimes, but it has to be done manually which includes wrapping invocation of <code>FunctionExecutionDelegate</code>. For the ones interested I recommend checking the <a href=\"https://github.com/Azure/azure-functions-dotnet-worker/blob/7c6f442b687f0b8db4427d01b35068855f18b754/src/DotNetWorker.Core/Hosting/WorkerMiddlewareWorkerApplicationBuilderExtensions.cs#L96\"><code>UseMiddleware()</code> source code</a> for inspiration.</p>\n<pre><code class=\"lang-cs\">var host = new HostBuilder()\n    .ConfigureFunctionsWorkerDefaults(workerApplication =&gt;\n    {\n        // Register middleware with the worker\n        workerApplication.UseMiddleware();\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>All the valuable information about the invoked function and the invocation itself can be accessed through <code>FunctionContext</code> class. There are also some extension methods available for it, which make it easier to work with that class. One such extension method is <code>GetHttpResponseData()</code> which will return an instance of <code>HttpResponseData</code> if the function has been invoked by an HTTP trigger. This is where the HTTP response can be modified, for example by adding headers related to Server Timing API.</p>\n<pre><code class=\"lang-cs\">internal class ServerTimingMiddleware : IFunctionsWorkerMiddleware\n{\n    public async Task Invoke(FunctionContext context, FunctionExecutionDelegate next)\n    {\n        // Pre-function execution\n\n        await next(context);\n\n        // Post-function execution\n        HttpResponseData? response = context.GetHttpResponseData();\n        if (response is not null)\n        {\n            response.Headers.Add(\n                \"Server-Timing\",\n                \"cache;dur=300;desc=\\\"Cache\\\",sql;dur=900;desc=\\\"Sql Server\\\",fs;dur=600;desc=\\\"FileSystem\\\",cpu;dur=1230;desc=\\\"Total CPU\\\"\"\n            );\n        }\n    }\n}\n</code></pre>\n<p>To make this functional, the values for the header need to be gathered during the invocation, which means that there needs to be a shared service between the function and the middleware. It's time to bring the dependency injection into the picture.</p>\n<h2 id=\"dependency-injection\">Dependency Injection</h2>\n<p>The support for dependency injection in isolated worker process Azure Functions is exactly what you can expect if you have been working with modern .NET. It's based on <code>Microsoft.Extensions.DependencyInjection</code> and supports all lifetimes options. The option which might require clarification is the scoped lifetime. In Azure Functions, it matches a function execution lifetime, which is exactly what is needed for gathering values in the context of a single invocation.</p>\n<pre><code class=\"lang-cs\">var host = new HostBuilder()\n    ...\n    .ConfigureServices(s =&gt;\n    {\n        s.AddScoped();\n    })\n    .Build();\n\nhost.Run();\n</code></pre>\n<p>Functions that are using dependency injection must be implemented as instance methods. When using instance methods, each invocation will create a new instance of the function class. That means that all parameters passed into the constructor of the function class are scoped to that invocation. This makes usage of constructor-based dependency injection safe for services with scoped lifetime.</p>\n<pre><code class=\"lang-cs\">public class ServerTimingFunctions\n{\n    private readonly IServerTiming _serverTiming;\n\n    public ServerTimingFunctions(IServerTiming serverTiming)\n    {\n        _serverTiming = serverTiming;\n    }\n\n    [Function(\"basic\")]\n    public HttpResponseData Basic([HttpTrigger(AuthorizationLevel.Anonymous, \"get\")] HttpRequestData request)\n    {\n\n        var response = request.CreateResponse(HttpStatusCode.OK);\n        response.Headers.Add(\"Content-Type\", \"text/plain; charset=utf-8\");\n\n        _serverTiming.Metrics.Add(new ServerTimingMetric(\"cache\", 300, \"Cache\"));\n        _serverTiming.Metrics.Add(new ServerTimingMetric(\"sql\", 900, \"Sql Server\"));\n        _serverTiming.Metrics.Add(new ServerTimingMetric(\"fs\", 600, \"FileSystem\"));\n        _serverTiming.Metrics.Add(new ServerTimingMetric(\"cpu\", 1230, \"Total CPU\"));\n\n        response.WriteString(\"-- Demo.Azure.Functions.Worker.ServerTiming --\");\n\n        return response;\n    }\n}\n</code></pre>\n<p>The above statement is not true for the middleware. As I've already mentioned, the <code>UseMiddleware()</code> method registers the middleware as a singleton. So, even though middleware is being resolved for every invocation separately, it is always the same instance. This means that constructor-based dependency injection is safe only for services with a singleton lifetime. To properly use a service with scoped or transient lifetime we need to use the service locator approach. An invocation-scoped service locator is available for us under <code>FunctionContext.InstanceServices</code> property.</p>\n<pre><code class=\"lang-cs\">internal class ServerTimingMiddleware : IFunctionsWorkerMiddleware\n{\n    public async Task Invoke(FunctionContext context, FunctionExecutionDelegate next)\n    {\n        ...\n\n        // Post-function execution\n        InvocationResult invocationResult = context.GetInvocationResult();\n\n        HttpResponseData? response = invocationResult.Value as HttpResponseData;\n        if (response is not null)\n        {\n            IServerTiming serverTiming = context.InstanceServices.GetRequiredService();\n            response.Headers.Add(\"Server-Timing\", String.Join(\",\", serverTiming.Metrics));\n        }\n    }\n}\n</code></pre>\n<h2 id=\"it-works-and-you-can-use-it-\">It Works! (And You Can Use It)</h2>\n<p>This way, by combining support for middleware and dependency injection, I've established the core functionality of my <a href=\"https://www.nuget.org/packages/Lib.Azure.Functions.Worker.ServerTiming\">small library</a>. It's out there on NuGet, so if you want to use Server Timing to communicate performance information to your Azure Functions based API consumers you are welcome to use it. If you want to dig a little bit into the code (or maybe you have some suggestions or improvements in mind) it lives in the same <a href=\"https://github.com/tpeczek/Lib.AspNetCore.ServerTiming\">repository</a> as the ASP.NET Core one.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2023/03/server-timing-api-meets-isolated-worker.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-7005159588109239934",
      "Title": "Experimenting With .NET & WebAssembly - Running .NET Based Spin Application On WASI Node Pool in AKS",
      "PublishDate": "2022-12-20T14:17:00+00:00",
      "Summary": "<p>I'm quite an enthusiast of <a href=\"https://webassembly.org/\">WebAssembly</a> beyond the browser. It's already made its way into edge computing with <em>WasmEdge</em>, <em>Cloudflare Workers</em>, or <em>EdgeWorkers</em>. It's also made its way into cloud computing with dedicated clouds like <em>wasmCloud</em> or <em>Fermyon Cloud</em>. So it shouldn't be a surprise that large cloud vendors are starting to experiment with bringing WASM to their platforms as well. In the case of Azure (my cloud of choice), it's running WASM workloads on <a href=\"https://wasi.dev/\">WASI</a> node pools in Azure Kubernetes Service. This is great because since Steve Sanderson showed an experimental <a href=\"https://github.com/SteveSandersonMS/dotnet-wasi-sdk\">WASI SDK for .NET Core</a> back in March, I was looking for a good context to play with it too.</p>\n<p>I took my first look at WASM/WASI node pools for AKS a couple of months ago. Back then the feature was based on <a href=\"https://krustlet.dev/\">Krustlet</a> but I've quickly learned that the team is moving away from this approach and the feature doesn't work with the current version of the AKS control plane (it's a preview, it has that right). I've decided to wait. Time has passed, Deis Labs has evolved its <a href=\"https://deislabs.io/posts/helpful-webassembly-resources/\">tooling for running WebAssembly in Kubernetes</a> from Krustlet to ContainerD shims, and the WASM/WASI node pools for AKS feature has embraced it. I've decided to take a look at it again.</p>\n<p>The current implementation of WASM/WASI node pools provides support for two ContainerD shims: <a href=\"https://developer.fermyon.com/spin/index\">Spin</a> and <a href=\"https://github.com/deislabs/spiderlightning#spiderlightning-or-slight\">SpiderLightning</a>. Both, Spin and Slight (alternative name for SpiderLightning) provide structure and interfaces for building distributed event-driven applications built from WebAssembly components. After inspecting both of them, I've decided to go with Spin for two reasons:</p>\n<ul>\n<li>Spin is a framework for building applications in <a href=\"https://developer.fermyon.com/cloud/index\">Fermyon Cloud</a>. That meant a potentially stronger ecosystem and community. Also whatever I would learn, would have a broader application (not only WASM/WASI node pools for AKS).</li>\n<li>Spin has (an alpha but still) a <a href=\"https://github.com/fermyon/spin-dotnet-sdk\">.NET SDK</a>.</li>\n</ul>\n<h2 id=\"your-scientists-were-so-preoccupied-with-whether-they-could-they-didn-t-stop-to-think-if-they-should\">Your Scientists Were So Preoccupied With Whether They Could, They Didn't Stop to Think if They Should</h2>\n<p>When Steve Sanderson revealed the experimental WASI SDK for .NET Core, he showed that you can use it to run an ASP.NET Core server in a browser. He also clearly stated you absolutely shouldn't do that. Thinking about compiling .NET to WebAssembly and running it in AKS can make one wonder if it is the same case. After all, we can just run .NET in a container. Well, I believe it makes sense. WebAssembly apps have several advantages over containers:</p>\n<ul>\n<li>WebAssembly apps are smaller than containers. In general, size is an Achilles' heel of .NET, but, even for the sample application, I've used here the WASM version is about ten times smaller than a container based on <code>dotnet/runtime:7.0</code> (18.81 MB vs 190 MB).</li>\n<li>WebAssembly apps start faster and execute faster than containers. This is something I haven't measured myself yet, but this <a href=\"https://arxiv.org/ftp/arxiv/papers/2010/2010.07115.pdf\">paper</a> seems to make quite a strong case for it.</li>\n<li>WebAssembly apps are more secure than containers. This one is a killer aspect for me. Containers are not secure by default and significant effort has to be put to secure them. WebAssembly sandbox is secure by default.</li>\n</ul>\n<p>This is why I believe exploring this truly makes sense.</p>\n<p>But before we go further I want to highlight one thing - almost everything I'm using in this post is currently either an alpha or in preview. It's early and subject to change.</p>\n<h2 id=\"configuring-azure-cli-and-azure-subscription-to-support-wasi-node-pools\">Configuring Azure CLI and Azure Subscription to Support WASI Node Pools</h2>\n<p>Working with preview features in Azure requires some preparation. The first step is registering the feature in your subscription</p>\n<pre><code class=\"lang-ps\">az feature register \\\n    --namespace Microsoft.ContainerService \\\n    --name WasmNodePoolPreview\n</code></pre>\n<p>The registration takes some time, you can query the features list to see if it has been completed.</p>\n<pre><code class=\"lang-ps\">az feature list \\\n    --query \"[?contains(name, 'Microsoft.ContainerService/WasmNodePoolPreview')].{Name:name,State:properties.state}\" \\\n    -o table\n</code></pre>\n<p>Once it's completed, the resource provider for AKS must be refreshed to pick it up.</p>\n<pre><code class=\"lang-ps\">az provider register \\\n    --namespace Microsoft.ContainerService\n</code></pre>\n<p>The subscription part is now ready, but to be able to use the feature you also have to add the preview extension to Azure CLI (WASM/WASI node pools can't be created from the Azure Portal).</p>\n<pre><code class=\"lang-ps\">az extension add \\\n    --name aks-preview \\\n    --upgrade\n</code></pre>\n<p>This is everything we need to start having fun with WASM/WASI node pools.</p>\n<h2 id=\"creating-an-aks-cluster\">Creating an AKS Cluster</h2>\n<p>A WASM/WASI node pool can't be used for a system node pool. This means that before we create one, we have to create a cluster with a system node pool. Something like on the diagram below should be enough.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg70L9o7hHu4KD-igajo07MFScYZNlxvog2cMflfqOEFmIy5QRWDG3nBu9V6yCBjw1oIgH86zPgSLyammbRaLgA1qa0jClEzKyzpMGAyd-YJWI76fzjUx1bVFvPukI8JJulQsNKup-EWKXT9_MpJ6rVU5cRBvo0ZHQkZ-3ZBwVB5vi85JxDsUh87SJa/s1600/aks-cluster.png\" alt=\"AKS Cluster\"></p>\n<p>If you are familiar with spinning up an AKS cluster you can jump directly to the next section.</p>\n<p>If you are looking for something to copy and paste, the below commands will create a resource group, container registry, and cluster with a single node in the system node pool.</p>\n<pre><code class=\"lang-ps\">az group create \\\n    -l ${LOCATION} \\\n    -g ${RESOURCE_GROUP}\n\naz acr create \\\n    -n ${CONTAINER_REGISTRY} \\\n    -g ${RESOURCE_GROUP} \\\n    --sku Basic\n\naz aks create \\\n    -n ${AKS_CLUSTER} \\\n    -g ${RESOURCE_GROUP} \\\n    -c 1 \\\n    --generate-ssh-keys \\\n    --attach-acr ${CONTAINER_REGISTRY}\n</code></pre>\n<h2 id=\"adding-a-wasm-wasi-node-pool-to-the-aks-cluster\">Adding a WASM/WASI Node Pool to the AKS Cluster</h2>\n<p>A WASM/WASI node pool can be added to the cluster as any other node pool, with <code>az aks nodepool add</code> command. The part which makes it special is the <code>workload-runtime</code> parameter which takes a value of <code>WasmWasi</code>.</p>\n<pre><code class=\"lang-ps\">az aks nodepool add \\\n    -n ${WASI_NODE_POLL} \\\n    -g ${RESOURCE_GROUP} \\\n    -c 1 \\\n    --cluster-name ${AKS_CLUSTER} \\\n    --workload-runtime WasmWasi\n</code></pre>\n<p>The updated diagram representing the deployment looks like this.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9in7S1O-f-hpWRB1LpIruuZwsFzo9jbJP_PhJ__Ts1B75KQ82V5-JmryM2WRHQrKGuFkTb1L_qBTqaWUHgVc5Ia2N6o1GGRmGXxmYW7A6Aj1RJuMLC40wLMJEas7eWFrmazbch7gMWnxLItdzrgdwGINVz4C-XkcGs0qZaClelojHAuQdpPfGyPX5/s1600/aks-cluster-with-a-wasi-node-pool.png\" alt=\"AKS Cluster With a WASI Node Pool\"></p>\n<p>You can inspect the WASM/WASI node pool by running <code>kubectl get nodes</code> and <code>kubectl describe node</code> commands.</p>\n<p>With the infrastructure in place, it's time to build a Spin application.</p>\n<h2 id=\"building-a-spin-application-with-net-7\">Building a Spin Application With .NET 7</h2>\n<p>A Spin application has a pretty straightforward structure:</p>\n<ul>\n<li>A Spin application manifest (<code>spin.toml</code> file).</li>\n<li>One or more WebAssembly components.</li>\n</ul>\n<p>The WebAssembly components are nothing else than event handlers, while the application manifest defines where there are located and maps them to triggers. The application mentions two triggers: HTTP and Redis. In the case of HTTP, you map components directly to routes.</p>\n<p>So, first we need a component that will serve as a handler. In the introduction, I've written that one of the reasons why I have chosen Spin was the availability of .NET SDK. Sadly, when I tried to build an application using it, the application failed to start. The reason for that was that Spin SDK has too many features. Among other things it allows for making outbound HTTP requests which require <code>wasi-outbound-http::request</code> module,  which is not present in WASM/WASI node pool (which makes sense as it's experimental and predicted to die once WASI networking APIs are stable).</p>\n<p>Luckily, a Spin application supports fallback to <a href=\"https://github.com/deislabs/wagi/blob/main/docs/README.md\">WAGI</a>. WAGI stands for WebAssembly Gateway Interface and is an implementation of <a href=\"https://www.rfc-editor.org/rfc/rfc3875\">CGI</a> (now that's a blast from the past). It enables writing the WASM component as a \"command line\" application that handles HTTP requests by reading its properties from environment variables and writing responses to the standard output. This means we should start by creating a new .NET console application.</p>\n<pre><code class=\"lang-ps\">dotnet new console -o Demo.Wasm.Spin\n</code></pre>\n<p>Next we need to add a reference to <code>Wasi</code>.Sdk` package.</p>\n<pre><code class=\"lang-ps\">dotnet add package Wasi.Sdk --prerelease\n</code></pre>\n<p>It's time for the code. The bare required minimum for WAGI is outputting a <code>Content-Type</code> header and an empty line that separates headers from body. If you want to include a body, it goes after that empty line.</p>\n<pre><code class=\"lang-cs\">using System.Runtime.InteropServices;\n\nConsole.WriteLine(\"Content-Type: text/plain\");\nConsole.WriteLine();\nConsole.WriteLine(\"-- Demo.Wasm.Spin --\");\n</code></pre>\n<p>With the component ready, it's time for the application manifest. The one below defines an application using the HTTP trigger and mapping the component to a top-level wildcard route (so it will catch all requests). The <code>executor</code> is how the fallback to WAGI is specified.</p>\n<pre><code class=\"lang-toml\">spin_version = \"1\"\nauthors = [\"Tomasz Peczek &lt;tpeczek@gmail.com&gt;\"]\ndescription = \"Basic Spin application with .NET 7\"\nname = \"spin-with-dotnet-7\"\ntrigger = { type = \"http\", base = \"/\" }\nversion = \"1.0.0\"\n\n[[component]]\nid = \"demo-wasm-spin\"\nsource = \"Demo.Wasm.Spin/bin/Release/net7.0/Demo.Wasm.Spin.wasm\"\n[component.trigger]\nroute = \"/...\"\nexecutor = { type = \"wagi\" }\n</code></pre>\n<p>The last missing part is a <code>Dockerfile</code> which will allow us to build an image for deployment.</p>\n<pre><code class=\"lang-Dockerfile\">FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build\n\nWORKDIR /src\nCOPY . .\nRUN dotnet build -c Release\n\nFROM scratch\nCOPY --from=build /src/bin/Release/net7.0/Demo.Wasm.Spin.wasm ./bin/Release/net7.0/Demo.Wasm.Spin.wasm\nCOPY --from=build /src/spin.toml .\n</code></pre>\n<p>To run the image on WASM/WASI node pool it needs to be built and pushed to the container registry.</p>\n<pre><code class=\"lang-ps\">az acr login -n ${CONTAINER_REGISTRY}\ndocker build . -t ${CONTAINER_REGISTRY}.azurecr.io/spin-with-dotnet-7:latest\ndocker push ${CONTAINER_REGISTRY}.azurecr.io/spin-with-dotnet-7:latest\n</code></pre>\n<h2 id=\"running-a-spin-application-in-wasm-wasi-node-pool\">Running a Spin Application in WASM/WASI Node Pool</h2>\n<p>To run the Spin application we need to create proper resources in our AKS cluster. First is <code>RuntimeClass</code> which serves as a selection mechanism, so the Pods run on the WASM/WASI node pool. There are two node selectors related to WASM/WASI node pool <code>kubernetes.azure.com/wasmtime-spin-v1</code> and <code>kubernetes.azure.com/wasmtime-slight-v1</code>, with <code>spin</code> and <code>slight</code> being their respective handlers. In our case, we only care about creating a <code>RuntimeClass</code> for <code>kubernetes.azure.com/wasmtime-spin-v1</code>.</p>\n<pre><code class=\"lang-yaml\">apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: \"wasmtime-spin-v1\"\nhandler: \"spin\"\nscheduling:\n  nodeSelector:\n    \"kubernetes.azure.com/wasmtime-spin-v1\": \"true\"\n</code></pre>\n<p>With the <code>RuntimeClass</code> in place, we can define a <code>Deployment</code>.</p>\n<pre><code class=\"lang-yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spin-with-dotnet-7\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spin-with-dotnet-7\n  template:\n    metadata:\n      labels:\n        app: spin-with-dotnet-7\n    spec:\n      runtimeClassName: wasmtime-spin-v1\n      containers:\n        - name: spin-with-dotnet-7\n          image: crdotnetwasi.azurecr.io/spin-with-dotnet-7:latest\n          command: [\"/\"]\n</code></pre>\n<p>Last part is exposing our Spin application to the world. As this is just a demo I've decided to expose it directly as a <code>Service</code> of type <code>LoadBalancer</code>.</p>\n<pre><code class=\"lang-yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: spin-with-dotnet-7\nspec:\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  selector:\n    app: spin-with-dotnet-7\n  type: LoadBalancer\n</code></pre>\n<p>Now we can run <code>kubectl apply</code> and after a moment <code>kubectl get svc</code> to retrieve the IP address of the <code>Service</code>. You can paste that address into a browser and voilà.</p>\n<h2 id=\"that-was-fun-\">That Was Fun!</h2>\n<p>Yes, that was really fun. All the stuff used here is still early bits, but it already shows possibilities. I intend to observe this space closely and possibly revisit it whenever some updates happen.</p>\n<p>If you want to play with a ready-to-use demo, it's available on <a href=\"https://github.com/tpeczek/demo-dotnet-on-aks-wasi-node-pool\">GitHub</a> with a workflow ready to deploy it to Azure.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/12/experimenting-with-net-webassembly.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-390007151575047369",
      "Title": "Micro Frontends in Action With ASP.NET Core - Universal Rendering With Blazor WebAssembly Based Web Components",
      "PublishDate": "2022-10-25T12:24:00+00:00",
      "Summary": "<p>In the last two posts of this series on implementing the <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction\">samples</a> in ASP.NET Core, I've focused on Blazor WebAssembly based Web Components as a way to achieve client-side composition. As a result, we have well-encapsulated frontend parts which can communicate with each other and the page. But there is a problem with the client-side rendered fragments, they appear after a delay. While the page loads, the user sees an empty placeholder. This is for sure a bad user experience, but it has even more serious consequences, those fragments may not be visible to search engine crawlers. In the case of something like a buy button, it is very important. So, how to deal with this problem? A possible answer is <em>universal rendering</em>.</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html\">Server-Side Routing via YARP in Azure Container Apps</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/07/micro-frontends-in-action-with-aspnet.html\">Composition via YARP Transforms and Server-Side Includes (SSI)</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">Composition via Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/09/micro-frontends-in-action-with-aspnet.html\">Communication Patterns for Blazor WebAssembly Based Web Components</a></li>\n<li>Universal Rendering With Blazor WebAssembly Based Web Components</li>\n</ul>\n<h2 id=\"what-is-universal-rendering-\">What Is Universal Rendering?</h2>\n<p>Universal rendering is about combining server-side and client-side rendering in a way that enables having a single codebase for both purposes. The typical approach is to handle the initial HTML rendering on the server with help of the server-side composition and then, when the page is loaded in the browser, seamlessly rerender the fragments on the client side. The initial rendering should only generate the static markup, while the rerender brings the full functionality. When done properly, this allows for a fast <em>First Contentful Paint</em> while maintaining encapsulation.</p>\n<p>The biggest challenge is usually the single codebase, which in this case means rendering Blazor WebAssembly based Web Components on the server.</p>\n<h2 id=\"server-side-rendering-for-blazor-webassembly-based-web-components\">Server-Side Rendering for Blazor WebAssembly Based Web Components</h2>\n<p>There is no standard approach to rendering Web Components on the server. Usually, that requires some creative solutions. But Blazor WebAssembly based Web Components are different because on the server they are Razor components and ASP.NET Core provides support for <a href=\"https://learn.microsoft.com/en-us/aspnet/core/blazor/components/prerendering-and-integration?pivots=webassembly&amp;WT.mc_id=DT-MVP-5002979\">prerendering Razor components</a>. This support comes in form of a <a href=\"https://learn.microsoft.com/en-us/aspnet/core/mvc/views/tag-helpers/built-in/component-tag-helper?WT.mc_id=DT-MVP-5002979\">Component Tag Helper</a>. But, before we get to it, we need to modify the Checkout service so it can return the rendered HTML. This is where the choice of <a href=\"https://learn.microsoft.com/en-us/aspnet/core/blazor/host-and-deploy/webassembly?view=aspnetcore-6.0&amp;WT.mc_id=DT-MVP-5002979#hosted-deployment-with-aspnet-core\">hosted deployment with ASP.NET Core</a> will be beneficial. We can modify the hosting application to support Blazor WebAssembly and controllers with views.</p>\n<pre><code class=\"lang-cs\">...\n\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddControllersWithViews();\n\nvar app = builder.Build();\n\n...\n\napp.UseBlazorFrameworkFiles();\napp.UseStaticFiles();\n\napp.UseRouting();\n\napp.MapControllerRoute(\n    name: \"checkout-fragments\",\n    pattern: \"fragment/buy/{sku}/{edition}\",\n    defaults: new { controller = \"Fragments\", action = \"Buy\" }\n);\n\napp.Run();\n\n...\n</code></pre>\n<p>The controller for the defined route doesn't need any sophisticated logic, it only needs to pass the parameters to the view. For simplicity, I've decided to go with a dictionary as a model.</p>\n<pre><code class=\"lang-cs\">public class FragmentsController : Controller\n{\n    public IActionResult Buy(string sku, string edition)\n    {\n        IDictionary&lt;string, string&gt; model = new Dictionar&lt;string, string&gt;\n        {\n            { \"Sku\", sku },\n            { \"Edition\", edition }\n        };\n\n        return View(\"Buy\", model);\n    }\n}\n</code></pre>\n<p>The only remaining thing is the view which will be using the Component Tag Helper. In general, two pieces of information should be provided to this tag helper: the type of the component and the render mode. There are multiple render modes that render different markers to be used for later bootstrapping, but here we want to use the <code>Static</code> mode which renders only static HTML.</p>\n<p>In addition to the component type and render mode, the Component Tag Helper also enables providing values for any component parameters with a <code>param-{ParameterName}</code> syntax. This is how we will pass the values from the model.</p>\n<pre><code class=\"lang-html\">@using Demo.AspNetCore.MicroFrontendsInAction.Checkout.Frontend.Components\n@addTagHelper *, Microsoft.AspNetCore.Mvc.TagHelpers\n@model IDictionary&lt;string, string&gt;\n\n&lt;component type=\"typeof(BuyButton)\" render-mode=\"Static\" param-Sku=\"@(Model[\"Sku\"])\" param-Edition=\"@(Model[\"Edition\"])\" /&gt;\n</code></pre>\n<p>If we start the Checkout service and use a browser to navigate to the controller route, we will see an exception complaining about the absence of <code>IBroadcastChannelService</code>. At runtime Razor components are classes and ASP.NET Core will need to satisfy the dependencies while creating an instance. Sadly there is no support for <a href=\"https://github.com/dotnet/razor-compiler/issues/357\">optional dependencies</a>. The options are either a workaround based on injecting <code>IServiceProvider</code> or making sure that the needed dependency is registered. I believe the latter to be more elegant.</p>\n<pre><code class=\"lang-cs\">...\n\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddBroadcastChannel();\nbuilder.Services.AddControllersWithViews();\n\nvar app = builder.Build();\n\n...\n</code></pre>\n<p>After this change, navigating to the controller route will display HTML, but in the case of the <code>BuyButton</code>, it is not exactly what we want. The <code>BuyButton</code> component contains the markup for a popup which is displayed upon clicking the button. The issue is, that the popup is hidden only with CSS. This is fine for the Web Component scenario (where the styles are already loaded when the component is being rendered) but not desired for this one. This is why I've decided to put a condition around the popup markup.</p>\n<pre><code class=\"lang-razor\">...\n\n&lt;button type=\"button\" @ref=\"_buttonElement\" @onclick=\"OnButtonClick\"&gt;\n    buy for @(String.IsNullOrWhiteSpace(Sku) || String.IsNullOrWhiteSpace(Edition)  ? \"???\" : _prices[Sku][Edition])\n&lt;/button&gt;\n@if (_confirmationVisible)\n{\n    &lt;div class=\"confirmation confirmation-visible\"&gt;\n        ...\n    &lt;/div&gt;\n}\n\n...\n</code></pre>\n<p>Now the HTML returned by the controller contains only the button markup.</p>\n<h2 id=\"combining-server-side-and-client-side-rendering\">Combining Server-Side and Client-Side Rendering</h2>\n<p>The Checkout service is now able to provide static HTML representing the <code>BuyButton</code> fragment, based on a single codebase. In the case of micro frontends that's not everything that is needed for universal rendering. The static HTML needs to be composed into the page before it's served. In this series, I've explored a single server-side composition technique based on <a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html#:~:text=YARP%20Transforms%20and%20Server%2DSide%20Includes%20(SSI\">YARP Transforms and Server-Side Includes</a>), so I've decided to reuse it. First, I've copied the code for the body transform from the <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/05-composition-via-yarp-and-ssi/Demo.AspNetCore.MicroFrontendsInAction.Proxy/Transforms/Ssi\">previous sample</a>. Then, I modified the routing in the proxy to transform the request coming to the Decide service. The same as previously, I've created a dedicated route for static content so it doesn't go through the transform unnecessarily.</p>\n<pre><code class=\"lang-cs\">...\n\nvar routes = new[]\n{\n    ...\n    new RouteConfig {\n        RouteId = Constants.ROOT_ROUTE_ID,\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = \"/\" },\n        Metadata = SsiTransformProvider.SsiEnabledMetadata\n    },\n    (new RouteConfig {\n        RouteId = Constants.DECIDE_ROUTE_ID + \"-static\",\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = Constants.DECIDE_ROUTE_PREFIX + \"/static/{**catch-all}\" }\n    }).WithTransformPathRemovePrefix(Constants.DECIDE_ROUTE_PREFIX),\n    (new RouteConfig {\n        RouteId = Constants.DECIDE_ROUTE_ID,\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = Constants.DECIDE_ROUTE_PREFIX + \"/{**catch-all}\" },\n        Metadata = SsiTransformProvider.SsiEnabledMetadata\n    }).WithTransformPathRemovePrefix(Constants.DECIDE_ROUTE_PREFIX),\n    ...\n};\n\n...\n\nbuilder.Services.AddReverseProxy()\n    .LoadFromMemory(routes, clusters);\n\n...\n</code></pre>\n<p>Now I could modify the markup returned by the Decide service by placing the SSI directives inside the tag representing the Custom Element.</p>\n<pre><code class=\"lang-html\">&lt;html&gt;\n  ...\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;div class=\"decide_details\"&gt;\n      &lt;checkout-buy sku=\"porsche\" edition=\"standard\"&gt;\n        &lt;!--#include virtual=\"/checkout/fragment/buy/porsche/standard\" --&gt;\n      &lt;/checkout-buy&gt;\n    &lt;/div&gt;\n    ...\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>This way the proxy can inject the static HTML into the markup while serving the initial response and once the JavaScript for Web Components is loaded they will be rerendered. We have achieved universal rendering.</p>\n<h2 id=\"what-about-progressive-enhancements-\">What About Progressive Enhancements?</h2>\n<p>You might have noticed that there is a problem hiding in this solution. It's deceiving the users. The page looks like it's fully loaded but it's not interactive. There is a delay (until the JavaScript is loaded) before clicking the <code>BuyButton</code> has any effect. This is where progressive enhancements come into play.</p>\n<p>I will not go into this subject further here, but one possible approach could be wrapping the button inside a form when the Checkout service is rendering static HTML.</p>\n<pre><code class=\"lang-html\">@using Demo.AspNetCore.MicroFrontendsInAction.Checkout.Frontend.Components\n@addTagHelper *, Microsoft.AspNetCore.Mvc.TagHelpers\n@model IDictionary&lt;string, string&gt;\n\n&lt;form asp-controller=\"Checkout\" asp-action=\"Buy\" method=\"post\"&gt;\n    &lt;input type=\"hidden\" name=\"sku\" valeu=\"@(Model[\"Sku\"])\"&gt;\n    &lt;input type=\"hidden\" name=\"edition\" valeu=\"@(Model[\"Edition\"])\"&gt;\n    &lt;component type=\"typeof(BuyButton)\" render-mode=\"Static\" param-Sku=\"@(Model[\"Sku\"])\" param-Edition=\"@(Model[\"Edition\"])\" /&gt;\n&lt;/form&gt;\n</code></pre>\n<p>Of course, that's not all the needed changes. The button would have to be rendered with <code>submit</code> type and the Checkout service needs to handle the POST request, redirect back to the product page, and manage the cart in the background.</p>\n<p>If you are interested in doing that exercise, the sample code with universal rendering that you can use as a starter is available on <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/16-universal-rendering-with-blazor-webassembly-based-web-components\">GitHub</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/10/micro-frontends-in-action-with-aspnet.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-2336112181316150606",
      "Title": "Commits Promotion Between GitHub and Azure Databricks",
      "PublishDate": "2022-10-11T12:30:00+00:00",
      "Summary": "<p>One of the projects I'm currently working on is utilizing Azure Databricks for its machine learning component. The machine learning engineers working on the project wanted to use <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/index-ide?WT.mc_id=DT-MVP-5002979\">external IDEs</a> for the development. Unfortunately, using external IDEs doesn't remove all needs for developing or testing directly in Azure Databricks. As we wanted our GitHub repository to be the only source of truth, we had to establish a commits promotion approach that would enable that.</p>\n<p>Azure Databricks has support for <a href=\"https://learn.microsoft.com/en-us/azure/databricks/repos/?WT.mc_id=DT-MVP-5002979\">Git integration</a>, so we've decided to start by using it to integrate Azure Databricks with GitHub.</p>\n<h2 id=\"configuring-github-credentials-in-azure-databricks\">Configuring GitHub Credentials in Azure Databricks</h2>\n<p>The first step in setting up Git integration with Azure Databricks is credentials configuration. This is something that every engineer needs to do independently, to enable syncing workspace with a specific branch. It requires the following actions:</p>\n<ol>\n<li>Login to GitHub, click the profile picture and go to <code>Settings</code> and then <code>Developer settings</code> at the bottom.</li>\n<li>On the <code>Settings / Developer settings</code> switch to <code>Personal access tokens</code> and click <code>Generate new token</code>.</li>\n<li><p>Fill in the form:</p>\n<ul>\n<li>Provide a recognizable <code>Note</code> for the token.</li>\n<li>Set the <code>Expiration</code> corresponding to the expected time of work on the project.</li>\n<li><p>Select the <code>repo</code> scope.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjraHucsgXEWDH1QsCDxpITQ3B5Awvjm8-jffbTKgXH5tdpyFKGjzQ_LS9pDg9hjrTmlCPcODCBxk7mqrbowBbDL5OrsTYYXEyz2x-4U2WNECS2_EjeeMKezFCm3B3o83XFrvZ9aNm2-cbJpJ05ztBToi5vtpLXqddC_BNUo6rBitP6Wg5pPqqtGWOb/s1600/github-new-pat-form.png\" alt=\"GitHub - New Personal Access Token Form\"></p>\n</li>\n</ul>\n</li>\n<li>Click <code>Generate token</code> and copy the generated string.</li>\n<li>Launch the Azure Databricks workspace.</li>\n<li>Click the workspace name in the top right corner and then click the <code>User Settings</code>.</li>\n<li><p>On the <code>Git Integration</code> tab select <code>GitHub</code>, provide your username, paste the copied token, and click <code>Save</code>.</p>\n<p> <img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwMRSDVEpICBQ0ljnD7fEaxuF0tpz6I6vcOw9AFHHzdIYhUXd3IVZC_65cXr-3AOsEYlZzAl5j0XLJq8yj1mVLWVXsvOo9kZylBg6QSKOvZgs_VKW_3c-nAuE9rP4E5v9GikxK-kEgxCXMyF57iTcKQHrfV7QI_hLC_siZ9fIbXrOkEz4Xtv5RmcjK/s1600/azure-databricks-git-integration.png\" alt=\"Azure Databricks - Git Integration\"></p>\n</li>\n</ol>\n<p>Once the credentials to GitHub have been configured, the next step is the creation of an Azure Databricks Repo.</p>\n<h2 id=\"creating-azure-databricks-repo-based-on-github-repository\">Creating Azure Databricks Repo Based on GitHub Repository</h2>\n<p>An Azure Databricks Repo is a clone of your remote Git repository (in this case GitHub repository) which can be managed through Azure Databricks UI. The creation process also happens through UI:</p>\n<ol>\n<li>Launch the Azure Databricks workspace.</li>\n<li>From the left menu choose <code>Repos</code> and then click <code>Add Repo</code>.</li>\n<li><p>Fill in the form:</p>\n<ul>\n<li>Check the <code>Create repo by cloning a Git repository</code>.</li>\n<li>Select <code>GitHub</code> as <code>Git provider</code>.</li>\n<li>Provide the <code>Git repository URL</code>.</li>\n<li><p>The <code>Repository name</code> will auto-populate, but you can modify it to your liking.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvUqxc8E8HUQAy3ThVk8NcUTn-52v5H_ctAGXv4GIWP3Y9oRd2pkFV3CQkrV-qAAEIEb2v6tm0l7I8Z55dDh5wnrA_lJgYHpAkuXSzAyiPA4cONuKsVrJeABnhjUp_FTUxC08sn4s75hXuzRuR1x7VgK-YAsr-yjgU5JDEtaHuP_W2Z_XHoUySlthD/s1600/azure-databricks-add-repo.png\" alt=\"Azure Databricks - Add Repo\"></p>\n</li>\n</ul>\n</li>\n<li>Click <code>Submit</code>.</li>\n</ol>\n<p>And it's done. You can now select a branch next to the newly created Azure Databricks Repo. If you wish you can click the down arrow next to the repo/branch name and create a notebook, folder, or file. If the notebook you want to develop in has been already in the cloned repository, you can just select it and start developing.</p>\n<h2 id=\"promoting-commits-from-azure-databricks-repo-to-github-repository\">Promoting Commits From Azure Databricks Repo to GitHub Repository</h2>\n<p>As I've already mentioned, Azure Databricks Repo is managed through the UI. The Git dialog is accessible through the down arrow next to the repo/branch name or directly from the notebook through a button placed next to the name of the notebook (the label of the button is the current Git branch name). From the Git dialog, you can commit and push changes to the GitHub repository.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwh4qc_mNmb2-xDGQCKFBRAdZKrbeoPVusv-93pVma6naXnEqncO2Gg9HBmXH1lBlk0lED1RWxF4nXuIcrudh0bPi9TN0CA_erHoXiERx661O3Mg7frRTvtnht09oQYch8gtauDcF5Sv4xvnI_EsnNVyN71pRnZd2qz1dbL8ja-MZmW17WKkP5bLLv/s1600/azure-databricks-git-dialog.png\" alt=\"Azure Databricks - Git Dialog\"></p>\n<p>If you are interested in other manual operations, like pulling changes or resolving merge conflicts, they are well described in the <a href=\"https://learn.microsoft.com/en-us/azure/databricks/repos/sync-remote-repo?WT.mc_id=DT-MVP-5002979\">documentation</a>. I'm not going to describe their details here, because those are the operations we wanted to avoid by performing the majority of development in external IDEs and automating commits promotion from GitHub to Azure Databricks Repo.</p>\n<h2 id=\"promoting-commits-from-github-repository-to-azure-databricks-repo\">Promoting Commits From GitHub Repository to Azure Databricks Repo</h2>\n<p>There are two ways to to manage Azure Databricks Repos programmatically: <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/repos?WT.mc_id=DT-MVP-5002979\">Repos API</a> and <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/repos-cli?WT.mc_id=DT-MVP-5002979\">Repos CLI</a>. As GitHub-hosted runners doesn't come with preinstalled Databricks CLI, we've decided to go with Repos API and PowerShell.</p>\n<p>We wanted a GitHub Actions workflow which would run on every push and update all Azure Databricks Repos mapped to the branch to which the push has happened. After going through API endpoints we came up with following flow.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1zizXP-x14tdFv4tEFBQfEQ9BwU-GrFB_TrNI41-RG3SqbGSo3xWCQnATFtJZN24SUFBtaDyiJo9dRedJfhCgSXbypvynQbH88jtOFIInE_Nrl0rdbYZ8DSa63fwLIb5dTT91kzk1Zvd7GrghTLn1BqTsToOCBFmn03srZiacZGC9m1ajjV4FiJ3N/s1600/commits-promotion-workflow.pn\" alt=\"GitHub Actions Workflow for Commits Promotion to Azure Databricks Repo\"></p>\n<p>Before we could start the implementation there was one more missing aspect - authentication.</p>\n<p>Azure Databricks can use an Azure AD service principal as an identity for an automated tool or a CI/CD process. Creation of a service principal and adding it to an Azure Databricks workspace is a multistep process, which is quite well described in the <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/service-principals?WT.mc_id=DT-MVP-5002979#--add-an-azure-ad-service-principal-to-an-azure-databricks-workspace\">documentation</a>. After going through it, you should be able to create the following actions secrets for your repository:</p>\n<ul>\n<li><code>AZURE_SP_CLIENT_ID</code> - Application (client) ID for the service principal.</li>\n<li><code>AZURE_SP_TENANT_ID</code> - Directory (tenant) ID for the service principal.</li>\n<li><code>AZURE_SP_CLIENT_SECRET</code> - Client secret for the service principal.</li>\n<li><code>AZURE_DATABRICKS_WORKSPACE_INSTANCE_NAME</code> - The Azure Databricks workspace instance name.</li>\n</ul>\n<p>With help of the first three of those secrets and the Microsoft identity platform REST API, we can obtain an Azure AD access token for the service principal. The request we need to make looks like this.</p>\n<pre><code>https://login.microsoftonline.com/&lt;AZURE_SP_TENANT_ID&gt;/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id=&lt;AZURE_SP_CLIENT_ID&gt;&amp;grant_type=client_credentials&amp;scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default&amp;client_secret=&lt;AZURE_SP_CLIENT_SECRET&gt;\n</code></pre><p>The magical scope value (the URL-encoded <code>2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default</code>) is a programmatic identifier for Azure Databricks. The response to this request is a JSON object which contains the Azure AD access token in the <code>access_token</code> field. The PowerShell script to make the request and retrieve the token can look like the one below (assuming that the secrets have been put into environment variables).</p>\n<pre><code class=\"lang-powershell\">$azureAdAccessTokenUri = \"https://login.microsoftonline.com/$env:AZURE_SP_TENANT_ID/oauth2/v2.0/token\"\n$azureAdAccessTokenHeaders = @{ \"Content-Type\" = \"application/x-www-form-urlencoded\" }\n$azureAdAccessTokenBody = \"client_id=$env:AZURE_SP_CLIENT_ID&amp;grant_type=client_credentials&amp;scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default&amp;client_secret=$env:AZURE_SP_CLIENT_SECRET\"\n\n$azureAdAccessTokenResponse = Invoke-RestMethod -Method POST -Uri $azureAdAccessTokenUri -Headers $azureAdAccessTokenHeaders -Body $azureAdAccessTokenBody\n$azureAdAccessToken = $azureAdAccessTokenResponse.access_token\n</code></pre>\n<p>Having the token, we can start making requests against Repos API. The first request we want to make in our flow is for getting the repos.</p>\n<pre><code class=\"lang-powershell\">$azureDatabricksReposUri = \"https://$env:AZURE_DATABRICKS_WORKSPACE_INSTANCE_NAME/api/2.0/repos\"\n$azureDatabricksReposHeaders = @{ Authorization = \"Bearer $azureAdAccessToken\" }\n\n$azureDatabricksReposResponse = Invoke-RestMethod -Method GET -Uri $azureDatabricksReposUri -Headers $azureDatabricksReposHeaders\n</code></pre>\n<p>The <code>$azureDatabricksReposHeaders</code> will be used for subsequent requests as well, because we assume that the access token shouldn't expire before all repos are updated (the default expiration time is ~60 minutes). There is one more assumption here - that there are no more than twenty repos. The results from the <code>/repos</code> endpoint are paginated (with twenty being the page size) which the above script ignores. If there are more than twenty repos, the script needs to be adjusted to handle that.</p>\n<p>Once we have all the repos we can iterate through them and update those which have matching URL (in case different repositories than the current one has also been mapped) and branch (so we don't perform unnecessary updates).</p>\n<pre><code class=\"lang-powershell\">$githubRepositoryUrl = $env:GITHUB_REPOSITORY_URL.replace(\"git://\",\"https://\")\n\nforeach ($azureDatabricksRepo in $azureDatabricksReposResponse.repos)\n{\n    if (($azureDatabricksRepo.url -eq $githubRepositoryUrl) -and ($azureDatabricksRepo.branch -eq $env:GITHUB_BRANCH_NAME))\n    {\n    $azureDatabricksRepoId = $azureDatabricksRepo.id;\n    $azureDatabricksRepoUri  = \"https://$env:AZURE_DATABRICKS_WORKSPACE_INSTANCE_NAME/api/2.0/repos/$azureDatabricksRepoId\"\n    $updateAzureDatabricksRepoBody = @{ \"branch\" = $azureDatabricksRepo.branch }\n\n    Invoke-RestMethod -Method PATCH -Uri $azureDatabricksRepoUri -Headers $azureDatabricksReposHeaders -Body ($updateAzureDatabricksRepoBody|ConvertTo-Json)\n    }\n}\n</code></pre>\n<p>The <code>GITHUB_REPOSITORY_URL</code> and <code>GITHUB_BRANCH_NAME</code> are being injected into environment variables from <code>github</code> context of the action.</p>\n<p>That's all the logic we need, you can find the complete workflow <a href=\"https://gist.github.com/tpeczek/eba5b757fb465dadf28cc11be85c1114\">here</a>. Sadly, at least in our case, it has thrown the following error on the first run.</p>\n<blockquote>\n<p>{\"error_code\":\"PERMISSION_DENIED\",\"message\":\"Missing Git | provider credentials. Go to User Settings &gt; Git Integration to | add your personal access token.\"}</p>\n</blockquote>\n<p>The error does make sense. After all, from the perspective of Azure Databricks, the service principal is a user and we have never configured GitHub credentials for that user. This raised two questions.</p>\n<p>The first question was, which GitHub user should those credentials represent? This is where the concept of a <em>GitHub machine user</em> comes into play. A GitHub machine user is a GitHub personal account, separate from the GitHub personal accounts of engineers/developers in your organization. It should be created against a dedicated email provided by your IT department and used only for automation scenarios.</p>\n<p>The second question was, how to configure the credentials. You can't launch the Azure Databricks workspace as the service principal user and do it through the UI. Luckily, Azure Databricks provides <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/gitcredentials?WT.mc_id=DT-MVP-5002979\">Git Credentials API</a> which can be used for this task. You can use Postman (or any other tool of your preference) to first make the described above request for Azure AD access token, and then make the below request to configure the credentials.</p>\n<pre><code>https://&lt;WORKSPACE_INSTANCE_NAME&gt;/api/2.0/git-credentials\nContent-Type: application/json\n\n{\n   \"personal_access_token\": \"&lt;GitHub Machine User Personal Access Token&gt;\",\n   \"git_username\": \"&lt;GitHub Machine User Username&gt;\",\n   \"git_provider\": \"GitHub\"\n}\n</code></pre><p>After this operation, the GitHub Actions workflow started working as expected.</p>\n<h2 id=\"what-this-is-not\">What This Is Not</h2>\n<p>This is not CI/CD for Azure Databricks. This is just a process supporting daily development in the Azure Databricks context. If you are looking for CI/CD approaches to Azure Databricks, you can take a look <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/index-ci-cd?WT.mc_id=DT-MVP-5002979\">here</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/10/commits-promotion-between-github-and.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-3822728770880711347",
      "Title": "Micro Frontends in Action With ASP.NET Core - Communication Patterns for Blazor WebAssembly Based Web Components",
      "PublishDate": "2022-09-12T19:11:00+00:00",
      "Summary": "<p>I'm continuing my series on implementing the <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction\">samples</a> in ASP.NET Core, and I'm continuing the subject of Blazor WebAssembly based Web Components. In the previous post, the project has been expanded with a new service that provides its fronted fragment as a Custom Element power by Blazor WebAssembly. In this post, I will explore how Custom Elements can communicate with other frontend parts.</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html\">Server-Side Routing via YARP in Azure Container Apps</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/07/micro-frontends-in-action-with-aspnet.html\">Composition via YARP Transforms and Server-Side Includes (SSI)</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">Composition via Blazor WebAssembly Based Web Components</a></li>\n<li>Communication Patterns for Blazor WebAssembly Based Web Components</li>\n<li><a href=\"https://www.tpeczek.com/2022/10/micro-frontends-in-action-with-aspnet.html\">Universal Rendering With Blazor WebAssembly Based Web Components</a></li>\n</ul>\n<p>There are three communication scenarios I would like to explore: passing information from page to Custom Element (parent to child), passing information from Custom Element to page (child to parent), and passing information between Custom Elements (child to child). Let's go through them one by one.</p>\n<h2 id=\"page-to-custom-element\">Page to Custom Element</h2>\n<p>When it comes to passing information from page to Custom Element, there is a standard approach that every web developer will expect. If I want to disable a button, I set an attribute. If I want to change the text on a button, I set an attribute. In general, if I want to change the state of an element, I set an attribute. The same expectation applies to Custom Elements. How to achieve that?</p>\n<p>As mentioned in the <a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">previous post</a>, the ES6 class, which represents a Custom Element, can implement a set of lifecycle methods. One of these methods is <code>attributeChangedCallback</code>. It will be invoked each time an attribute from a specified list is added, removed, or its value is changed. The list of the attributes which will result in invoking the <code>attributeChangedCallback</code> is defined by a value returned from <code>observedAttributes</code> static get method.</p>\n<p>So, in the case of Custom Elements implemented in JavaScript, one has to implement the <code>observedAttributes</code> to return an array of attributes that can modify the state of the Custom Element and implement the <code>attributeChangedCallback</code> to modify that state. Once again, you will be happy to know that all this work has already been done in the case of Blazor WebAssembly. The <a href=\"https://github.com/aspnet/AspLabs/tree/main/src/BlazorCustomElements\"><code>Microsoft.AspNetCore.Components.CustomElements</code></a> package, which wraps Blazor components as Custom Elements handles that. It provides an implementation of <code>observedAttributes</code> which returns all the properties marked as parameters, and an implementation of <code>attributeChangedCallback</code> which will update parameters values and give the component a chance to rerender. That makes the implementation quite simple.</p>\n<p>I've added a new property named <code>Edition</code> to the <code>BuyButton</code> component, which I created in the <a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">previous post</a>. The new property impacts the price depending if the client has chosen a standard or platinum edition. I've also marked the new property as a parameter.</p>\n<pre><code class=\"lang-razor\">&lt;button type=\"button\" @onclick=\"OnButtonClick\"&gt;\n    buy for @(String.IsNullOrWhiteSpace(Sku) || String.IsNullOrWhiteSpace(Edition)  ? \"???\" : _prices[Sku][Edition])\n&lt;/button&gt;\n...\n\n@code {\n    private IDictionary&lt;string, Dictionary&lt;string, int&gt;&gt; _prices = new Dictionary&lt;string, Dictionary&lt;string, int&gt;&gt;\n    {\n        { \"porsche\", new Dictionary&lt;string, int&gt; { { \"standard\", 66 }, { \"platinum\", 966 } } },\n        { \"fendt\", new Dictionary&lt;string, int&gt; { { \"standard\", 54 }, { \"platinum\", 945 } }  },\n        { \"eicher\", new Dictionary&lt;string, int&gt; { { \"standard\", 58 }, { \"platinum\", 958 } }  }\n    };\n\n    [Parameter]\n    public string? Sku { get; set; }\n\n    [Parameter]\n    public string? Edition { get; set; }\n\n    ...\n}\n</code></pre>\n<p>This should be all from the component perspective. The rest should be only about using the attribute representing the property. First, I've added it to the markup served by the Decide service with the default value. I've also added a checkbox that allows choosing the edition.</p>\n<pre><code class=\"lang-html\">&lt;html&gt;\n    ...\n    &lt;body class=\"decide_layout\"&gt;\n        ...\n        &lt;div class=\"decide_details\"&gt;\n            &lt;label class=\"decide_editions\"&gt;\n                &lt;p&gt;Material Upgrade?&lt;/p&gt;\n                &lt;input type=\"checkbox\" name=\"edition\" value=\"platinum\" /&gt;\n                &lt;span&gt;Platinum&lt;br /&gt;Edition&lt;/span&gt;\n                &lt;img src=\"https://mi-fr.org/img/porsche_platinum.svg\" /&gt;\n            &lt;/label&gt;\n            &lt;checkout-buy sku=\"porsche\" edition=\"standard\"&gt;&lt;/checkout-buy&gt;\n        &lt;/div&gt;\n        ...\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>Then I implemented an event handler for the <code>change</code> event of that checkbox, where depending on its state, I would change the value of the <code>edition</code> attribute on the custom element.</p>\n<pre><code class=\"lang-js\">(function() {\n    ...\n    const editionsInput = document.querySelector(\".decide_editions input\");\n    ...\n    const buyButton = document.querySelector(\"checkout-buy\");\n\n    ...\n\n    editionsInput.addEventListener(\"change\", e =&gt; {\n        const edition = e.target.checked ? \"platinum\" : \"standard\";\n        buyButton.setAttribute(\"edition\", edition);\n        ...\n    });\n})();\n</code></pre>\n<p>It worked without any issues. Checking and unchecking the checkbox would result in nicely displaying different prices on the button.</p>\n<h2 id=\"custom-element-to-page\">Custom Element to Page</h2>\n<p>The situation with passing information from Custom Element to the page is similar to passing information from page to Custom Element - there is an expected standard mechanism: events. If something important has occurred internally in the Custom Element and the external world should know about it, Custom Element should raise an event to which whoever is interested can subscribe.</p>\n<p>How to raise a JavaScript event from Blazor? This requires <a href=\"https://docs.microsoft.com/en-us/aspnet/core/blazor/javascript-interoperability/call-javascript-from-dotnet?view=aspnetcore-6.0&amp;WT.mc_id=DT-MVP-5002979\">calling a JavaScript function</a> which will wrap a call to <code>dispatchEvent</code>. Why can't <code>dispatchEvent</code> be called directly? That's because Blazor requires function identifier to be relative to the global scope, while <code>dispatchEvent</code> needs to be called on an instance of an element. This raises another challenge. Our wrapper function will require a reference to the Custom Element. Blazor supports capturing references to elements to pass them to JavaScript. The <code>@ref</code> attribute can be included in HTML element markup, resulting in a reference being stored in the variable it is pointing to. This means that the reference to the Custom Element itself can't be passed directly, but a reference to its child element can.</p>\n<p>I've written a wrapper function that takes the reference to the button element (but it could be any direct child of the Custom Element) as a parameter and then calls <code>dispatchEvent</code> on its parent.</p>\n<pre><code class=\"lang-js\">window.checkout = (function () {\n    return {\n        dispatchItemAddedEvent: function (checkoutBuyChildElement) {\n            checkoutBuyChildElement.parentElement.dispatchEvent(new CustomEvent(\"checkout:item_added\"));\n        }\n    };\n})();\n</code></pre>\n<p>I wanted the event to be raised when the button has been clicked, so I've modified the <code>OnButtonClick</code> to use injected <code>IJSRuntime</code> to call my JavaScript function. In the below code, you can also see the <code>@ref</code> attribute in action and how I'm passing that element reference to the wrapper function.</p>\n<pre><code class=\"lang-razor\">@using Microsoft.JSInterop\n\n@inject IJSRuntime JS\n\n&lt;button type=\"button\" @ref=\"_buttonElement\" @onclick=\"OnButtonClick\"&gt;\n    buy for @(String.IsNullOrWhiteSpace(Sku) || String.IsNullOrWhiteSpace(Edition)  ? \"???\" : _prices[Sku][Edition])\n&lt;/button&gt;\n...\n\n@code {\n    private ElementReference _buttonElement;\n\n    ...\n\n    private async Task OnButtonClick(MouseEventArgs e)\n    {\n        ...\n\n        await JS.InvokeVoidAsync(\"checkout.dispatchItemAddedEvent\", _buttonElement);\n    }\n\n    ...\n}\n</code></pre>\n<p>For the whole thing to work, I had to reference the JavaScript from the Decide service markup so that the wrapper function could be called.</p>\n<pre><code class=\"lang-html\">&lt;html&gt;\n    ...\n    &lt;body class=\"decide_layout\"&gt;\n        ...\n        &lt;script src=\"/checkout/static/components.js\"&gt;&lt;/script&gt;\n        &lt;script src=\"/checkout/_content/Microsoft.AspNetCore.Components.CustomElements/BlazorCustomElements.js\"&gt;&lt;/script&gt;\n        ...\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>Now I could subscribe to the <code>checkout:item_added</code> event and add some bells and whistles whenever it's raised.</p>\n<pre><code class=\"lang-js\">(function() {\n    ...\n    const productElement = document.querySelector(\".decide_product\");\n    const buyButton = document.querySelector(\"checkout-buy\");\n\n    ...\n\n    buyButton.addEventListener(\"checkout:item_added\", e =&gt; {\n        productElement.classList.add(\"decide_product--confirm\");\n    });\n\n    ...\n})();\n</code></pre>\n<h2 id=\"custom-element-to-custom-element\">Custom Element to Custom Element</h2>\n<p>Passing information between Custom Elements is where things get interesting. That is because there is no direct relation between Custom Elements. Let's assume that the Checkout service exposes a second Custom Element which provides a cart representation. The checkout button and mini-cart don't have to be used together. There might be a scenario where only one of them is present, or there might be scenarios where there are rendered by independent parents.</p>\n<p>Of course, everything is happening in the browser's context, so there is always an option to search through the entire DOM tree. This is an approach that should be avoided. First, it's tight coupling, as it requires Custom Element to have detailed knowledge about other Custom Element. Second, it wouldn't scale. What if there are ten different types of Custom Elements to which information should be passed? That would require ten different searches.</p>\n<p>Another option is leaving orchestration to the parent. The parent would listen to events from one Custom Element and change properties on the other. This breaks the separation of responsibilities as the parent (in our case, the Decide service) is now responsible for implementing logic that belongs to someone else (in our case, the Checkout Service).</p>\n<p>What is needed is a communication channel that will enable a publish-subscribe pattern. This will ensure proper decoupling. The classic implementation of such a channel is events based bus. The publisher raises events with bubbling enabled (by default, it's not), so subscribers can listen for those events on the <code>window</code> object. This is an established approach, but it's not the one I've decided to implement. An events-based bus is a little bit \"too public\" for me. In the case of multiple Custom Elements communicating, there are a lot of events on the <code>window</code> object, and I would prefer more organization. Luckily, modern browsers provide an alternative way to implement such a channel - the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Broadcast_Channel_API\"><em>Broadcast Channel API</em></a>. You can think about <em>Broadcast Channel API</em> as a simple message bus that provides the capability of creating named channels. The hidden power of <em>Broadcast Channel API</em> is that it allows communication between windows/tabs, iframes, web workers, and service workers.</p>\n<p>Using <em>Broadcast Channel API</em> in Blazor once again requires JavaScript interop. I've decided to use this opportunity to build a <a href=\"https://github.com/tpeczek/Blazor.BroadcastChannel\">component library</a> that provides easy access to it. I'm not going to describe the process of creating a component library in this post, but if you are interested just let me know and I'm happy to write a separate post about it. If you want to use the Broadcast Channel API, it's available on <a href=\"https://www.nuget.org/packages/Blazor.BroadcastChannel/\">NuGet</a>.</p>\n<p>After building and publishing the component library, I referenced it in the Checkout project and registered the service it provides.</p>\n<pre><code class=\"lang-cs\">...\n\nvar builder = WebAssemblyHostBuilder.CreateDefault(args);\n\nbuilder.RootComponents.RegisterAsCustomElement&lt;BuyButton&gt;(\"checkout-buy\");\n\nbuilder.Services.AddBroadcastChannel();\n\n...\n</code></pre>\n<p>In the checkout button component, I've injected the service. The channel can be created by calling <code>CreateOrJoinAsync</code>, and I'm doing that in <code>OnAfterRenderAsync</code>. I've also made the component implement <code>IAsyncDisposable</code>, where the channel is disposed to avoid JavaScript memory leaks. The last part was calling <code>PostMessageAsync</code> as part of <code>OnButtonClick</code> to send the message to the channel. This completes the publisher.</p>\n<pre><code class=\"lang-razor\">...\n\n@implements IAsyncDisposable\n\n...\n@inject IBroadcastChannelService BroadcastChannelService\n\n...\n\n@code {\n    ...\n    private IBroadcastChannel? _broadcastChannel;\n\n    ...\n\n    protected override async Task OnAfterRenderAsync(bool firstRender)\n    {\n        if (firstRender)\n        {\n            _broadcastChannel = await BroadcastChannelService.CreateOrJoinAsync(\"checkout:item-added\");\n        }\n    }\n\n    private async Task OnButtonClick(MouseEventArgs e)\n    {\n        ...\n\n        if (_broadcastChannel is not null)\n        {\n            await _broadcastChannel.PostMessageAsync(new CheckoutItem { Sku = Sku, Edition = Edition });\n        }\n\n        ...\n    }\n\n    ...\n\n    public async ValueTask DisposeAsync()\n    {\n        if (_broadcastChannel is not null)\n        {\n            await _broadcastChannel.DisposeAsync();\n        }\n    }\n}\n</code></pre>\n<p>The mini-cart component will be the subscriber. I've added there the same code for injecting the service, joining the channel, and disposing of it. The main difference here is that the component will subscribe to the channel <code>Message</code> event instead of sending anything. The <code>BroadcastChannelMessageEventArgs</code> contains the message which has been sent in the <code>Data</code> property as <code>JsonDocument</code>, which can be deserialized to the desired type. In the mini-cart component, I'm using the message to add items.</p>\n<pre><code class=\"lang-razor\">@using System.Text.Json;\n\n@implements IAsyncDisposable\n\n@inject IBroadcastChannelService BroadcastChannelService\n\n@(_items.Count == 0  ? \"Your cart is empty.\" : $\"You've picked {_items.Count} tractors:\")\n@foreach (var item in _items)\n{\n    &lt;img src=\"https://mi-fr.org/img/@(item.Sku)_@(item.Edition).svg\" /&gt;\n}\n\n@code {\n    private IList&lt;CheckoutItem&gt; _items = new List&lt;CheckoutItem&gt;();\n    private IBroadcastChannel? _broadcastChannel;\n    private JsonSerializerOptions _jsonSerializerOptions = new JsonSerializerOptions { PropertyNamingPolicy = JsonNamingPolicy.CamelCase };\n\n    protected override async Task OnAfterRenderAsync(bool firstRender)\n    {\n        if (firstRender)\n        {\n            _broadcastChannel = await BroadcastChannelService.CreateOrJoinAsync(\"checkout:item-added\");\n            _broadcastChannel.Message += OnMessage;\n        }\n    }\n\n    private void OnMessage(object? sender, BroadcastChannelMessageEventArgs e)\n    {\n        _items.Add(e.Data.Deserialize&lt;CheckoutItem&gt;(_jsonSerializerOptions));\n\n        StateHasChanged();\n    }\n\n    public async ValueTask DisposeAsync()\n    {\n        if (_broadcastChannel is not null)\n        {\n            await _broadcastChannel.DisposeAsync();\n        }\n    }\n}\n</code></pre>\n<p>The last thing I did in the Checkout service was exposing the mini-cart component.</p>\n<pre><code class=\"lang-cs\">...\n\nvar builder = WebAssemblyHostBuilder.CreateDefault(args);\n\nbuilder.RootComponents.RegisterAsCustomElement&lt;BuyButton&gt;(\"checkout-buy\");\nbuilder.RootComponents.RegisterAsCustomElement&lt;MiniCart&gt;(\"checkout-minicart\");\n\nbuilder.Services.AddBroadcastChannel();\n\n...\n</code></pre>\n<p>Now the mini-cart could be included in the HTML owned by the Decide service.</p>\n<pre><code class=\"lang-html\">&lt;html&gt;\n  ...\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;div class=\"decide_details\"&gt;\n      &lt;checkout-buy sku=\"porsche\"&gt;&lt;/checkout-buy&gt;\n    &lt;/div&gt;\n    ...\n    &lt;div class=\"decide_summary\"&gt;\n      &lt;checkout-minicart&gt;&lt;/checkout-minicart&gt;\n    &lt;/div&gt;\n    ...\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h2 id=\"playing-with-the-complete-sample\">Playing With the Complete Sample</h2>\n<p>The complete sample is available on <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/12-child-child-communication-with-blazor-webassembly-based-web-components\">GitHub</a>. You can run it locally by spinning up all the services, but I've also included a GitHub Actions <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/blob/main/.github/workflows/12-child-child-communication-with-blazor-webassembly-based-web-components.yml\">workflow</a> that can deploy the whole solution to Azure (you just need to fork the repository and provide your own credentials).</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/09/micro-frontends-in-action-with-aspnet.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-4412698974992852521",
      "Title": "Micro Frontends in Action With ASP.NET Core - Composition via Blazor WebAssembly Based Web Components",
      "PublishDate": "2022-08-17T19:21:00+00:00",
      "Summary": "<p>This is another post in my series on implementing <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction\">the samples</a> from <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> in ASP.NET Core:</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html\">Server-Side Routing via YARP in Azure Container Apps</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/07/micro-frontends-in-action-with-aspnet.html\">Composition via YARP Transforms and Server-Side Includes (SSI)</a></li>\n<li>Composition via Blazor WebAssembly Based Web Components</li>\n<li><a href=\"https://www.tpeczek.com/2022/09/micro-frontends-in-action-with-aspnet.html\">Communication Patterns for Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/10/micro-frontends-in-action-with-aspnet.html\">Universal Rendering With Blazor WebAssembly Based Web Components</a></li>\n</ul>\n<p>This time I'm jumping from server-side composition to client-side composition. I've used the word jumping because I haven't fully covered server-side composition yet. There is one more approach to server-side composition which I intend to cover later, but as lately I was doing some Blazor WebAssembly work I was more eager to write this one.</p>\n<h2 id=\"expanding-the-project\">Expanding The Project</h2>\n<p>As you may remember from the <a href=\"https://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html\">first post</a>, the project consists of two services that are hosted in <em>Azure Container Apps</em>.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfaCy5JrPShFrxsBWEP62y4i2oJto56TXmTI8zJuZEmbiptudsSfjU2WYij2hl-YdSPKsJrqQfFarRopYabKot2fz-69JzhCn4RTfJ22zXD2Ft1_AIWDjnbwfTHfDYkOqXNr5uoK5yRwBN9GrdGXP5gZU-3z-VLdh-LuURi6wD9UA8Ozro5BXeK4S1/s1600/decide-and-inspire-frontend-layout.png\" alt=\"Decide and Inspire Frontend Layout\"></p>\n<p>Both services are using server-side rendering for their frontends, and the Decide service is loading Inspire service frontend fragment via Ajax. It's time to bring a new service to the picture, the Checkout service.</p>\n<p>The Checkout service is responsible for checkout flow. As this flow is more sophisticated than what Decide and Inspire services provide, the Checkout service requires client-side rendering to provide the experience of a single page application. This requirement creates a need for isolation and encapsulation of the Checkout service frontend fragment. There is a suite of technologies that can help solve that problem - Web Components.</p>\n<h2 id=\"web-components\">Web Components</h2>\n<p>Web Components aim at enabling web developers to create reusable elements with well-encapsulated functionality. To achieve that, they bring together four different specifications:</p>\n<ul>\n<li><em>Custom Elements</em>, which allows defining your own tags (custom elements) with their business logic.</li>\n<li><em>Shadow DOM</em>, which enables scripting and styling without collisions with other elements.</li>\n<li><em>HTML Templates</em>, which allows writing markup templates.</li>\n<li><em>ES Module</em>, which defines a consistent way for JavaScript inclusion and reuse.</li>\n</ul>\n<p>The Custom Elements is the most interesting one in this context. The way to create a Custom Element is to implement an ES6 class that extends <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement\"><code>HTMLElement</code></a> and register it via <code>window.customElements.define</code>. The class can also implement a set of lifecycle methods (<code>constructor</code>, <code>connectedCallback</code>, <code>disconnectedCallback</code>, or <code>attributeChangedCallback</code>). This allows for initializing a SPA framework (Angular, React, Vue, etc.) and instructing it to use <code>this</code> as a root for rendering.</p>\n<p>This is exactly what is needed for the Checkout service, where the SPA framework will be Blazor WebAssembly.</p>\n<h2 id=\"creating-a-blazor-webassembly-based-custom-element\">Creating a Blazor WebAssembly Based Custom Element</h2>\n<p>The way Blazor WebAssembly works fits nicely with Custom Elements. If you've ever taken a look at the <code>Program.cs</code> of a Blazor WebAssembly project, you might have noticed some calls to <code>builder.RootComponents.Add</code>. This is because the WebAssembly to which your project gets compiled is designed to perform rendering into elements. Thanks to that a Blazor WebAssembly application can be wrapped into a Custom Element, it just requires proper initialization. You will be happy to learn, that this work has already been done. As part of <a href=\"https://github.com/aspnet/AspLabs/tree/main/src/BlazorCustomElements\">AspLabs</a> Steve Sanderson has prepared a package and instructions on how to make Blazor components available as Custom Elements. Let's do it.</p>\n<p>I've started with an empty Blazor WebAssembly application <a href=\"https://docs.microsoft.com/en-us/aspnet/core/blazor/host-and-deploy/webassembly?view=aspnetcore-6.0&amp;WT.mc_id=DT-MVP-5002979#hosted-deployment-with-aspnet-core\">hosted in ASP.NET Core</a>, to which I've added a component that will server as a button initiating the checkout flow (the final version of that component will also contain some confirmation toast, if you're interested you can find it <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/blob/main/08-composition-via-blazor-webassembly-based-web-components/Demo.AspNetCore.MicroFrontendsInAction.Checkout/Frontend/Components/BuyButton.razor\">here</a>).</p>\n<pre><code>&lt;button type=\"button\" @onclick=\"OnButtonClick\"&gt;buy for @(String.IsNullOrWhiteSpace(Sku) ? \"???\" : _prices[Sku])&lt;/button&gt;\n...\n\n@code {\n    // Dictionary of tractor prices\n    private IDictionary&lt;string, int&gt; _prices = new Dictionary&lt;string, int&gt;\n    {\n        { \"porsche\", 66 },\n        { \"fendt\", 54 },\n        { \"eicher\", 58 }\n    };\n\n    ...\n}\n</code></pre><p>Next, I've added the <code>Microsoft.AspNetCore.Components.CustomElements</code> package to the project.</p>\n<pre><code>&lt;Project Sdk=\"Microsoft.NET.Sdk.BlazorWebAssembly\"&gt;\n  ...\n  &lt;ItemGroup&gt;\n    ...\n    &lt;PackageReference Include=\"Microsoft.AspNetCore.Components.CustomElements\" Version=\"0.1.0-alpha.*\" /&gt;\n  &lt;/ItemGroup&gt;\n&lt;/Project&gt;\n</code></pre><p>I've removed all <code>.razor</code> files besides the above component and <code>_Imports.razor</code>. After that, I modified the <code>Program.cs</code> by removing the <code>builder.RootComponents.Add</code> calls and adding <code>builder.RootComponents.RegisterAsCustomElement</code> to expose my component as a <code>checkout-buy</code> element.</p>\n<pre><code>...\n\nvar builder = WebAssemblyHostBuilder.CreateDefault(args);\n\nbuilder.RootComponents.RegisterAsCustomElement&lt;BuyButton&gt;(\"checkout-buy\");\n\nawait builder.Build().RunAsync();\n</code></pre><p>This is it. After the build/publish the service will server the custom element through the <code>_content/Microsoft.AspNetCore.Components.CustomElements/BlazorCustomElements.js</code> script, so it's time to plug it into the page served by Decide service.</p>\n<h2 id=\"using-a-blazor-webassembly-based-custom-element\">Using a Blazor WebAssembly Based Custom Element</h2>\n<p>Before the Decide service can utilize the custom element, it is necessary to set up the routing. As described in previous posts, all services are hidden behind a <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/08-composition-via-blazor-webassembly-based-web-components/Demo.AspNetCore.MicroFrontendsInAction.Proxy\">YARP-based proxy</a>, which routes the incoming requests based on prefixes (to avoid conflicts). So far, both services were built in a way where the prefixes were an integral part of their implementation (they were included in actions and static content paths). With the Checkout service that would be hard to achieve, due to Blazor WebAssembly static assets.</p>\n<p>There is a way to control the base path for Blazor WebAssembly static assets (through <code>StaticWebAssetBasePath</code> project property) but it doesn't affect the <code>BlazorCustomElements.js</code> path. So, instead of complicating the service implementation to handle the prefix, it seems a lot better to make the prefixes a proxy concern and remove them there. YARP has an out-of-the-box capability to do so through <a href=\"https://microsoft.github.io/reverse-proxy/articles/transforms.html#pathremoveprefix\">PathRemovePrefix</a> transform. There is a ready-to-use extension method (<code>.WithTransformPathRemovePrefix</code>) which allows adding that transform to a specific route.</p>\n<pre><code>...\n\nvar routes = new[]\n{\n    ...\n    (new RouteConfig\n    {\n        RouteId = Constants.CHECKOUT_ROUTE_ID,\n        ClusterId = Constants.CHECKOUT_CLUSTER_ID,\n        Match = new RouteMatch { Path = Constants.CHECKOUT_ROUTE_PREFIX + \"/{**catch-all}\" }\n    }).WithTransformPathRemovePrefix(Constants.CHECKOUT_ROUTE_PREFIX),\n    ...\n};\n\nvar clusters = new[]\n{\n    ...\n    new ClusterConfig()\n    {\n        ClusterId = Constants.CHECKOUT_CLUSTER_ID,\n        Destinations = new Dictionary&lt;string, DestinationConfig&gt;(StringComparer.OrdinalIgnoreCase)\n        {\n            { Constants.CHECKOUT_SERVICE_URL, new DestinationConfig() { Address = builder.Configuration[Constants.CHECKOUT_SERVICE_URL] } }\n        }\n    }\n};\n\nbuilder.Services.AddReverseProxy()\n    .LoadFromMemory(routes, clusters);\n\n...\n</code></pre><p>Now the Decide service <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/08-composition-via-blazor-webassembly-based-web-components/Demo.AspNetCore.MicroFrontendsInAction.Decide/Views/Products\">views</a> can be modified to include the custom element. The first step is to add the static assets.</p>\n<pre><code>&lt;html&gt;\n  &lt;head&gt;\n    ...\n    &lt;link href=\"/checkout/static/components.css\" rel=\"stylesheet\" /&gt;\n  &lt;/head&gt;\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;script src=\"/checkout/_content/Microsoft.AspNetCore.Components.CustomElements/BlazorCustomElements.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"/checkout/_framework/blazor.webassembly.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>Sadly, this will not be enough for Blazor to work. When Blazor WebAssembly starts, it requests additional boot resources. They must be loaded from the Checkout service as well, which means including the prefix in URIs. This can be achieved thanks to the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/blazor/fundamentals/startup?view=aspnetcore-6.0&amp;WT.mc_id=DT-MVP-5002979#load-boot-resources\">JS initializers feature</a> which has been added in .NET 6. The automatic start of Blazor WebAssembly can be disabled, and it can be started manually which allows providing a function to customize the URIs.</p>\n<pre><code>&lt;html&gt;\n  ...\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;script src=\"/checkout/_framework/blazor.webassembly.js\" autostart=\"false\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n      Blazor.start({\n        loadBootResource: function (type, name, defaultUri, integrity) {\n          return `/checkout/_framework/${name}`;\n        }\n      });\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>Finally, the custom element can be used. It will be available through a tag matching the name provided as a parameter to <code>builder.RootComponents.RegisterAsCustomElement</code>.</p>\n<pre><code>&lt;html&gt;\n  ...\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;div class=\"decide_details\"&gt;\n      &lt;checkout-buy sku=\"porsche\"&gt;&lt;/checkout-buy&gt;\n    &lt;/div&gt;\n    ...\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><h2 id=\"the-expanded-project\">The Expanded Project</h2>\n<p>As with previous samples, I've created a GitHub Actions <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/blob/main/.github/workflows/08-composition-via-blazor-webassembly-based-web-components.yml\">workflow</a> that deploys the <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/08-composition-via-blazor-webassembly-based-web-components\">solution</a> to Azure. After the deployment, if you navigate to the URL of the <code>ca-app-proxy</code> Container App, you will see a page with following layout.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8ygipn8iq-8qnICr-4meqt38rUL2hv9x7Fqz2jqkfPaGYzw5xbbb-u2Y7SH7A2Al9CM-lOxgdtdWEIO7dEFKJyyfW42H4feeVFCgCTdQ2_HL2-uN70b5hnKR9RM-dbiY56Foq1yTzb78tjUwQzQg1tY0ufsbe6WdwLbDjUWK8LBF5cTn8BweTx0mS/s1600/decide-inspire-and-checkout-frontend-layout.png\" alt=\"Decide, Inspire, and Checkout Frontend Layout\"></p>\n<p>You can click the button rendered by the custom element and see the toast notification.</p>\n<p>This approach highlights one of the benefits of micro frontends, the freedom to choose the fronted stack. Thanks to the encapsulation of the Blazor WebAssembly app into a custom element, the Decide service can host it in its static HTML without understanding anything except how to load the scripts. If we would want to hide even that (something I haven't done here) we could create a script that encapsulates Blazor WebAssembly loading and initialization. That script could be a shared asset that loads Blazor WebAssembly from CDN (which besides the performance benefit would be also a way to go for a solution with multiple services using Blazor WebAssembly).</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-5699190681247385026",
      "Title": "Exploring Communication of Rate Limits in ASP.NET Core With Rate Limit Headers",
      "PublishDate": "2022-07-28T12:25:00+00:00",
      "Summary": "<p>Rate limiting (sometimes also referred to as throttling) is a key mechanism when it comes to ensuring API responsiveness and availability. By enforcing usage quotas, it can protect an API from issues like:</p>\n<ul>\n<li>Denial Of Service (DOS) attacks</li>\n<li>Degraded performance due to traffic spikes</li>\n<li>Monopolization by a single consumer</li>\n</ul>\n<p>Despite its importance, the typical approach to rate limiting is far from perfect when it comes to communicating usage quotas by services and (as a result) respecting those quotas by clients. It shouldn't be a surprise that various services were experimenting with different approaches to solve this problem. The common pattern in the web world is that some of such experiments start to gain traction which results in standardization efforts. This is exactly what is currently happening around communicating services usage quotas with <a href=\"https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/\"><em>RateLimit Fields for HTTP</em></a> Internet-Draft. As rate limiting will have <a href=\"https://devblogs.microsoft.com/dotnet/announcing-rate-limiting-for-dotnet/\">built-in support</a> with .NET 7, I thought it might be a good time to take a look at what this potential standard is bringing. But before that, let's recall how HTTP currently supports rate limiting (excluding custom extensions).</p>\n<h2 id=\"current-http-support-for-rate-limiting\">Current HTTP Support for Rate Limiting</h2>\n<p>When it comes to current support for rate limiting in HTTP, it's not much. If a service detects that a client has reached the quota, instead of a regular response it may respond with <em>429 (Too Many Request)</em> or <em>503 (Service Unavailable)</em>. Additionally, the service may include a <em>Retry-After</em> header in the response to indicate how long the client should wait before making another request. That's it. It means that client can be only reactive. There is no way for a client to get the information about the quota to avoid hitting it.</p>\n<p>In general, this works. That said, handling requests which are above the quota still consumes some resources on the service side. Clients would also prefer to be able to understand the quota and adjust their usage patterns instead of handling it as an exceptional situation. So as I said, it's not much.</p>\n<h2 id=\"proposed-rate-limit-headers\">Proposed Rate Limit Headers</h2>\n<p>The <a href=\"https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/\"><em>RateLimit Fields for HTTP</em></a> Internet-Draft proposes four new headers which aim at enabling a service to communicate usage quotas and policies:</p>\n<ul>\n<li><code>RateLimit-Limit</code> to communicate the total quota within a time window.</li>\n<li><code>RateLimit-Remaining</code> to communicate the remaining quota within the current time window.</li>\n<li><code>RateLimit-Reset</code> to communicate the time (in seconds) remaining in the current time window.</li>\n<li><code>RateLimit-Policy</code> to communicate the overall quota policy.</li>\n</ul>\n<p>The most interesting one is <code>RateLimit-Policy</code>. It is a list of quota policy items. A quota policy item consists of a quota limit and a single required parameter <code>w</code> which provides a time window in seconds. Custom parameters are allowed and should be treated as comments. Below you can see an example of <code>RateLimit-Policy</code> which informs that client is allowed to make 10 requests per second, 50 requests per minute, 1000 requests per hour, and 5000 per 24 hours.</p>\n<pre><code>RateLimit-Policy: 10;w=1, 50;w=60, 1000;w=3600, 5000;w=86400\n</code></pre><p>The only headers which intend to be required are <code>RateLimit-Limit</code> and <code>RateLimit-Reset</code> (<code>RateLimit-Remaining</code> is strongly recommended). So, how an ASP.NET Core based service can server those headers.</p>\n<h2 id=\"communicating-quotas-when-using-asp-net-core-middleware\">Communicating Quotas When Using ASP.NET Core Middleware</h2>\n<p>As I've already mentioned, <a href=\"https://devblogs.microsoft.com/dotnet/announcing-rate-limiting-for-dotnet/\">built-in support for rate limiting</a> comes to .NET with .NET 7. It brings generic purpose primitives for writing rate limiters as well as a few ready-to-use implementations. It also brings a middleware for ASP.NET Core. The below example shows the definition of a fixed time window policy which allows 5 requests per 10 seconds. The <code>OnRejected</code> callback is also provided to return <em>429 (Too Many Request)</em> status code and set the <em>Retry-After</em> header value based on provided metadata.</p>\n<pre><code class=\"lang-cs\">using System.Globalization;\nusing System.Threading.RateLimiting;\nusing Microsoft.AspNetCore.RateLimiting;\n\nvar builder = WebApplication.CreateBuilder(args);\n\nvar app = builder.Build();\n\napp.UseHttpsRedirection();\n\napp.UseRateLimiter(new RateLimiterOptions\n{\n    OnRejected = (context, cancellationToken) =&gt;\n    {\n        if (context.Lease.TryGetMetadata(MetadataName.RetryAfter, out var retryAfter))\n        {\n            context.HttpContext.Response.Headers.RetryAfter = ((int)retryAfter.TotalSeconds).ToString(NumberFormatInfo.InvariantInfo);\n        }\n\n        context.HttpContext.Response.StatusCode = StatusCodes.Status429TooManyRequests;\n\n        return new ValueTask();\n    }\n}.AddFixedWindowLimiter(\"fixed-window\", new FixedWindowRateLimiterOptions(\n    permitLimit: 5,\n    queueProcessingOrder: QueueProcessingOrder.OldestFirst,\n    queueLimit: 0,\n    window: TimeSpan.FromSeconds(10),\n    autoReplenishment: true\n)));\n\napp.MapGet(\"/\", context =&gt; context.Response.WriteAsync(\"-- Demo.RateLimitHeaders.AspNetCore.RateLimitingMiddleware --\"))\n    .RequireRateLimiting(\"fixed-window\");\n\napp.Run();\n</code></pre>\n<p>The question is, if and how this can be extended to return the rate limit headers? The answer is, sadly, that there seems to be no way to provide the required ones right now. All the information about rate limit policies is well hidden from public access. It would be possible to provide <code>RateLimit-Limit</code> and <code>RateLimit-Policy</code> as they are a direct result of provided options. It is also possible to provide <code>RateLimit-Remaining</code>, but it requires rewriting a lot of the middleware ecosystem to get the required value. What seems completely impossible to get right now is <code>RateLimit-Reset</code> as timers are managed centrally deep in <code>System.Threading.RateLimiting</code> core without any access to their state. There is an option to provide your own timers, but it would mean rewriting the entire middleware stack and taking a lot of responsibility from <code>System.Threading.RateLimiting</code>. Let's hope that things will improve.</p>\n<h2 id=\"communicating-quotas-when-using-aspnetcoreratelimit-package\">Communicating Quotas When Using AspNetCoreRateLimit Package</h2>\n<p>That built-in support for rate limiting is something that is just coming to .NET. So far the ASP.NET Core developers were using their own implementations or non-Microsoft packages for this purpose. Arguably, the most popular rate limiting solution for ASP.NET Core is <a href=\"https://github.com/stefanprodan/AspNetCoreRateLimit\">AspNetCoreRateLimit</a>. The example below provides similar functionality to the one from the built-in rate limiting example.</p>\n<pre><code class=\"lang-cs\">using AspNetCoreRateLimit;\n\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddMemoryCache();\n\nbuilder.Services.Configure&lt;IpRateLimitOptions&gt;(options =&gt;\n{\n    options.EnableEndpointRateLimiting = true;\n    options.StackBlockedRequests = false;\n    options.HttpStatusCode = 429;\n    options.GeneralRules = new List&lt;RateLimitRule&gt;\n    {\n        new RateLimitRule { Endpoint = \"*\", Period = \"10s\", Limit = 5 }\n    };\n});\n\nbuilder.Services.AddInMemoryRateLimiting();\n\nbuilder.Services.AddSingleton&lt;IRateLimitConfiguration, RateLimitConfiguration&gt;();\n\nvar app = builder.Build();\n\napp.UseHttpsRedirection();\n\napp.UseIpRateLimiting();\n\napp.MapGet(\"/\", context =&gt; context.Response.WriteAsync(\"-- Demo.RateLimitHeaders.AspNetCore.RateLimitPackage --\"));\n\napp.Run();\n</code></pre>\n<p>The AspNetCoreRateLimit has its own custom way of communicating quotas with HTTP headers. In the case of the above example, you might receive the following values in response.</p>\n<pre><code>X-Rate-Limit-Limit: 10s\nX-Rate-Limit-Remaining: 4\nX-Rate-Limit-Reset: 2022-07-24T11:30:47.2291052Z\n</code></pre><p>As you can see, they provide potentially useful information but not in a way that <em>RateLimit Fields for HTTP</em> is going for. Luckily, AspNetCoreRateLimit is not as protective about its internal state and algorithms, the information needed can be accessed and served in a different way.</p>\n<p>The information about the current state is kept in <code>IRateLimitCounterStore</code>. This is a dependency that could be accessed directly, but the method for generating needed identifiers lives in <code>ProcessingStrategy</code> so it will be better to create an implementation of it dedicated for purpose of just getting the counters state.</p>\n<pre><code class=\"lang-cs\">internal interface IRateLimitHeadersOnlyProcessingStrategy : IProcessingStrategy\n{ }\n\ninternal class RateLimitHeadersOnlyProcessingStrategy : ProcessingStrategy, IRateLimitHeadersOnlyProcessingStrategy\n{\n    private readonly IRateLimitCounterStore _counterStore;\n    private readonly IRateLimitConfiguration _config;\n\n    public RateLimitHeadersOnlyProcessingStrategy(IRateLimitCounterStore counterStore, IRateLimitConfiguration config) : base(config)\n    {\n        _counterStore = counterStore;\n        _config = config;\n    }\n\n    public override async Task ProcessRequestAsync(ClientRequestIdentity requestIdentity, RateLimitRule rule,\n        ICounterKeyBuilder counterKeyBuilder, RateLimitOptions rateLimitOptions, CancellationToken cancellationToken = default)\n    {\n        string rateLimitCounterId = BuildCounterKey(requestIdentity, rule, counterKeyBuilder, rateLimitOptions);\n\n        RateLimitCounter? rateLimitCounter = await _counterStore.GetAsync(rateLimitCounterId, cancellationToken);\n        if (rateLimitCounter.HasValue)\n        {\n            return new RateLimitCounter\n            {\n                Timestamp = rateLimitCounter.Value.Timestamp,\n                Count = rateLimitCounter.Value.Count\n            };\n        }\n        else\n        {\n            return new RateLimitCounter\n            {\n                Timestamp = DateTime.UtcNow,\n                Count = _config.RateIncrementer?.Invoke() ?? 1\n            };\n        }\n    }\n}\n</code></pre>\n<p>The second thing that is needed are rules which apply to specific endpoint and identity. Those can be retrieved from specific (either based on IP or client identifier) <code>IRateLimitProcessor</code>. The <code>IRateLimitProcessor</code> is also a tunnel to <code>IProcessingStrategy</code>, so it's nice we have a dedicated one. But what about the identity I've just mentioned? The algorithm to retrieve it lies in <code>RateLimitMiddleware</code>, so access to it will be needed. There are two options here. One is to inherit from <code>RateLimitMiddleware</code> and the other is to create an instance of one of its implementation and use it as a dependency. The first case would require hiding the base implementation of <code>Invoke</code> as it can't be overridden. I didn't like that, so I went with keeping an instance as a dependency approach. This led to the following code.</p>\n<pre><code class=\"lang-cs\">internal class IpRateLimitHeadersMiddleware\n{\n    private readonly RequestDelegate _next;\n    private readonly RateLimitOptions _rateLimitOptions;\n    private readonly IpRateLimitProcessor _ipRateLimitProcessor;\n    private readonly IpRateLimitMiddleware _ipRateLimitMiddleware;\n\n    public IpRateLimitHeadersMiddleware(RequestDelegate next,\n        IRateLimitHeadersOnlyProcessingStrategy processingStrategy, IOptions&lt;IpRateLimitOptions&gt; options, IIpPolicyStore policyStore,\n        IRateLimitConfiguration config, ILogger&lt;IpRateLimitMiddleware&gt; logger)\n    {\n        _next = next;\n        _rateLimitOptions = options?.Value;\n        _ipRateLimitProcessor = new IpRateLimitProcessor(options?.Value, policyStore, processingStrategy);\n        _ipRateLimitMiddleware = new IpRateLimitMiddleware(next, processingStrategy, options, policyStore, config, logger);\n    }\n\n    public async Task Invoke(HttpContext context)\n    {\n        ClientRequestIdentity identity = await _ipRateLimitMiddleware.ResolveIdentityAsync(context);\n\n        if (!_ipRateLimitProcessor.IsWhitelisted(identity))\n        {\n            var rateLimitRulesWithCounters = new Dictionary&lt;RateLimitRule, RateLimitCounter&gt;();\n\n            foreach (var rateLimitRule in await _ipRateLimitProcessor.GetMatchingRulesAsync(identity, context.RequestAborted))\n            {\n                rateLimitRulesWithCounters.Add(\n                    rateLimitRule,\n                    await _ipRateLimitProcessor.ProcessRequestAsync(identity, rateLimitRule, context.RequestAborted)\n                 );\n            }\n        }\n\n        await _next.Invoke(context);\n\n        return;\n    }\n}\n</code></pre>\n<p>The <code>rateLimitRulesWithCounters</code> contains all the rules applying to the endpoint in the context of the current request. This can be used to calculate the rate limit headers values.</p>\n<pre><code class=\"lang-cs\">internal class IpRateLimitHeadersMiddleware\n{\n    private class RateLimitHeadersState\n    {\n        public HttpContext Context { get; set; }\n\n        public int Limit { get; set; }\n\n        public int Remaining { get; set; }\n\n        public int Reset { get; set; }\n\n        public string Policy { get; set; } = String.Empty;\n\n        public RateLimitHeadersState(HttpContext context)\n        {\n            Context = context;\n        }\n    }\n\n    ...\n\n    public async Task Invoke(HttpContext context)\n    {\n        ...\n    }\n\n    private RateLimitHeadersState PrepareRateLimitHeaders(HttpContext context, Dictionary&lt;RateLimitRule, RateLimitCounter&gt; rateLimitRulesWithCounters)\n    {\n        RateLimitHeadersState rateLimitHeadersState = new RateLimitHeadersState(context);\n\n        var rateLimitHeadersRuleWithCounter = rateLimitRulesWithCounters.OrderByDescending(x =&gt; x.Key.PeriodTimespan).FirstOrDefault();\n        var rateLimitHeadersRule = rateLimitHeadersRuleWithCounter.Key;\n        var rateLimitHeadersCounter = rateLimitHeadersRuleWithCounter.Value;\n\n        rateLimitHeadersState.Limit = (int)rateLimitHeadersRule.Limit;\n\n        rateLimitHeadersState.Remaining = rateLimitHeadersState.Limit - (int)rateLimitHeadersCounter.Count;\n\n        rateLimitHeadersState.Reset = (int)(\n            (rateLimitHeadersCounter.Timestamp+ (rateLimitHeadersRule.PeriodTimespan ?? rateLimitHeadersRule.Period.ToTimeSpan())) - DateTime.UtcNow\n            ).TotalSeconds;\n\n        rateLimitHeadersState.Policy = String.Join(\n            \", \",\n            rateLimitRulesWithCounters.Keys.Select(rateLimitRule =&gt;\n                $\"{(int)rateLimitRule.Limit};w={(int)(rateLimitRule.PeriodTimespan ?? rateLimitRule.Period.ToTimeSpan()\n            ).TotalSeconds}\")\n        );\n\n        return rateLimitHeadersState;\n    }\n}\n</code></pre>\n<p>The only thing that remains is setting the headers on the response.</p>\n<pre><code class=\"lang-cs\">internal class IpRateLimitHeadersMiddleware\n{\n    ...\n\n    public async Task Invoke(HttpContext context)\n    {\n        ...\n\n        if (!_ipRateLimitProcessor.IsWhitelisted(identity))\n        {\n            ...\n\n            if (rateLimitRulesWithCounters.Any() &amp;&amp; !_rateLimitOptions.DisableRateLimitHeaders)\n            {\n                context.Response.OnStarting(\n                    SetRateLimitHeaders,\n                    state: PrepareRateLimitHeaders(context, rateLimitRulesWithCounters)\n                );\n            }\n        }\n\n        ...\n    }\n\n    ...\n\n    private Task SetRateLimitHeaders(object state)\n    {\n        var rateLimitHeadersState = (RateLimitHeadersState)state;\n\n        rateLimitHeadersState.Context.Response.Headers[\"RateLimit-Limit\"] = rateLimitHeadersState.Limit.ToString(CultureInfo.InvariantCulture);\n        rateLimitHeadersState.Context.Response.Headers[\"RateLimit-Remaining\"] = rateLimitHeadersState.Remaining.ToString(CultureInfo.InvariantCulture);\n        rateLimitHeadersState.Context.Response.Headers[\"RateLimit-Reset\"] = rateLimitHeadersState.Reset.ToString(CultureInfo.InvariantCulture);\n        rateLimitHeadersState.Context.Response.Headers[\"RateLimit-Policy\"] = rateLimitHeadersState.Policy;\n\n        return Task.CompletedTask;\n    }\n}\n</code></pre>\n<p>After registering the <code>RateLimitHeadersOnlyProcessingStrategy</code> and <code>IpRateLimitHeadersMiddleware</code> (I've registered it after the <code>IpRateLimitMiddleware</code>) response will contain values similar to the following ones.</p>\n<pre><code>RateLimit-Limit: 5\nRateLimit-Remaining: 4\nRateLimit-Reset: 9\nRateLimit-Policy: 5;w=10\nX-Rate-Limit-Limit: 10s\nX-Rate-Limit-Remaining: 4\nX-Rate-Limit-Reset: 2022-07-25T20:57:32.0746592Z\n</code></pre><p>The code works but certainly isn't perfect, so I've created an <a href=\"https://github.com/stefanprodan/AspNetCoreRateLimit/issues/352\">issue</a> in hope that AspNetCoreRateLimit will get those headers built in.</p>\n<h2 id=\"limiting-the-number-of-outbound-requests-in-httpclient\">Limiting the Number of Outbound Requests in HttpClient</h2>\n<p>The general rule around rate limit headers is that they should be treated as informative, so the client doesn't have to do anything specific with them. They are also described as generated at response time without any guarantee of consistency between requests. This makes perfect sense. In the simple examples above, multiple clients would be competing for the same quota so received headers values don't exactly tell how many requests a specific client can make within a given window. But, real-life scenarios are usually more specific and complex. It's very common for quotas to be per client or per IP address (this is why AspNetCoreRateLimit has concepts like request identity as a first-class citizen, the ASP.NET Core built-in middleware also enables sophisticated scenarios by using <code>PartitionedRateLimiter</code> at its core). In a such scenario, the client might want to use rate limit headers to avoid making requests which have a high likelihood of being throttled. Let's explore that, below is a simple code that can handle <em>429 (Too Many Request)</em> responses and utilize the <em>Retry-After</em> header.</p>\n<pre><code class=\"lang-cs\">HttpClient client = new();\nclient.BaseAddress = new(\"http://localhost:5262\");\n\nwhile (true)\n{\n    Console.Write(\"{0:hh:mm:ss}: \", DateTime.UtcNow);\n\n    int nextRequestDelay = 1;\n\n    try\n    {\n        HttpResponseMessage response = await client.GetAsync(\"/\");\n        if (response.IsSuccessStatusCode)\n        {\n            Console.WriteLine(await response.Content.ReadAsStringAsync());\n        }\n        else\n        {\n            Console.Write($\"{(int)response.StatusCode}: {await response.Content.ReadAsStringAsync()}\");\n\n            string? retryAfter = response.Headers.GetValues(\"Retry-After\").FirstOrDefault();\n            if (Int32.TryParse(retryAfter, out nextRequestDelay))\n            {\n                Console.Write($\" | Retry-After: {nextRequestDelay}\");\n            }\n\n            Console.WriteLine();\n        }\n    }\n    catch (Exception ex)\n    {\n        Console.WriteLine(ex.Message);\n    }\n\n    await Task.Delay(TimeSpan.FromSeconds(nextRequestDelay));\n}\n</code></pre>\n<p>Let's assume that the service is sending all rate limit headers and that they are dedicated to the client. We can rate limit the <code>HttpClient</code> by creating a <code>DelegatingHandler</code> which will read the <code>RateLimit-Policy</code> header value and instantiate a <code>FixedWindowRateLimiter</code> based on it. The <code>FixedWindowRateLimiter</code> will be used to rate limit the outbound requests - if a lease can't be acquired a locally created <code>HttpResponseMessage</code> will be returned.</p>\n<pre><code class=\"lang-cs\">internal class RateLimitPolicyHandler : DelegatingHandler\n{\n    private string? _rateLimitPolicy;\n    private RateLimiter? _rateLimiter;\n\n    private static readonly Regex RATE_LIMIT_POLICY_REGEX = new Regex(@\"(\\d+);w=(\\d+)\", RegexOptions.Compiled);\n\n    public RateLimitPolicyHandler() : base(new HttpClientHandler())\n    { }\n\n    protected override async Task&lt;HttpResponseMessage&gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n    {\n        if (_rateLimiter is not null)\n        {\n            using var rateLimitLease = await _rateLimiter.WaitAsync(1, cancellationToken);\n            if (rateLimitLease.IsAcquired)\n            {\n                return await base.SendAsync(request, cancellationToken);\n            }\n\n            var rateLimitResponse = new HttpResponseMessage(HttpStatusCode.TooManyRequests);\n            rateLimitResponse.Content = new StringContent($\"Service rate limit policy ({_rateLimitPolicy}) exceeded!\");\n\n            if (rateLimitLease.TryGetMetadata(MetadataName.RetryAfter, out var retryAfter))\n            {\n                rateLimitResponse.Headers.Add(\"Retry-After\", ((int)retryAfter.TotalSeconds).ToString(NumberFormatInfo.InvariantInfo));\n            }\n\n            return rateLimitResponse;\n        }\n\n        var response = await base.SendAsync(request, cancellationToken);\n\n        if (response.Headers.Contains(\"RateLimit-Policy\"))\n        {\n            _rateLimitPolicy = response.Headers.GetValues(\"RateLimit-Policy\").FirstOrDefault();\n\n            if (_rateLimitPolicy is not null)\n            {\n                Match rateLimitPolicyMatch = RATE_LIMIT_POLICY_REGEX.Match(_rateLimitPolicy);\n\n                if (rateLimitPolicyMatch.Success)\n                {\n                    int limit = Int32.Parse(rateLimitPolicyMatch.Groups[1].Value);\n                    int window = Int32.Parse(rateLimitPolicyMatch.Groups[2].Value);\n\n                    _rateLimiter = new FixedWindowRateLimiter(new FixedWindowRateLimiterOptions(\n                        limit,\n                        QueueProcessingOrder.NewestFirst,\n                        0,\n                        TimeSpan.FromSeconds(window),\n                        true\n                    ));\n\n                    string? rateLimitRemaining = response.Headers.GetValues(\"RateLimit-Remaining\").FirstOrDefault();\n                    if (Int32.TryParse(rateLimitRemaining, out int remaining))\n                    {\n                        using var rateLimitLease = await _rateLimiter.WaitAsync(limit - remaining, cancellationToken);\n                    }\n                }\n            }\n        }\n\n        return response;\n    }\n}\n</code></pre>\n<p>The above code also uses the <code>RateLimit-Remaining</code> header value to acquire leases for requests which are no longer available in the initial window.</p>\n<p>Now depending if the sample code is run with the <code>RateLimitPolicyHandler</code> in the <code>HttpClient</code> pipeline or not, the console output will be different as those 429 will be coming from a different place.</p>\n<h2 id=\"opinions\">Opinions</h2>\n<p>The rate limit headers seem like an interesting addition for communicating services usage quotas. Properly used in the right situations might be a useful tool, it is just important not to treat them as guarantees.</p>\n<p>Serving rate limit headers from ASP.NET Core right now has its challenges. If they will become a standard and gain popularity, I think this will change.</p>\n<p>If you want to play with the samples, they are available on <a href=\"https://github.com/tpeczek/Demo.RateLimitHeaders\">GitHub</a>.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/07/exploring-communication-of-rate-limits.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-310692594729019759",
      "Title": "Micro Frontends in Action With ASP.NET Core - Composition via YARP Transforms and Server-Side Includes (SSI)",
      "PublishDate": "2022-07-05T14:46:00+00:00",
      "Summary": "<p>I'm continuing my series on implementing <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction\">the samples</a> from <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> in ASP.NET Core:</p>\n<ul>\n<li><a href=\"https://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html\">Server-Side Routing via YARP in Azure Container Apps</a></li>\n<li>Composition via YARP Transforms and Server-Side Includes (SSI)</li>\n<li><a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">Composition via Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/09/micro-frontends-in-action-with-aspnet.html\">Communication Patterns for Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/10/micro-frontends-in-action-with-aspnet.html\">Universal Rendering With Blazor WebAssembly Based Web Components</a></li>\n</ul>\n<p>In the previous post, I described how I've deployed the two services which provide fragments of the frontend to the single <a href=\"https://docs.microsoft.com/en-us/azure/container-apps/environment\">Container Apps environment</a> and then hidden them behind a <a href=\"https://github.com/microsoft/reverse-proxy\">YARP</a>-based proxy. This technique (called server-side routing) solves some problems related to bad user experience (multiple domains), browser security (CORS), or search engines. That said, there are some other problems on the table.</p>\n<p>The first common problem is performance. Server-side routing does improve performance over solution where fragments are loaded directly from different domains by removing multiple DNS lookups, SSL handshakes, etc. Still, separated AJAX calls can have a very negative impact on overall page load time, especially on slower connections.</p>\n<p>The second common problem is layout shifts. When fragments are being loaded, it can often cause already visible page content to \"jump\". This is frustrating for the end-users.</p>\n<p>A solution to both of these problems can be providing a complete page to the browser in a response to the first request.</p>\n<h2 id=\"server-side-composition\">Server-Side Composition</h2>\n<p>Server-side composition is a technique, where the page is being fully assembled (which means requesting all the required fragments) on the server. The composition can be done either by a central service (a proxy) or can be done in a decentralized manner, where every service requests fragments it requires to build its own UI. As the current solution already has a proxy in place, I've decided to start with a centralized approach. There are two possible mechanisms discussed in the book for this purpose: <a href=\"https://www.w3.org/Jigsaw/Doc/User/SSI.html\">Server-Side Includes (SSI)</a> and <a href=\"https://www.w3.org/TR/esi-lang/\">Edge-Side Includes (ESI)</a>.</p>\n<h2 id=\"server-side-includes-ssi-\">Server-Side Includes (SSI)</h2>\n<p>Server-Side Includes is a quite old mechanism, it dates back to the <a href=\"https://en.wikipedia.org/wiki/NCSA_HTTPd\"><em>NCSA HTTPd</em></a> web server. It defines a set of directives (called commands or elements in some implementations) that can be placed in HTML and evaluated by the server while the page is being served. Currently, SSI is supported by <em>Apache</em>, <em>Nginx</em>, and <em>IIS</em>. The subset of supported directives varies between implementations, but the most useful and always available one is <code>include</code>, which the server replaces with a file or result of a request. All that a service needs to do, is put that directive as part of the returned markup.</p>\n<pre><code>&lt;html&gt;\n  ...\n  &lt;body class=\"decide_layout\"&gt;\n    ...\n    &lt;aside class=\"decide_recos\"&gt;\n      &lt;!--#include virtual=\"/inspire/fragment/recommendations/porsche\" --&gt;\n    &lt;/aside&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>All the magic needs to happen at the proxy level, the only question is how?</p>\n<h2 id=\"supporting-ssi-in-yarp-with-response-body-transform\">Supporting SSI in YARP With Response Body Transform</h2>\n<p>Every well-respected reverse proxy provides more capabilities than just routing and YARP is no different. One of the capabilities provided by YARP, which goes beyond routing, is <a href=\"https://microsoft.github.io/reverse-proxy/articles/transforms.html\">transforms</a>. Transforms allow for modifying parts of the request or response as part of the flow. Currently, there are three categories of transforms:</p>\n<ul>\n<li>Request</li>\n<li>Response</li>\n<li>Response Trailers</li>\n</ul>\n<p>In every one of those categories, YARP provides a number of built-in transforms which allow for modifying path, query string, client certificates, and headers. There are no built-in transforms for request and response body, which probably makes sense as transforms including request and response body are slightly tricky and potentially dangerous. The first tricky part is that <a href=\"https://microsoft.github.io/reverse-proxy/articles/direct-forwarding.html\">direct forwarding</a> ignores any modifications to the response body which transforms could make, so the proxy service needs to be switched to a \"full\" reverse proxy experience.</p>\n<pre><code class=\"lang-cs\">app.Run();\n\nvar builder = WebApplication.CreateBuilder(args);\n\n...\n\nbuilder.Services.AddReverseProxy();\n\nvar app = builder.Build();\n\napp.MapReverseProxy();\n\napp.Run();\n</code></pre>\n<p>Moving away from direct forwarding means that there is no longer a way to define path prefixes directly. The out-of-the-box approach for configuring the reverse proxy requires is through <a href=\"https://microsoft.github.io/reverse-proxy/articles/config-files.html\">configuration files</a>. This would mean that I would have to change the way in which the deployment workflow provides the proxy with URLs to other services. That's something I really didn't want to do. Luckily, there is a possibility of implementing configuration providers to load the configuration programmatically from any source. The documentation even contains an <a href=\"https://microsoft.github.io/reverse-proxy/articles/config-providers.html#example\">example</a> of an in-memory configuration provider which I've basically copy-pasted (looking at the YARP <a href=\"https://github.com/microsoft/reverse-proxy/issues/1713\">backlog</a> it seems that the team has noticed its usefulness and it will be available out-of-the-box as well). This allowed me to keep the routes and clusters (this is how destinations are represented in YARP) configuration in the code.</p>\n<pre><code class=\"lang-cs\">...\n\nvar routes = new[]\n{\n    ...\n    new RouteConfig\n    {\n        RouteId = Constants.DECIDE_ROUTE_ID,\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = \"/decide/{**catch-all}\" }\n    },\n    ...\n};\n\nvar clusters = new[]\n{\n    new ClusterConfig()\n    {\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Destinations = new Dictionary&lt;string, DestinationConfig&gt;(StringComparer.OrdinalIgnoreCase)\n        {\n            { Constants.DECIDE_SERVICE_URL, new DestinationConfig() { Address = builder.Configuration[Constants.DECIDE_SERVICE_URL] } }\n        }\n    },\n    ...\n};\n\nbuilder.Services.AddReverseProxy()\n    .LoadFromMemory(routes, clusters);\n\n...\n</code></pre>\n<p>With the configuration in place, it's time for the transform. There are two main ways for adding transforms. One is a callback and the other is <code>ITransformProvider</code> implementation. The <code>ITransformProvider</code> implementation gives more flexibility and isolation, so I've decided to go with it. As the implementation will be a registered dependency, it gives full access to dependency injection. It also gives validation capabilities for routes and clusters.</p>\n<p>The simplest (\"no-op\") implementation of <code>ITransformProvider</code> can look like below.</p>\n<pre><code class=\"lang-cs\">internal class SsiTransformProvider : ITransformProvider\n{\n    public void ValidateRoute(TransformRouteValidationContext context)\n    { }\n\n    public void ValidateCluster(TransformClusterValidationContext context)\n    { }\n\n    public void Apply(TransformBuilderContext transformBuildContext)\n    {\n        transformBuildContext.AddResponseTransform(TransformResponse);\n    }\n\n    private ValueTask TransformResponse(ResponseTransformContext responseContext)\n    {\n        return default;\n    }\n}\n</code></pre>\n<p>In order to register that <code>ITransformProvider</code> implementation (and make <code>TranformResponse</code> part of the flow) it is enough to call <code>AddTransforms</code>.</p>\n<pre><code class=\"lang-cs\">...\n\nbuilder.Services.AddReverseProxy()\n    .LoadFromMemory(routes, clusters)\n    .AddTransforms&lt;SsiTransformProvider&gt;();\n\n...\n</code></pre>\n<p>This is where it is important to understand further specifics of transforms that are working with the request or response body. As a result of the <code>Apply</code> method from the above implementation, the <code>TranformResponse</code> will be executed for every flow going through YARP. This is too broad because if that transform deals with request or response body, it will come with a performance penalty. It will have to read (essentially buffer) the response from the destination. The moment the body has been read, it will also have to be written to the <code>HttpContext</code> of the YARP response, otherwise the response will be empty. This is happening because YARP doesn't buffer the response as part of the flow (due to performance reasons), instead it attempts to read the stream which in this case is at the end.</p>\n<p>The performance penalty means that there is a need to limit the scope of impact by adding the transform only to certain routes. The routes which should be transformed need to be somehow marked. For that purpose I've decided to include additional information in the routes through metadata. Metadata is a dictionary available on <code>RouteConfig</code>. I've defined a specific key and value for which the <code>Apply</code> method will check before adding the transform. I've also added a statically available dictionary which can be used to set the metadata.</p>\n<pre><code class=\"lang-cs\">internal class SsiTransformProvider : ITransformProvider\n{\n    private const string SSI_METADATA_FLAG = \"SSI\";\n    private const string SSI_METADATA_FLAG_ON = \"ON\";\n\n    public static IReadOnlyDictionary&lt;string, string&gt; SsiEnabledMetadata { get; } = new Dictionary&lt;string, string&gt;()\n    {\n        { SSI_METADATA_FLAG, SSI_METADATA_FLAG_ON }\n    };\n\n    ...\n\n    public void Apply(TransformBuilderContext transformBuildContext)\n    {\n        if (transformBuildContext.Route.Metadata is not null\n            &amp;&amp; transformBuildContext.Route.Metadata.ContainsKey(SSI_METADATA_FLAG)\n            &amp;&amp; transformBuildContext.Route.Metadata[SSI_METADATA_FLAG] == SSI_METADATA_FLAG_ON)\n        {\n            transformBuildContext.AddResponseTransform(TransformResponse);\n        }\n    }\n\n    ...\n}\n</code></pre>\n<p>Thanks to the in-memory configuration provider, including those metadata mean just setting one more property. While doing this I've also increased the granularity of routes to further limit the affected scope.</p>\n<pre><code class=\"lang-cs\">...\n\nvar routes = new[]\n{\n    ...\n    new RouteConfig\n    {\n        RouteId = Constants.DECIDE_ROUTE_ID + \"-static\",\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = \"/decide/static/{**catch-all}\" }\n    },\n    new RouteConfig\n    {\n        RouteId = Constants.DECIDE_ROUTE_ID,\n        ClusterId = Constants.DECIDE_CLUSTER_ID,\n        Match = new RouteMatch { Path = \"/decide/{**catch-all}\" },\n        Metadata = SsiTransformProvider.SsiEnabledMetadata\n    },\n    ...\n};\n\n...\n</code></pre>\n<p>Now it's time for the actual transformation. To focus on the logic needed to support SSI <code>include</code> directive, let's get the response body reading and writing out of the way.</p>\n<pre><code class=\"lang-cs\">internal class SsiTransformProvider : ITransformProvider\n{\n    ...\n\n    private ValueTask TransformResponse(ResponseTransformContext responseContext)\n    {\n\n        string proxyResponseContent = await responseContext.ProxyResponse.Content.ReadAsStringAsync();\n\n        responseContext.SuppressResponseBody = true;\n\n        ...\n\n        byte[] proxyResponseContentBytes = Encoding.UTF8.GetBytes(proxyResponseContent);\n        responseContext.HttpContext.Response.ContentLength = proxyResponseContentBytes.Length;\n        await responseContext.HttpContext.Response.Body.WriteAsync(proxyResponseContentBytes);\n    }\n}\n</code></pre>\n<p>In order to get the <code>include</code> directive from the response body, I've decided to use (I don't believe I'm saying this) regex. The snippet below will grab all the directives. The group with index one of the resulting match will contain the directive name (all others than <code>include</code> are to be ignored) while the group with index two will contain parameters for further parsing (I'm doing this with another regex, but ultimately I care only for <code>virtual</code> parameter).</p>\n<pre><code class=\"lang-cs\">internal class SsiTransformProvider : ITransformProvider\n{\n    private static readonly Regex SSI_DIRECTIVE_REGEX = new Regex(\n        @\"&lt;!--\\#([a-z]+)\\b([^&gt;]+[^\\/&gt;])?--&gt;\",\n        RegexOptions.Compiled | RegexOptions.IgnoreCase | RegexOptions.IgnorePatternWhitespace\n    );\n\n    ...\n\n    private ValueTask TransformResponse(ResponseTransformContext responseContext)\n    {\n\n        ...\n\n        var directives = SSI_DIRECTIVE_REGEX.Matches(proxyResponseContent)\n\n        ...\n    }\n}\n</code></pre>\n<p>To get the content to which the <code>include</code> directive is pointing, I will need to make an HTTP request to a specific service. The configuration provided for YARP already has an URL for that service. YARP also maintains a <code>HttpClient</code> instance dedicated to the cluster to which the service belongs. It would be nice to reuse it. In order to do that, first I needed to identify the endpoint for the path to which the <code>virtual</code> parameter is pointing.</p>\n<pre><code class=\"lang-cs\">Endpoint? virtualEndpoint = null;\n\nvar endpointDataSource = context.RequestServices.GetService&lt;EndpointDataSource&gt;();\nif (endpointDataSource is not null)\n{\n    var virtualPath = new PathString(directive.Parameters[VIRTUAL_PARAMETER]);\n    foreach (Endpoint possibleVirtualEndpoint in endpointDataSource.Endpoints)\n    {\n        var routeEndpoint = possibleVirtualEndpoint as RouteEndpoint;\n        if (routeEndpoint is not null)\n        {\n            var routeTemplateMatcher = new TemplateMatcher(new RouteTemplate(routeEndpoint.RoutePattern), _emptyRouteValueDictionary);\n            if (routeTemplateMatcher.TryMatch(virtualPath, _emptyRouteValueDictionary))\n            {\n                virtualEndpoint = possibleVirtualEndpoint;\n                break;\n            }\n        }\n    }\n}\n</code></pre>\n<p>The endpoint also has a metadata collection. In this collection, YARP keeps the route model, which includes the cluster model.</p>\n<pre><code class=\"lang-cs\">ClusterModel? cluster = null;\n\nforeach (var endpointMetadata in virtualEndpoint.Metadata)\n{\n    var proxyRoute = endpointMetadata as RouteModel;\n    if (proxyRoute is not null)\n    {\n        cluster = proxyRoute.Cluster?.Model;\n        break;\n    }\n}\n</code></pre>\n<p>The cluster model contains the configured destinations (with URLs) and that mentioned <code>HttpClient</code> instance. All that remains is to build the request URI, make the request, and read the content.</p>\n<pre><code class=\"lang-cs\">string virtualUri = cluster.Config.Destinations.FirstOrDefault().Value.Address + parameters[\"virtual\"];\n\nHttpResponseMessage response = await cluster.HttpClient.SendAsync(new HttpRequestMessage(HttpMethod.Get, virtualUri), CancellationToken.None);\n\nstring directiveOutputContent = await response.Content.ReadAsStringAsync();\n</code></pre>\n<p>Once the content has been read, the directive can be replaced in the body.</p>\n<pre><code class=\"lang-cs\">proxyResponseContent = proxyResponseContent.Substring(0, directives[directiveIndex].Index)\n    + directiveOutputContent\n    + proxyResponseContent.Substring(directives[directiveIndex].Index + directives[directiveIndex].Length);\n</code></pre>\n<h2 id=\"this-is-just-a-proof-of-concept\">This Is Just a Proof of Concept</h2>\n<p>Yes, this code (even the little more polished version available in the <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/05_06.01-composition-via-yarp-and-ssi\">repository</a>) is just a POC. It's missing constraints, error checking, support for multiple destinations in a cluster, support for other parameters of the <code>include</code> directive, and much more.</p>\n<p>There should be also further performance considerations. The approach that the sample code takes (buffer the body, request content for <code>include</code> directives in parallel, and then build the final response body) is typical for SSI, but an approach that streams the body whenever possible could be considered. This is for example how ESI (which is a more modern mechanism) is sometimes implemented.</p>\n<p>The only goal of this post (and related sample) is to show some YARP capabilities which can be used for achieving server-side composition at its level.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/07/micro-frontends-in-action-with-aspnet.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-8133906837345607575",
      "Title": "Micro Frontends in Action With ASP.NET Core - Server-Side Routing via YARP in Azure Container Apps",
      "PublishDate": "2022-06-28T13:35:00+00:00",
      "Summary": "<p>Recently, I've been reading <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> and while doing it I've decided that I want to implement <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction\">the samples</a> in ASP.NET Core and host them on Azure. Then I had a thought, that maybe I can use this as an opportunity to play with some technologies I didn't have a chance to play with yet, like <em>YARP</em> or <em>Azure Container Apps</em>. Once I did that, the next thought was that maybe I should write down some of this and maybe someone will find it useful. So here we are, possibly starting a new series around micro frontends techniques:</p>\n<ul>\n<li>Server-Side Routing via YARP in Azure Container Apps</li>\n<li><a href=\"https://www.tpeczek.com/2022/07/micro-frontends-in-action-with-aspnet.html\">Composition via YARP Transforms and Server-Side Includes (SSI)</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/08/micro-frontends-in-action-with-aspnet.html\">Composition via Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/09/micro-frontends-in-action-with-aspnet.html\">Communication Patterns for Blazor WebAssembly Based Web Components</a></li>\n<li><a href=\"https://www.tpeczek.com/2022/10/micro-frontends-in-action-with-aspnet.html\">Universal Rendering With Blazor WebAssembly Based Web Components</a></li>\n</ul>\n<h2 id=\"micro-frontends-in-action\">Micro Frontends in Action</h2>\n<p>One of the praises for <a href=\"https://www.manning.com/books/micro-frontends-in-action\">Micro Frontends in Action</a> says that it's an excellent starting point to understanding how to introduce micro frontends in your projects. I'm about one-third through it and I'm willing to agree with that statement. The book starts with very simple techniques like page transition via links, composition via iframe, and composition via Ajax. Then it moves to server-side composition, advanced techniques for client-side composition, communication patterns, and more (I don't know, I haven't gotten there yet). Up to this point, it has been interesting and engaging (this is my opinion and just to be clear - nobody is paying me to read the book and provide an opinion about it).</p>\n<p>The book contains samples for every discussed technique and it was just too tempting not to implement them with technologies I like. That doesn't mean I will implement every single one of those samples. I will also certainly not describe every single one of those implementations. I'm doing it for fun.</p>\n<p>I also recommend you read the book if you are interested in the subject - a blog post describing some specific implementation will not provide you with information about benefits, drawbacks, and when to use a certain technique.</p>\n<h2 id=\"the-landscape-so-far\">The Landscape So Far</h2>\n<p>The technique I'm going to describe in this post is server-side routing. Prior to introducing that technique, the project consists of two services: <em><a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/03-composition-via-ajax/Demo.AspNetCore.MicroFrontendsInAction.Decide\">Decide</a></em> and <em><a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/03-composition-via-ajax/Demo.AspNetCore.MicroFrontendsInAction.Inspire\">Inspire</a></em>. The Decide service is loading frontend fragments provided by Inspire service via Ajax request.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPDiYpsJfWonHLVcUrCANbXxSCJ83Clz2RL-zgf2ybeLUNXAEBr2aPXAMA88twe12XsW6BRVYMW9AGt2ddJO4vYf1ehdWpuLMwluxM3kU_rSO_altIVJg3NNjs96N3dX3CCY1xPRr939itMzjHlBGwxXE_5743xH3Fyclkjh7HPXqoKKHwKmdY7iTl/s1600/composition-via-ajax-frontend-layout.png\" alt=\"Composition via Ajax Frontend Layout\"></p>\n<p>Under the hood, both services are simple ASP.NET Core MVC applications that serve views based on static HTML from <a href=\"https://github.com/naltatis/micro-frontends-in-action-code\">original samples</a>, where the URLs are generated based on configuration.</p>\n<pre><code class=\"lang-cs\">@using Microsoft.Extensions.Configuration\n@inject IConfiguration Configuration\n@{string decideServiceUrl = Configuration[\"DECIDE_SERVICE_URL\"];}\n&lt;link href=\"@(Context.Request.Scheme + \"://\" + Context.Request.Host + \"/static/fragment.css\")\" rel=\"stylesheet\" /&gt;\n&lt;div class=\"inspire_fragment\"&gt;\n  &lt;h2 class=\"inspire_headline\"&gt;Recommendations&lt;/h2&gt;\n  &lt;div class=\"inspire_recommendations\"&gt;\n    &lt;a href=\"@(decideServiceUrl + \"/product/fendt\")\"&gt;&lt;img src=\"https://mi-fr.org/img/fendt.svg\" /&gt;&lt;/a&gt;\n    &lt;a href=\"@(decideServiceUrl + \"/product/eicher\")\"&gt;&lt;img src=\"https://mi-fr.org/img/eicher.svg\" /&gt;&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n<p>The Inspire service additionally defines a <a href=\"https://docs.microsoft.com/en-us/aspnet/core/security/cors#cors-with-named-policy-and-middleware\">CORS policy</a> to enable requesting the fragments from different domain via Ajax.</p>\n<pre><code class=\"lang-cs\">var builder = WebApplication.CreateBuilder(args);\n\nstring decideServiceCorsPolicyName = \"decide-service-cors-policy\";\n\nbuilder.Services.AddCors(options =&gt;\n{\n    options.AddPolicy(name: decideServiceCorsPolicyName, policy =&gt;\n    {\n        policy.WithOrigins(builder.Configuration[\"DECIDE_SERVICE_URL\"]);\n        policy.AllowAnyHeader();\n        policy.WithMethods(\"GET\");\n    });\n});\n\n...\n\napp.UseRouting();\n\napp.UseCors(decideServiceCorsPolicyName);\n\n...\n</code></pre>\n<p>Both services are containerized and deployed as two Container Apps within a single <a href=\"https://docs.microsoft.com/en-us/azure/container-apps/environment\">Container Apps environment</a>.</p>\n<pre><code class=\"lang-bash\">az containerapp create \\\n  -n ${DECIDE_CONTAINERAPP} \\\n  -i ${CONTAINER_REGISTRY}.azurecr.io/decide:latest \\\n  -g ${RESOURCE_GROUP} \\\n  --environment ${CONTAINERAPPS_ENVIRONMENT} \\\n  --ingress external \\\n  --target-port 3001 \\\n  --min-replicas 1 \\\n  --registry-server ${CONTAINER_REGISTRY}.azurecr.io\n\naz containerapp create \\\n  -n ${INSPIRE_CONTAINERAPP} \\\n  -i ${CONTAINER_REGISTRY}.azurecr.io/inspire:latest \\\n  -g ${RESOURCE_GROUP} \\\n  --environment ${CONTAINERAPPS_ENVIRONMENT} \\\n  --ingress external \\\n  --target-port 3002 \\\n  --min-replicas 1 \\\n  --registry-server ${CONTAINER_REGISTRY}.azurecr.io\n</code></pre>\n<p>This results in the following Container Apps solution.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEMWsYgxIpmZpDDqiSfy6eD7z_yKklf3V4qC0yTw7GOiMcIVOnqGdK0vxVfaXC02UaHaMqYVg32ACrgBYHBUTOJoOc3KRJtTG2n9bK4qrC-ogH5d1fk2re7ZsaZhD1UUV2c2D1FymnHZlhA8DKdf_mQqnq54VxIFOK3rWL-ERiySl-UuvEiZyC5wz4/s1600/composition-via-ajax-container-apps-solution.png\" alt=\"Composition via Ajax Container Apps Solution\"></p>\n<p>I've created a GitHub Actions <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/blob/main/.github/workflows/03-composition-via-ajax.yml\">workflow</a> that performs all the necessary steps (creating Azure resources, building and pushing containers images, and deploying Container Apps) to set up the entire solution from scratch. There is one tricky step in that workflow. Both services must know each other URLs but those are available only after Container Apps are created. To solve this I'm first getting the ingress information for every app (with <code>az containerapp ingress show</code>) and then write them to environment variables (using the <a href=\"https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\">multiline strings</a> approach).</p>\n<pre><code class=\"lang-yaml\">jobs:\n  ..\n  deploy-to-container-apps:\n  ..\n  steps:\n    ..\n    - name: Get Services Ingress\n      run: |\n        echo 'DECIDE_CONTAINERAPP_INGRESS_JSON&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        az containerapp ingress show -n ${DECIDE_CONTAINERAPP} -g ${RESOURCE_GROUP} &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n        echo 'INSPIRE_CONTAINERAPP_INGRESS_JSON&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        az containerapp ingress show -n ${INSPIRE_CONTAINERAPP} -g ${RESOURCE_GROUP} &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n    ..\n</code></pre>\n<p>Next, I'm using the <a href=\"https://docs.github.com/en/actions/learn-github-actions/expressions#fromjson\"><code>fromJson</code></a> expression to get FQDN from ingress information and update the Container Apps.</p>\n<pre><code class=\"lang-yaml\">jobs:\n  ..\n  deploy-to-container-apps:\n  ..\n  steps:\n    ..\n    - name: Configure Services URLs\n      run: |\n        az containerapp update -n ${DECIDE_CONTAINERAPP} -g ${RESOURCE_GROUP} --set-env-vars INSPIRE_SERVICE_URL=https://${{ fromJSON(env.INSPIRE_CONTAINERAPP_INGRESS_JSON).fqdn }}\n        az containerapp update -n ${INSPIRE_CONTAINERAPP} -g ${RESOURCE_GROUP} --set-env-vars DECIDE_SERVICE_URL=https://${{ fromJSON(env.DECIDE_CONTAINERAPP_INGRESS_JSON).fqdn }}\n    ..\n</code></pre>\n<h2 id=\"the-challenge\">The Challenge</h2>\n<p>There is a problem with this solution. There are two services and each of them is available to public requests under a different domain. This has several drawbacks:</p>\n<ul>\n<li>Bad user experience (requests flying to different domains).</li>\n<li>Internal infrastructure of the solution is exposed publicly.</li>\n<li>Performance (two DNS lookups, two SSL handshakes, etc.).</li>\n<li>Indexing by search engines.</li>\n</ul>\n<p>To solve this a central service (a proxy), where all requests will arrive, is needed.</p>\n<h2 id=\"introducing-yarp-as-a-solution\">Introducing YARP as a Solution</h2>\n<p>For about a year, the ASP.NET Core stack have its own reverse proxy - <a href=\"https://github.com/microsoft/reverse-proxy\">YARP</a>. It provides a lot of routing features like headers-based routing, session affinity, load balancing, or destination health checks. In this scenario I'm going to use <a href=\"https://microsoft.github.io/reverse-proxy/articles/direct-forwarding.html\">direct forwarding</a> to simply forward requests to specific services based on a path. I've also decided to set up the rules from code instead of using configuration (it seemed simpler as I still could provide service URLs through environment variables). For this purpose, I've created a <code>MapForwarder</code> extension method.</p>\n<pre><code class=\"lang-cs\">public static void MapForwarder(this IEndpointRouteBuilder endpoints, string pattern, string serviceUrl)\n{\n    var forwarder = endpoints.ServiceProvider.GetRequiredService&lt;IHttpForwarder&gt;();\n    var requestConfig = new ForwarderRequestConfig { ActivityTimeout = TimeSpan.FromMilliseconds(500) };\n\n    endpoints.Map(pattern, async httpContext =&gt;\n    {\n        var error = await forwarder.SendAsync(httpContext, serviceUrl, HttpClient, requestConfig, HttpTransformer.Default);\n\n        if (error != ForwarderError.None)\n        {\n            var errorFeature = httpContext.GetForwarderErrorFeature();\n            var exception = errorFeature?.Exception;\n        }\n    });\n}\n</code></pre>\n<p>I've followed the approach of defining service-specific path prefixes as well as specific prefixes for important pages.</p>\n<pre><code class=\"lang-cs\">var builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddHttpForwarder();\n\nvar app = builder.Build();\n\nstring decideServiceUrl = app.Configuration[\"DECIDE_SERVICE_URL\"];\nstring inspireServiceUrl = app.Configuration[\"INSPIRE_SERVICE_URL\"];\n\napp.UseRouting();\napp.UseEndpoints(endpoints =&gt;\n{\n    // Per service prefixes\n    endpoints.MapForwarder(\"/\", decideServiceUrl);\n\n    // Per service prefixes\n    endpoints.MapForwarder(\"/decide/{**catch-all}\", decideServiceUrl);\n    endpoints.MapForwarder(\"/inspire/{**catch-all}\", inspireServiceUrl);\n\n    // Per page prefixes\n    endpoints.MapForwarder(\"/product/{**catch-all}\", decideServiceUrl);\n    endpoints.MapForwarder(\"/recommendations/{**catch-all}\", inspireServiceUrl);\n});\n\napp.Run();\n</code></pre>\n<p>As it will be now the proxy responsibility to know the addresses of both services, they no longer need to be able to point to each other. All the paths can now be relative, as long as they follow the established rules.</p>\n<pre><code class=\"lang-cs\">&lt;link href=\"/inspire/static/fragment.css\" rel=\"stylesheet\" /&gt;\n&lt;div class=\"inspire_fragment\"&gt;\n  &lt;h2 class=\"inspire_headline\"&gt;Recommendations&lt;/h2&gt;\n  &lt;div class=\"inspire_recommendations\"&gt;\n    &lt;a href=\"/product/fendt\"&gt;&lt;img src=\"https://mi-fr.org/img/fendt.svg\" /&gt;&lt;/a&gt;\n    &lt;a href=\"/product/eicher\"&gt;&lt;img src=\"https://mi-fr.org/img/eicher.svg\" /&gt;&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n<p>There is also no need for CORS anymore, as from a browser perspective there are no cross-origin requests. So I've removed that code from the Inspire service.</p>\n<p>Now it's time to hide the services from the public. For starters let's change their ingress to <code>internal</code>.</p>\n<pre><code class=\"lang-bash\">az containerapp create \\\n  -n ${DECIDE_CONTAINERAPP} \\\n  -i ${CONTAINER_REGISTRY}.azurecr.io/decide:latest \\\n  -g ${RESOURCE_GROUP} \\\n  --environment ${CONTAINERAPPS_ENVIRONMENT} \\\n  --ingress internal \\\n  --target-port 3001 \\\n  --min-replicas 1 \\\n  --registry-server ${CONTAINER_REGISTRY}.azurecr.io\n\naz containerapp create \\\n  -n ${INSPIRE_CONTAINERAPP} \\\n  -i ${CONTAINER_REGISTRY}.azurecr.io/inspire:latest \\\n  -g ${RESOURCE_GROUP} \\\n  --environment ${CONTAINERAPPS_ENVIRONMENT} \\\n  --ingress internal \\\n  --target-port 3002 \\\n  --min-replicas 1 \\\n  --registry-server ${CONTAINER_REGISTRY}.azurecr.io\n</code></pre>\n<p>This way the services are now accessible only from within the Container Apps environment. The proxy can be deployed to the same Container Apps environment and expose the desired traffic outside.</p>\n<pre><code class=\"lang-bash\">az containerapp create \\\n  -n ${PROXY_CONTAINERAPP} \\\n  -i ${CONTAINER_REGISTRY}.azurecr.io/proxy:latest \\\n  -g ${RESOURCE_GROUP} \\\n  --environment ${CONTAINERAPPS_ENVIRONMENT} \\\n  --ingress external \\\n  --target-port 3000 \\\n  --min-replicas 1 \\\n  --registry-server ${CONTAINER_REGISTRY}.azurecr.io \\\n  --env-vars INSPIRE_SERVICE_URL=https://${INSPIRE_CONTAINERAPP_FQDN} DECIDE_SERVICE_URL=https://${DECIDE_CONTAINERAPP_FQDN}\n</code></pre>\n<p>After this change, the Container Apps solution looks like in the below diagram.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoR0qmRQhGKeUJtiJQPWGCSAHPYJsvJ0rCXasadWnR8FqlkZlYUvpp2xPzS_mj--2mIRQzSA96prSTcInUc-7dFPWzcpWuXOCxG9n06sQuSdRktYF-Qmw7xOsvuIkiYM2ON9jrqK9RfB6pGiTvbmerSGHzRLHtH6s9OsFc2wLXNNiJ-CoOrHQVNCgd/s1600/server-side-routing-via-yarp-container-apps-solution.png\" alt=\"Server-Side Routing via YARP Container Apps Solution\"></p>\n<h2 id=\"that-s-all-folks\">That's All Folks</h2>\n<p>At least for this post. You can find the final solution <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/tree/main/04-server-side-routing-via-yarp\">here</a> and its GitHub Actions workflow <a href=\"https://github.com/tpeczek/Demo.AspNetCore.MicroFrontendsInAction/blob/main/.github/workflows/04-server-side-routing-via-yarp.yml\">here</a>. The services have become a little bit strange in the process (they server static HTML in an unnecessarily complicated way) but they are not the focus here, the server-side routing technique is.</p>\n<p>I don't know if or when I'm going to play with another technique, but if I will it's probably going to be something around server-side composition.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2022/06/micro-frontends-in-action-with-aspnet.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-3077980356761970106",
      "Title": "ASP.NET Core 6 and IAsyncEnumerable - Receiving Async Streamed JSON in Blazor WebAssembly",
      "PublishDate": "2021-12-06T14:00:00+00:00",
      "Summary": "<p>Back in July, I've shared my <a href=\"https://www.tpeczek.com/2021/07/aspnet-core-6-and-iasyncenumerable.html\">experiments around new JSON async streaming capabilities in ASP.NET Core 6</a>. Last week I've received a question about utilizing these capabilities in the Blazor WebAssembly application. The person asking the question has adopted the <code>DeserializeAsyncEnumerable</code> based client code, but it didn't seem to work properly. All the results were always displayed at once. As I didn't have a Blazor WebAssembly sample as part of my <a href=\"https://github.com/tpeczek/Demo.Ndjson.AsyncStreams\">streaming JSON objects demo</a>, I've decided I'll add one and figure out the answer to the question along the way.</p>\n<h2 id=\"blazor-webassembly-and-iasyncenumerable\">Blazor WebAssembly and IAsyncEnumerable</h2>\n<p>Before I focus on the problem of results not being received in an async stream manner, I think it is worth discussing the way of working with <code>IAsyncEnumerable</code> in Blazor WebAssembly. What's the challenge here? The first one is that <code>await foreach</code> can't be used in the page markup, only in the code block. So the markup must use a synchronous loop.</p>\n<pre><code class=\"lang-html\">&lt;table&gt;\n    &lt;thead&gt;\n        &lt;tr&gt;\n            &lt;th&gt;Date&lt;/th&gt;\n            &lt;th&gt;Temp. (C)&lt;/th&gt;\n            &lt;th&gt;Temp. (F)&lt;/th&gt;\n            &lt;th&gt;Summary&lt;/th&gt;\n        &lt;/tr&gt;\n    &lt;/thead&gt;\n    &lt;tbody&gt;\n        @foreach (WeatherForecast weatherForecast in weatherForecasts)\n        {\n            &lt;tr&gt;\n                &lt;td&gt;@weatherForecast.DateFormatted&lt;/td&gt;\n                &lt;td&gt;@weatherForecast.TemperatureC&lt;/td&gt;\n                &lt;td&gt;@weatherForecast.TemperatureF&lt;/td&gt;\n                &lt;td&gt;@weatherForecast.Summary&lt;/td&gt;\n            &lt;/tr&gt;\n        }\n    &lt;/tbody&gt;\n&lt;/table&gt;\n</code></pre>\n<p>That brings us to the second challenge. If the <code>await foreach</code> can be used only in the code block, how the streamed results can be rendered as they come? Here the solution comes in form of the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/blazor/components/rendering#when-to-call-statehaschanged\">StateHasChanged</a> method. Calling this method will trigger a render. With this knowledge, we can adopt the <code>DeserializeAsyncEnumerable</code> based code from my previous post.</p>\n<pre><code class=\"lang-cs\">@code {\n\n    private List&lt;WeatherForecast&gt; weatherForecasts = new List&lt;WeatherForecast&gt;();\n\n    private async Task StreamWeatherForecastsJson()\n    {\n        weatherForecasts = new List&lt;WeatherForecast&gt;();\n\n        StateHasChanged();\n\n        using HttpResponseMessage response = await Http.GetAsync(\"api/WeatherForecasts/negotiate-stream\", HttpCompletionOption.ResponseHeadersRead);\n\n        response.EnsureSuccessStatusCode();\n\n        using Stream responseStream = await response.Content.ReadAsStreamAsync();\n\n        await foreach (WeatherForecast weatherForecast in JsonSerializer.DeserializeAsyncEnumerable&lt;WeatherForecast&gt;(\n            responseStream,\n            new JsonSerializerOptions\n            {\n                PropertyNameCaseInsensitive = true,\n                DefaultBufferSize = 128\n            }))\n        {\n            weatherForecasts.Add(weatherForecast);\n\n            StateHasChanged();\n        }\n    }\n}\n</code></pre>\n<p>Running that code put me in the exact same spot where the person asking the question was. All the results were rendered at once after the entire wait time. What to do, when you have no idea what might be wrong and where? Dump what you can to the console ;). No, I'm serious. Debugging through <code>console.log</code> is in fact quite useful in many situations and I'm not ashamed of using it here. I've decided that the diagnostic version will perform direct response stream reading.</p>\n<pre><code class=\"lang-cs\">@code {\n\n    ...\n\n    private async Task StreamWeatherForecastsJson()\n    {\n        Console.WriteLine($\"-- {nameof(StreamWeatherForecastsJson)} --\");\n        Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] Requesting weather forecasts . . .\");\n\n        using HttpResponseMessage response = await Http.GetAsync(\"api/WeatherForecasts/negotiate-stream\", HttpCompletionOption.ResponseHeadersRead);\n\n        response.EnsureSuccessStatusCode();\n\n        Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] Receving weather forecasts . . .\");\n\n        using Stream responseStream = await response.Content.ReadAsStreamAsync();\n\n        Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] Weather forecasts stream obtained . . .\");\n\n        while (true)\n        {\n            byte[] buffer = ArrayPool&lt;byte&gt;.Shared.Rent(128);\n            int bytesRead = await responseStream.ReadAsync(buffer);\n\n            Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] ({bytesRead}/{buffer.Length}) {Encoding.UTF8.GetString(buffer[0..bytesRead])}\");\n\n            ArrayPool&lt;byte&gt;.Shared.Return(buffer);\n\n            if (bytesRead == 0)\n            {\n                break;\n            }\n        }\n\n        Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] Weather forecasts has been received.\");\n        Console.WriteLine();\n    }\n}\n</code></pre>\n<p>Below you can see the output from browser developer tools.</p>\n<pre><code class=\"plaintext\">-- StreamWeatherForecastsJson --\n[08:04:01.183] Requesting weather forecasts . . .\n[08:04:01.436] Receving weather forecasts . . .\n[08:04:02.420] Weather forecasts stream obtained . . .\n[08:04:02.426] (128/128) [{\"dateFormatted\":\"06.12.2021\",\"temperatureC\":28,\"temperatureF\":82,\"summary\":\"Hot\"},{\"dateFormatted\":\"07.12.2021\",\"temperatureC\"\n[08:04:02.429] (128/128) :36,\"temperatureF\":96,\"summary\":\"Scorching\"},{\"dateFormatted\":\"08.12.2021\",\"temperatureC\":-7,\"temperatureF\":20,\"summary\":\"Mild\"}\n[08:04:02.431] (128/128) ,{\"dateFormatted\":\"09.12.2021\",\"temperatureC\":-6,\"temperatureF\":22,\"summary\":\"Hot\"},{\"dateFormatted\":\"10.12.2021\",\"temperatureC\"\n[08:04:02.433] (128/128) :40,\"temperatureF\":103,\"summary\":\"Cool\"},{\"dateFormatted\":\"11.12.2021\",\"temperatureC\":44,\"temperatureF\":111,\"summary\":\"Swelterin\n[08:04:02.434] (128/128) g\"},{\"dateFormatted\":\"12.12.2021\",\"temperatureC\":-3,\"temperatureF\":27,\"summary\":\"Balmy\"},{\"dateFormatted\":\"13.12.2021\",\"temperat\n[08:04:02.435] (128/128) ureC\":1,\"temperatureF\":33,\"summary\":\"Sweltering\"},{\"dateFormatted\":\"14.12.2021\",\"temperatureC\":3,\"temperatureF\":37,\"summary\":\"Ho\n[08:04:02.437] (88/128) t\"},{\"dateFormatted\":\"15.12.2021\",\"temperatureC\":19,\"temperatureF\":66,\"summary\":\"Mild\"}]tureC\":3,\"temperatureF\":37,\"summary\":\"Ho\n[08:04:02.438] (0/128) t\"},{\"dateFormatted\":\"15.12.2021\",\"temperatureC\":19,\"temperatureF\":66,\"summary\":\"Mild\"}]tureC\":3,\"temperatureF\":37,\"summary\":\"Ho\n[08:04:02.439] Weather forecasts has been received.\n</code></pre><p>So the call which is blocking the whole thing seems to be <code>ReadAsStreamAsync</code>, when it returns the entire response is already available. All I knew at this point was that Blazor WebAssembly is using a special <code>HttpMessageHandler</code>. I needed to dig deeper.</p>\n<h2 id=\"digging-into-browserhttphandler\">Digging Into BrowserHttpHandler</h2>\n<p>There is a number of things that have dedicated implementations for Blazor WebAssembly. The <code>HttpClient</code> stack is one of those things. Well, there is no access to native sockets in the browser, so the HTTP calls must be performed based on browser-provided APIs. The <a href=\"https://github.com/dotnet/runtime/blob/main/src/libraries/System.Net.Http/src/System/Net/Http/BrowserHttpHandler/BrowserHttpHandler.cs\"><code>BrowserHttpHandler</code></a> is implemented on top of <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\">Fetch API</a>. Inspecting its code shows that it can provide response content in one of two forms.</p>\n<p>The first one is <a href=\"https://github.com/dotnet/runtime/blob/main/src/libraries/System.Net.Http/src/System/Net/Http/BrowserHttpHandler/BrowserHttpHandler.cs#L394\"><code>BrowserHttpContent</code></a>, which is based on <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/arrayBuffer\"><code>arrayBuffer</code></a> method. This means, that it will always read the response stream to its completion, before making the content available.</p>\n<p>The second one is <code>StreamContent</code> wrapping <a href=\"https://github.com/dotnet/runtime/blob/main/src/libraries/System.Net.Http/src/System/Net/Http/BrowserHttpHandler/BrowserHttpHandler.cs#L461\"><code>WasmHttpReadStream</code></a>, which is based on <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Using_readable_streams#consuming_a_fetch_as_a_stream\">readable streams</a>. This one allows for reading response as it comes.</p>\n<p>How does <code>BrowserHttpHandler</code> decide which one to use? In order for <code>WasmHttpReadStream</code> to be used, <a href=\"https://github.com/dotnet/runtime/blob/main/src/libraries/System.Net.Http/src/System/Net/Http/BrowserHttpHandler/BrowserHttpHandler.cs#L281\">two conditions</a> must be met - the browser must support readable streams and the <code>WebAssemblyEnableStreamingResponse</code> option must be enabled on the request. Now we are getting somewhere. Further search for <code>WebAssemblyEnableStreamingResponse</code> will reveal a <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.components.webassembly.http.webassemblyhttprequestmessageextensions.setbrowserresponsestreamingenabled\"><code>SetBrowserResponseStreamingEnabled</code></a> extension method. Let's see what happens if it's used.</p>\n<pre><code class=\"lang-cs\">@code {\n\n    ...\n\n    private async Task StreamWeatherForecastsJson()\n    {\n        Console.WriteLine($\"-- {nameof(StreamWeatherForecastsJson)} --\");\n        Console.WriteLine($\"[{DateTime.UtcNow:hh:mm:ss.fff}] Requesting weather forecasts . . .\");\n\n        HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Get, \"api/WeatherForecasts/negotiate-stream\");\n        request.SetBrowserResponseStreamingEnabled(true);\n\n        using HttpResponseMessage response = await Http.SendAsync(request, HttpCompletionOption.ResponseHeadersRead);\n\n        response.EnsureSuccessStatusCode();\n\n        ...\n    }\n}\n</code></pre>\n<p>This gives the desired output.</p>\n<pre><code class=\"plaintext\">-- StreamWeatherForecastsJson --\n[08:53:14.722] Requesting weather forecasts . . .\n[08:53:15.002] Receving weather forecasts . . .\n[08:53:15.009] Weather forecasts stream obtained . . .\n[08:53:15.018] (84/128) [{\"dateFormatted\":\"06.12.2021\",\"temperatureC\":31,\"temperatureF\":87,\"summary\":\"Cool\"}\n[08:53:15.057] (84/128) ,{\"dateFormatted\":\"07.12.2021\",\"temperatureC\":18,\"temperatureF\":64,\"summary\":\"Cool\"}\n[08:53:15.166] (86/128) ,{\"dateFormatted\":\"08.12.2021\",\"temperatureC\":10,\"temperatureF\":49,\"summary\":\"Chilly\"}\n[08:53:15.276] (84/128) ,{\"dateFormatted\":\"09.12.2021\",\"temperatureC\":33,\"temperatureF\":91,\"summary\":\"Mild\"}\n[08:53:15.386] (88/128) ,{\"dateFormatted\":\"10.12.2021\",\"temperatureC\":-14,\"temperatureF\":7,\"summary\":\"Freezing\"}\n[08:53:15.492] (84/128) ,{\"dateFormatted\":\"11.12.2021\",\"temperatureC\":12,\"temperatureF\":53,\"summary\":\"Warm\"}\n[08:53:15.600] (86/128) ,{\"dateFormatted\":\"12.12.2021\",\"temperatureC\":6,\"temperatureF\":42,\"summary\":\"Bracing\"}\n[08:53:15.710] (85/128) ,{\"dateFormatted\":\"13.12.2021\",\"temperatureC\":48,\"temperatureF\":118,\"summary\":\"Mild\"}\n[08:53:15.818] (89/128) ,{\"dateFormatted\":\"14.12.2021\",\"temperatureC\":13,\"temperatureF\":55,\"summary\":\"Scorching\"}\n[08:53:15.931] (88/128) ,{\"dateFormatted\":\"15.12.2021\",\"temperatureC\":44,\"temperatureF\":111,\"summary\":\"Chilly\"}]\n[08:53:15.943] (0/128) \n[08:53:15.946] Weather forecasts has been received.\n</code></pre><h2 id=\"corrected-implementation\">Corrected Implementation</h2>\n<p>This means, that in order to have stream-based access to the response body (regardless if it's JSON or something else), one needs to explicitly enable it on the request. So the code which is able to receive async streamed JSON and properly deserialize it to <code>IAsyncEnumerable</code> should look like this.</p>\n<pre><code class=\"lang-cs\">@code {\n\n    ...\n\n    private async Task StreamWeatherForecastsJson()\n    {\n        weatherForecasts = new List&lt;WeatherForecast&gt;();\n\n        StateHasChanged();\n\n        HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Get, \"api/WeatherForecasts/negotiate-stream\");\n        request.SetBrowserResponseStreamingEnabled(true);\n\n        using HttpResponseMessage response = await Http.SendAsync(request, HttpCompletionOption.ResponseHeadersRead);\n\n        response.EnsureSuccessStatusCode();\n\n        using Stream responseStream = await response.Content.ReadAsStreamAsync();\n\n        await foreach (WeatherForecast weatherForecast in JsonSerializer.DeserializeAsyncEnumerable&lt;WeatherForecast&gt;(\n            responseStream,\n            new JsonSerializerOptions\n            {\n                PropertyNameCaseInsensitive = true,\n                DefaultBufferSize = 128\n            }))\n        {\n            weatherForecasts.Add(weatherForecast);\n\n            StateHasChanged();\n        }\n    }\n}\n</code></pre>\n<p>This works exactly as expected - the results are being rendered as they are being returned from the backend (with precision resulting from the size of the buffer).</p>\n<h2 id=\"lesson-learned\">Lesson Learned</h2>\n<p>Despite the fact that there is no dedicated TFM for Blazor WebAssembly, and it uses the \"runs everywhere\" one (<code>net5.0</code>, <code>net6.0</code>, etc.) there are platform differences. You will notice the APIs which aren't supported very quickly because they will throw <code>PlatformNotSupportedException</code>. But there are also more sneaky ones, ones which behavior is different. Blazor WebAssembly needs to be approached with this thought in the back of your mind. This will allow you to properly handle situations when things are not working the way you've expected.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2021/12/aspnet-core-6-and-iasyncenumerable.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-2338776660855107578",
      "Title": "Leveraging Azure Cosmos DB Partial Document Update With JSON Patch in an ASP.NET Core Web API",
      "PublishDate": "2021-11-30T13:24:00+00:00",
      "Summary": "<p>A couple of weeks ago the Cosmos DB team has announced support for <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/partial-document-update\">patching documents</a>. This is quite a useful and long-awaited feature, as up to this point the only way to change the stored document was to completely replace it. This feature opens up new scenarios. For example, if you are providing a Web API on top of Cosmos DB, you can now directly implement support for the <em>PATCH</em> request method. I happen to have a small demo of such <a href=\"https://github.com/tpeczek/Demo.AspNetCore.WebApi\">Web API</a>, so I've decided to refresh it and play with leveraging this new capability.</p>\n<h2 id=\"adding-patch-requests-support-to-an-asp-net-core-web-api\">Adding PATCH Requests Support to an ASP.NET Core Web API</h2>\n<p>An important part of handling a <em>PATCH</em> request is deciding on the request body format. The standard approach for that, in the case of JSON-based APIs, is <a href=\"https://datatracker.ietf.org/doc/html/rfc6902\"><em>JSON Patch</em></a>. In fact, Cosmos DB is also using <em>JSON Patch</em>, so it should be easier to leverage it this way.</p>\n<p>ASP.NET Core provides <a href=\"https://docs.microsoft.com/en-us/aspnet/core/web-api/jsonpatch\">support for <em>JSON Patch</em></a>, but I've decided not to go with it. Why? It's designed around applying operations to an instance of an object and as a result, has internal assumptions about the supported list of operations. In the case of Cosmos DB, the supported operations are different, and that \"applying\" capability is not needed (it was a great way to implement <em>PATCH</em> when Cosmos DB provided on replace option). I've figured out it will be better to start fresh, with a lightweight model.</p>\n<pre><code class=\"lang-cs\">public class JsonPatchOperation\n{\n    [Required]\n    public string Op { get; set; }\n\n    [Required]\n    public string Path { get; set; }\n\n    public object Value { get; set; }\n}\n\npublic class JsonPatch : List&lt;JsonPatchOperation&gt;\n{ }\n</code></pre>\n<p>This class is quite generic and should allow for handling any request which body is compliant with <em>JSON Patch</em> structure. The first concretization I want to introduce is the list of available operations. Cosmos DB currently supports five operations: <em>Add</em>, <em>Set</em>, <em>Remove</em>, <em>Replace</em>, and <em>Increment</em>. In this demo (at least for now) I'm skipping the <em>Increment</em> because it would require a little bit different handling than others.</p>\n<pre><code class=\"lang-cs\">public enum JsonPatchOperationType\n{\n    Add,\n    Set,\n    Remove,\n    Replace,,\n    Invalid\n}\n</code></pre>\n<p>As you can see, in the above enumeration I've also added the <code>Invalid</code> value. This will give me a way to represent the operations I don't intend to support through a specific value instead of e.g. throwing an exception.</p>\n<pre><code class=\"lang-cs\">public class JsonPatchOperation\n{\n    private string _op;\n    private JsonPatchOperationType _operationType;\n\n    [Required]\n    public string Op\n    {\n        get { return _op; }\n\n        set\n        {\n            JsonPatchOperationType operationType;\n            if (!Enum.TryParse(value, ignoreCase: true, result: out operationType))\n            {\n                operationType = JsonPatchOperationType.Invalid;\n            }\n\n            _operationType = operationType;\n\n            _op = value;\n        }\n    }\n\n    public JsonPatchOperationType OperationType =&gt; _operationType;\n\n    ...\n}\n</code></pre>\n<p>Having not supported operations represented through a specific value also allows me to implement <code>IValidatableObject</code> which checks for them. This way, if the class is used as an action model, making a request with an unsupported operation will trigger a <em>400 Bad Request</em> response.</p>\n<pre><code class=\"lang-cs\">public class JsonPatchOperation : IValidatableObject\n{\n    ...\n\n    public IEnumerable&lt;ValidationResult&gt; Validate(ValidationContext validationContext)\n    {\n        if (OperationType == JsonPatchOperationType.Invalid)\n        {\n            yield return new ValidationResult($\"Not supported operation: {Op}.\", new[] { nameof(Op) });\n        }\n    }\n}\n</code></pre>\n<p>Now all is needed is an action that will support the <em>PATCH</em> method.</p>\n<pre><code class=\"lang-cs\">[Route(\"api/[controller]\")]\n[ApiController]\npublic class CharactersController : Controller\n{\n    ...\n\n    [HttpPatch(\"{id}\")]\n    public async Task&lt;ActionResult&lt;Character&gt;&gt; Patch(string id, JsonPatch update)\n    {\n        ...\n    }\n\n    ...\n}\n</code></pre>\n<p>The next step will be proxying the deserialized <em>JSON Patch</em> to Cosmos DB .NET SDK.</p>\n<h2 id=\"utilizing-cosmos-db-partial-updates\">Utilizing Cosmos DB Partial Updates</h2>\n<p>The Cosmos DB .NET SDK exposes partial document updates through the <code>PatchItemAsync</code> method on a container. This method expects a collection of <code>PatchOperation</code> instances. Instances representing specific operations can be created through static methods on <code>PatchOperation</code> which names correspond to operations names. So the conversion from <code>JsonPatch</code> to <code>PatchOperation</code> collection requires calling the appropriate method for every <code>JsonPatchOperation</code>. This is something a simple extensions method should handle.</p>\n<pre><code class=\"lang-cs\">public static class JsonPatchExtensions\n{\n    public static IReadOnlyList&lt;PatchOperation&gt; ToCosmosPatchOperations(this JsonPatch jsonPatchOperations)\n    {\n\n        List&lt;PatchOperation&gt; cosmosPatchOperations = new List&lt;PatchOperation&gt;(jsonPatchOperations.Count);\n        foreach (JsonPatchOperation jsonPatchOperation in jsonPatchOperations)\n        {\n            switch (jsonPatchOperation.OperationType)\n            {\n                case JsonPatchOperationType.Add:\n                    cosmosPatchOperations.Add(PatchOperation.Add(jsonPatchOperation.Path, jsonPatchOperation.Value));\n                    break;\n                case JsonPatchOperationType.Remove:\n                    cosmosPatchOperations.Add(PatchOperation.Remove(jsonPatchOperation.Path));\n                    break;\n                case JsonPatchOperationType.Replace:\n                    cosmosPatchOperations.Add(PatchOperation.Replace(jsonPatchOperation.Path, jsonPatchOperation.Value));\n                    break;\n                case JsonPatchOperationType.Set:\n                    System.Int32 test = 25;\n                    cosmosPatchOperations.Add(PatchOperation.Set(jsonPatchOperation.Path, jsonPatchOperation.Value));\n                    break;\n            }\n        }\n\n        return cosmosPatchOperations;\n    }\n}\n</code></pre>\n<p>Making the proper call in our action.</p>\n<pre><code class=\"lang-cs\">[Route(\"api/[controller]\")]\n[ApiController]\npublic class CharactersController : Controller\n{\n    ...\n\n    [HttpPatch(\"{id}\")]\n    public async Task&lt;ActionResult&lt;Character&gt;&gt; Patch(string id, JsonPatch update)\n    {\n        ...\n\n        ItemResponse&lt;Character&gt; characterItemResponse = await _starWarsCosmosClient.Characters.PatchItemAsync&lt;Character&gt;(\n            id,\n            PartitionKey.None,\n            update.ToCosmosPatchOperations());\n\n        return characterItemResponse.Resource;\n    }\n\n    ...\n}\n</code></pre>\n<p>And we have some testable code. In order to test I've decided to attempt two operations: <em>Set</em> on <code>/height</code> and <em>Add</em> on <code>/weight</code>. This is represented by below <em>JSON Patch</em> body.</p>\n<pre><code class=\"lang-js\">[\n  {\n    \"op\": \"set\",\n    \"path\": \"/height\",\n    \"value\": 195\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/weight\",\n    \"value\": 90\n  }\n]\n</code></pre>\n<p>I've hit F5, crated the request in Postman, and clicked Send. What I've received was a <em>500 Internal Server Error</em> with a weird <code>Newtonsoft.Json</code> deserialization exception. A quick look into Cosmos DB Explorer revealed that the document now looks like this.</p>\n<pre><code class=\"lang-js\">{\n    ...\n    \"height\": {\n        \"valueKind\": 4\n    },\n    \"weight\": {\n        \"valueKind\": 4\n    },\n    ...\n}\n</code></pre>\n<p>This is not what I was expecting. What happened? The fact that <code>PatchItemAsync</code> worked without an exception suggested that this must be how Cosmos DB .NET SDK interpreted the <code>JsonPatchOperation.Value</code>. Through debugging I've quickly discovered that what <code>JsonPatchOperation.Value</code> actually holds is <code>System.Text.Json.JsonElement</code>. The Cosmos DB .NET SDK has no other way of dealing with that than serializing public properties - regardless of how smart the implementation is. This is because Cosmos DB .NET SDK (at least for now) is based on <code>Newtonsoft.Json</code>.</p>\n<p>So, this is going to be a little bit harder. Conversion from <code>JsonPatch</code> to <code>PatchOperation</code> collection will require deserializing the value along the way. As this is still part of deserializing the request, I've figured out it will be best to put it into <code>JsonPatchOperation</code>.</p>\n<pre><code class=\"lang-cs\">public class JsonPatchOperation : IValidatableObject\n{\n    ...\n\n    public T GetValue&lt;T&gt;()\n    {\n        return ((JsonElement)Value).Deserialize&lt;T&gt;();\n    }\n}\n</code></pre>\n<p>This will need to be called with the right type parameter, so a mapping between paths and types needs to be obtained. I've decided to make this information available through <code>JsonPatch</code> by making it generic and spicing with reflection.</p>\n<pre><code class=\"lang-cs\">public class JsonPatch&lt;T&gt; : List&lt;JsonPatchOperation&gt;\n{\n    private static readonly IDictionary&lt;string, Type&gt; _pathsTypes;\n\n    static JsonPatch()\n    {\n        _pathsTypes = typeof(T).GetProperties().ToDictionary(p =&gt; $\"/{Char.ToLowerInvariant(p.Name[0]) + p.Name[1..]}\", p =&gt; p.PropertyType);\n    }\n\n    public Type GetTypeForPath(string path)\n    {\n        return _pathsTypes[path];\n    }\n}\n</code></pre>\n<p>A disclaimer is in order. This simple code will work only in this simple case. I'm looking only at top-level properties because my document only has top-level properties. But, in general, a path can be targeting a nested property e.g. <code>/mather/familyName</code>. This code will have to get more complicated to handle such a case.</p>\n<p>To make the conversion from <code>JsonPatch</code> to <code>PatchOperation</code> work correctly, sadly, more reflection is needed as those generic calls have to be made with the right parameters at the runtime. The needed pieces can live together with the conversion extension method.</p>\n<pre><code class=\"lang-cs\">public static class JsonPatchExtensions\n{\n    private static MethodInfo _createAddPatchOperationMethodInfo = typeof(JsonPatchExtensions)\n        .GetMethod(nameof(JsonPatchExtensions.CreateAddPatchOperation), BindingFlags.NonPublic | BindingFlags.Static);\n    private static MethodInfo _createReplacePatchOperationMethodInfo = typeof(JsonPatchExtensions)\n        .GetMethod(nameof(JsonPatchExtensions.CreateReplacePatchOperation), BindingFlags.NonPublic | BindingFlags.Static);\n    private static MethodInfo _createSetPatchOperationMethodInfo = typeof(JsonPatchExtensions)\n        .GetMethod(nameof(JsonPatchExtensions.CreateSetPatchOperation), BindingFlags.NonPublic | BindingFlags.Static);\n\n    ...\n\n    private static PatchOperation CreateAddPatchOperation&lt;T&gt;(JsonPatchOperation jsonPatchOperation)\n    {\n        return PatchOperation.Add(jsonPatchOperation.Path, jsonPatchOperation.GetValue&lt;T&gt;());\n    }\n\n    private static PatchOperation CreateReplacePatchOperation&lt;T&gt;(JsonPatchOperation jsonPatchOperation)\n    {\n        return PatchOperation.Replace(jsonPatchOperation.Path, jsonPatchOperation.GetValue&lt;T&gt;());\n    }\n\n    private static PatchOperation CreateSetPatchOperation&lt;T&gt;(JsonPatchOperation jsonPatchOperation)\n    {\n        return PatchOperation.Set(jsonPatchOperation.Path, jsonPatchOperation.GetValue&lt;T&gt;());\n    }\n\n    private static PatchOperation CreatePatchOperation&lt;T&gt;(\n        MethodInfo createSpecificPatchOperationMethodInfo,\n        JsonPatch&lt;T&gt; jsonPatchOperations,\n        JsonPatchOperation jsonPatchOperation)\n    {\n        Type jsonPatchOperationValueType = jsonPatchOperations.GetTypeForPath(jsonPatchOperation.Path);\n\n        MethodInfo createSpecificPatchOperationWithValueTypeMethodInfo =\n            createSpecificPatchOperationMethodInfo.MakeGenericMethod(jsonPatchOperationValueType);\n\n        return (PatchOperation)createSpecificPatchOperationWithValueTypeMethodInfo.Invoke(null, new object[] { jsonPatchOperation });\n    }\n}\n</code></pre>\n<p>The code above has been structured to isolate the reflection related part and cache the well-known things. Of course, this is a subject of personal preference, but I hope it's readable.</p>\n<p>The last remaining thing is modifying the extension method.</p>\n<pre><code class=\"lang-cs\">public static class JsonPatchExtensions\n{\n    ...\n\n    public static IReadOnlyList&lt;PatchOperation&gt; ToCosmosPatchOperations&lt;T&gt;(this JsonPatch jsonPatchOperations)\n    {\n        List&lt;PatchOperation&gt; cosmosPatchOperations = new List&lt;PatchOperation&gt;(jsonPatchOperations.Count);\n        foreach (JsonPatchOperation jsonPatchOperation in jsonPatchOperations)\n        {\n            switch (jsonPatchOperation.OperationType)\n            {\n                case JsonPatchOperationType.Add:\n                    cosmosPatchOperations.Add(CreatePatchOperation(_createAddPatchOperationMethodInfo, jsonPatchOperations, jsonPatchOperation));\n                    break;\n                case JsonPatchOperationType.Remove:\n                    cosmosPatchOperations.Add(PatchOperation.Remove(jsonPatchOperation.Path));\n                    break;\n                case JsonPatchOperationType.Replace:\n                    cosmosPatchOperations.Add(CreatePatchOperation(_createReplacePatchOperationMethodInfo, jsonPatchOperations, jsonPatchOperation));\n                    break;\n                case JsonPatchOperationType.Set:\n                    cosmosPatchOperations.Add(CreatePatchOperation(_createSetPatchOperationMethodInfo, jsonPatchOperations, jsonPatchOperation));\n                    break;\n            }\n        }\n\n        return cosmosPatchOperations;\n    }\n\n    ...\n}\n</code></pre>\n<p>Adjusting the action code, building the demo, running, going to Postman, sending a request, and it works!</p>\n<h2 id=\"what-s-missing-\">What’s Missing?</h2>\n<p>This is demoware, so there is always something missing. I've already mentioned that parts of the code are handling only simple cases. But there are two additional subjects which require more attention.</p>\n<p>One is validation. This code is not validating if paths provided for operations represent valid properties. It's also not validating if values can be converted to the right types and if the operations result in an invalid state of the entity. First, two things should be achievable at the <code>JsonPatch</code> level. The last one will require additional code in action.</p>\n<p>The other subject is performance around that reflection code. It can be improved by caching the generic methods for specific types, but as the solution grows it might not be enough. It is worth thinking about another option here.</p>\n<p>I might tackle those two subjects, but I don't know if and when. You can keep an eye on the <a href=\"https://github.com/tpeczek/Demo.AspNetCore.WebApi\">demo project</a> because if I do, the code will certainly end there.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2021/11/leveraging-azure-cosmos-db-partial.html"
    },
    {
      "FeedId": "tag:blogger.com,1999:blog-7365737872932202828",
      "ItemId": "tag:blogger.com,1999:blog-7365737872932202828.post-4981240213044006715",
      "Title": "Server-Sent Events and ASP.NET Core - Disconnecting a Client",
      "PublishDate": "2021-11-08T20:28:00+00:00",
      "Summary": "<p>One of the common requests for my <a href=\"https://github.com/tpeczek/Lib.AspNetCore.ServerSentEvents\">Server-Sent Events library</a> is the ability to disconnect clients from a server. My usual answer was that this is best to be implemented as a logical operation where the server sends an event with a specific type and clients to react to it by closing the connection. I was avoiding putting disconnect functionality into the library because it's not fully defined by the protocol.</p>\n<h2 id=\"disconnecting-a-client-in-server-sent-events\">Disconnecting a Client in Server-Sent Events</h2>\n<p>Disconnecting a client from a server is a little bit tricky when it comes to Server-Sent Events. The reason for that is automatic reconnect. If the server simply closes the connection, the client will wait a defined period of time and reconnect. The only way to prevent the client from reconnecting is to respond with a <em>204 No Content</em> status code. So complete flow of disconnecting a client should look like on the diagram below.</p>\n<p><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1vDwX0KrD5WI3mvE_hc1Lk-CyXoQEj0hXby0twqX5W1bBvz6RRcRlEyXC9-ZjPH0Hogr7QZ7CgProw88xYtF9_p3nzMuSX6cyjXqZuXQdHODrJ3WTsHrYcCVDJptbp4WNuD5MIr96uVI/s0/SSE+Disconnect+Flow.png\" alt=\"Server-Sent Events Client Disconnect Flow Diagram\"></p>\n<p>There is just one challenge here - the server needs to be able to tell that it is the same client trying to reconnect.</p>\n<h2 id=\"identifying-clients-in-server-sent-events\">Identifying Clients in Server-Sent Events</h2>\n<p>Server-Sent Events doesn't provide any specific way of identifying clients. No dedicated session mechanism. This is a place where choices and opinions are starting, a place where a library should try to stay generic. I've given a lot of thought to whether I want it to include any specific implementation with the library and I've decided to provide only the contract and a no-op implementation.</p>\n<pre><code class=\"lang-cs\">public interface IServerSentEventsClientIdProvider\n{\n    Guid AcquireClientId(HttpContext context);\n\n    void ReleaseClientId(Guid clientId, HttpContext context);\n}\n</code></pre>\n<p>This gives consumers full freedom (and responsibility) to implement this aspect in whichever way they prefer. For example, below is a simple cookie-based implementation. In the case of a production scenario, you probably want to think a little bit more about cookie options, if the value should be protected, etc. Also, remember that cookies bear legal requirements.</p>\n<pre><code class=\"lang-cs\">internal class CookieBasedServerSentEventsClientIdProvider : IServerSentEventsClientIdProvider\n{\n    private const string COOKIE_NAME = \".ServerSentEvents.Guid\";\n\n    public Guid AcquireClientId(HttpContext context)\n    {\n        Guid clientId;\n\n        string cookieValue = context.Request.Cookies[COOKIE_NAME];\n        if (String.IsNullOrWhiteSpace(cookieValue) || !Guid.TryParse(cookieValue, out clientId))\n        {\n            clientId = Guid.NewGuid();\n\n            context.Response.Cookies.Append(COOKIE_NAME, clientId.ToString());\n        }\n\n        return clientId;\n    }\n\n    public void ReleaseClientId(Guid clientId, HttpContext context)\n    {\n        context.Response.Cookies.Delete(COOKIE_NAME);\n    }\n}\n</code></pre>\n<h2 id=\"tracking-disconnected-server-sent-events-clients\">Tracking Disconnected Server-Sent Events Clients</h2>\n<p>Being able to identify a client is only the first step. The second required thing is tracking the disconnected clients, so when they attempt to reconnect the server can respond with <em>204 No Content</em>.</p>\n<pre><code class=\"lang-cs\">public interface IServerSentEventsNoReconnectClientsIdsStore\n{\n    Task AddClientId(Guid clientId);\n\n    Task&lt;bool&gt; ContainsClientId(Guid clientId);\n\n    Task RemoveClientId(Guid clientId);\n}\n</code></pre>\n<p>This part doesn't seem to be complicated until you consider scaling out. When there are multiple instances behind a load balancer, there is a possibility that reconnect attempt will reach a different instance than the one to which the client has been previously connected. This is why I've decided to include two implementations of the above store in the library. One is simply keeping the identifiers in memory, while the second is backed by distributed cache.</p>\n<h2 id=\"putting-things-together\">Putting Things Together</h2>\n<p>The high-level flow which library currently performs to handle Server-Sent Events requests looks like below.</p>\n<pre><code class=\"lang-cs\">public class ServerSentEventsMiddleware&lt;TServerSentEventsService&gt; ...\n{\n    ...\n\n    public async Task Invoke(HttpContext context, IPolicyEvaluator policyEvaluator)\n    {\n        if (CheckAcceptHeader(context.Request.Headers))\n        {\n            if (!await AuthorizeAsync(context, policyEvaluator))\n            {\n                return;\n            }\n\n            ...\n\n            await context.Response.AcceptAsync(_serverSentEventsOptions.OnPrepareAccept);\n\n            ServerSentEventsClient client = new ServerSentEventsClient(clientId, context.User, context.Response, _clientDisconnectServicesAvailable);\n\n            ...\n\n            await ConnectClientAsync(context.Request, client);\n\n            await context.RequestAborted.WaitAsync();\n\n            await DisconnectClientAsync(context.Request, client);\n        }\n        else\n        {\n            await _next(context);\n        }\n    }\n\n    ...\n}\n</code></pre>\n<p>To enable the disconnect capability, this flow needs to be adjusted to acquire the client identifier as soon as possible and prevent connection (by responding with 204) if the identifier represents a client which shouldn't be allowed to reconnect.</p>\n<pre><code class=\"lang-cs\">public class ServerSentEventsMiddleware&lt;TServerSentEventsService&gt; ...\n{\n    ...\n\n    public async Task Invoke(HttpContext context, IPolicyEvaluator policyEvaluator)\n    {\n        if (CheckAcceptHeader(context.Request.Headers))\n        {\n            if (!await AuthorizeAsync(context, policyEvaluator))\n            {\n                return;\n            }\n\n            Guid clientId = _serverSentEventsClientIdProvider.AcquireClientId(context);\n\n            if (await PreventReconnectAsync(clientId, context))\n            {\n                return;\n            }\n\n            ...\n\n            await context.Response.AcceptAsync(_serverSentEventsOptions.OnPrepareAccept);\n\n            ...\n\n            await DisconnectClientAsync(context.Request, client);\n        }\n        else\n        {\n            await _next(context);\n        }\n    }\n\n    ...\n\n    private async Task PreventReconnectAsync(Guid clientId, HttpContext context)\n    {\n        if (!await _serverSentEventsNoReconnectClientsIdsStore.ContainsClientIdAsync(clientId))\n        {\n            return false;\n        }\n\n        response.StatusCode = StatusCodes.Status204NoContent;\n\n        _serverSentEventsClientIdProvider.ReleaseClientId(clientId, context);\n\n        await _serverSentEventsNoReconnectClientsIdsStore.RemoveClientIdAsync(clientId);\n\n        return true;\n    }\n\n    ...\n}\n</code></pre>\n<p>You can also see that as part of reconnecting prevention I'm releasing the client identifier and removing it from the \"no reconnect\" identifiers store. The goal is to allow any underlying implementations to clear any data and make sure that a stale identifier doesn't \"stick\" with the client.</p>\n<p>One more thing which requires adjustment are operations performed when a client is being disconnected. If the client is being disconnected by a server, its identifier should be added to the store. If the disconnect happens on the client-side, the client identifier should be released to avoid staleness.</p>\n<pre><code class=\"lang-cs\">public class ServerSentEventsMiddleware&lt;TServerSentEventsService&gt; ...\n{\n    ...\n\n    private async Task DisconnectClientAsync(HttpRequest request, ServerSentEventsClient client)\n    {\n        ...\n\n        if (client.PreventReconnect)\n        {\n            await _serverSentEventsNoReconnectClientsIdsStore.AddClientIdAsync(client.Id);\n        }\n        else\n        {\n            _serverSentEventsClientIdProvider.ReleaseClientId(client.Id, request.HttpContext);\n        }\n\n        ...\n    }\n\n    ...\n}\n</code></pre>\n<h2 id=\"coming-soon-to-a-nuget-feed-near-you\">Coming Soon to a NuGet Feed Near You</h2>\n<p>The code is ready and I'm going to push it out with the next release of <a href=\"https://www.nuget.org/packages/Lib.AspNetCore.ServerSentEvents\">Lib.AspNetCore.ServerSentEvents</a>. So, if you have been waiting for disconnect capabilities in the library they soon will be there. That said, it still leaves some decisions and implementation to consumers. By sharing parts of my thought process and some implementation details I wanted to make clear why it is like this.</p>",
      "Content": null,
      "Language": null,
      "Link": "http://www.tpeczek.com/2021/11/server-sent-events-and-aspnet-core.html"
    }
  ]
}
